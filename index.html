



<!--<p><a href="home.html" target="_blank">Street view outside my home.</a></p>-->





<title>Kai</title>

<!-- <script type="text/python" src="hello.py"></script>


<p><a href="hello.py" target="_blank">hello</a></p>


<p><a href="http://pages.iu.edu/~zhenk/Snip20150809_5.png" target="_blank">imaggge</a></p>





<a href="http://shijue.me/zone/contest/formula/about" target="_blank">history</a>


-->

<meta http-equiv="content-type" content="text/html;charset=gb18030">

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
  <meta name="google-site-verification" content="Y7dZHQ2oiL6gVpQu_b_FeORVefVQmwcCmQc3C893DhM" />
<link rel="shortcut icon" href="fff.png">
<meta name="description" content="Ph.D. Candidate in Computer Science and Cognitive Science at Indiana University. Machine Learning And Relevance Intern at LinkedIn Corporation. Proud to Be A Hoosier.">
<!--<link rel="shortcut icon" href="favicon3.ico" type="image/x-icon" >-->
  <title>Kai's home page</title>
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script language="javascript" src="gistfile1.js"></script>
</head>

<body bgcolor="#FFFFFF" text="#000000">

  <!--
  <div align="center">
    <img src="1419252313.jpg" width="732" height="549" alt="kai">
    <br>
    <b><font size="6">Kai Zhen</font></b>
  </div>
  -->

<b><font size="6">Kai Zhen</font></b>
<p><b> Ph.D. Candidate in <a href="http://www.cs.indiana.edu/">Computer Science </a> and <a href="http://www.cogs.indiana.edu/">Cognitive Science </a>at <a href="http://www.indiana.edu/">Indiana University</a>.<br>
</p>  






<p>
  Research Group: <a href="http://saige.sice.indiana.edu/"> Signals & Artificial Intelligence Group in Engineering (SAIGE) </a><br>
  Email: ZHenK At IU doT EdU <br>
  <!--<a href="https://www.linkedin.com/in/kai-zhen-01b7b5103/"> LinkedIn profile </a><br>-->
  <a href="cv.pdf"> Resume </a><br>
  <!--CV: <a href="cv_425.pdf"> last updated in April 2019.</a><br>
  
  Mail: Department of Psychological and Brain Sciences, Room 601K. 1001 E 10th St, Bloomington, IN 47408
<br>
  <a href="Papers/KAI-NOV-2015.pdf">Curriculum Vitae (as of Nov 2015)</a> <br>-->
<!--<br>
  Currently, I am open to internship opportunities. <a href="Resume9516.pdf">Check out my resume (as of September 2016).</a> <br> -->
</p>


<!-- 

<h3>News </h3>
<ul> 
  <li><p><b><a href="http://saige.sice.indiana.edu/neural-audio-coding" > A new speech coding paper, "Cascaded Cross-Module Residual Learning towards Lightweight End-to-End Speech Coding," was accepted for publication in INTERSPEECH 2019.</a> 
  <li><p><b><a href="https://economicgraph.linkedin.com/" > Received a return offer of Machine Learning and Relevance internship at LinkedIn, Sunnyvale! Looking forward to the Summer of 2019!</a> 
  <li><p><b><a href="http://www.indiana.edu/~pcl/mwcogsci/schedule.php"> Our submission on speech enhancement with psychoacoustic models was accepted for oral presentation in the 2018 Midwest Cognitive Science Conference.</a> 
  
</p> </ul>
-->

<h3>Research interests </h3>
<p>
<p>

  I conduct research on audio and acoustic signal processing in the current deep/machine learning paradigm, with the focus on both model capacity and efficiency. Concretely, I've been working on cross-module residual learning that is compatible with both advanced, fast changing data-driven modules and conventional methodologies in audiology for lightweight <a href="speechcoding.html">speech coding</a>. In terms of <a href="speechenhancement.html">monaural speech enhancement</a>, we proposed a hybrid architecture incorporating both CNN and RNN in a densely connected manner to enable dual-level context aggregation, efficiently. Besides, I worked on proposing a psychoacoustically weighting scheme to prioritize the model training towards an energy efficient speech denoising autoencoder. My supervisor is <a href="https://minjekim.com">Prof. Minje Kim</a>.</p>

  <!--My research focuses on audio and acoustic signal processing in the current machine learning paradigm. I've been working on designing model carriers that are compatible with advanced, fast changing data-driven modules and conventional methodologies in speech coding. Besides, we proposed a psychoacoustically weighting scheme to prioritize the model training procedure towards a compact and energy efficient speech denoising autoencoder. My supervisor is <a href="https://minjekim.com">Prof. Minje Kim</a>.</p>


  My research is mainly focused on audio, speech and music signal processing in the current machine learning paradigm.  I'm currently working on a psychoacoustically reweighed deep neural network with parameter quantization towards a compact and energy efficient speech denoising autoencoder.-->



<!--
<ul> 
  <li>a.wav 
<audio controls>
  <source src="audio/a.wav" type="audio/mpeg">
</audio>
<li>b.wav 
<audio controls>
  <source src="audio/b.wav" type="audio/mpeg">
</audio>
<li>c.wav 
<audio controls>
  <source src="audio/c.wav" type="audio/mpeg">
</audio>
<li>d.wav 
<audio controls>
  <source src="audio/d.wav" type="audio/mpeg">
</audio>
</ul>
-->

<!--
Our <a href="speechcoding.html"> speech coding </a> framework balances between objective and perceptual quality by blending conventional coding techniques with the state of the art data driven approach. Among following utterances, only one is the reference and the other three are decoded signals from our method. Could you identify the reference? More samples are provided <a href="speechcoding.html" style="color:#FF0000;"> here</a>.

<div id="qp_all2171043" style="width:100%;max-width:600px;"><STYLE>#qp_main2171043 .qp_btna:hover input {background:rgb(150,150,150)!important}</STYLE><div id="qp_main2171043" fp='0a7e501E-62' results=0 style="border-radius:6px;text-align:left;border:1px solid rgb(150,150,150);margin:0 auto;padding:10px;background:rgb(255,255,255);box-sizing:border-box"><div style="border-radius:6px;font-family:Arial;font-size:12px;font-weight:bold;background-color:rgb(44,44,44);color:rgb(255,255,255);margin-bottom:10px"><div style="padding:10px">Which one is the reference?</div></div><form id="qp_form2171043" action="//www.poll-maker.com/results2171043x0a7e501E-62" method="post" target="_blank" style="display:inline;margin:0px;padding:0px"><div style="border-radius:6px"><input type=hidden name="qp_d2171043" value="43436.2065625551-43436.206500981"><div style="display:block;font-family:Arial;font-size:12px;color:rgb(0,0,0);padding-top:5px;padding-bottom:5px;clear:both" class="qp_a" onClick="var c=this.getElementsByTagName('INPUT')[0]; if((!event.target?event.srcElement:event.target).tagName!='INPUT'){c.checked=(c.type=='radio'?true:!c.checked)};var i=this.parentNode.parentNode.parentNode.getElementsByTagName('INPUT');for(var k=0;k!=i.length;k=k+1){i[k].parentNode.parentNode.setAttribute('sel',i[k].checked?1:0)}"><span style="display:block;padding-left:30px;cursor:inherit"><input style="float:left;width:18px;margin-left:-25px;margin-top:-2px;padding:0px;height:18px;-webkit-appearance:radio;" name="qp_v2171043" type="radio" value="1" />a.wav  <audio controls>
  <source src="audio/a.wav" type="audio/mpeg">
</audio>
</span></div><div style="display:block;font-family:Arial;font-size:12px;color:rgb(0,0,0);padding-top:5px;padding-bottom:5px;clear:both" class="qp_a" onClick="var c=this.getElementsByTagName('INPUT')[0]; if((!event.target?event.srcElement:event.target).tagName!='INPUT'){c.checked=(c.type=='radio'?true:!c.checked)};var i=this.parentNode.parentNode.parentNode.getElementsByTagName('INPUT');for(var k=0;k!=i.length;k=k+1){i[k].parentNode.parentNode.setAttribute('sel',i[k].checked?1:0)}"><span style="display:block;padding-left:30px;cursor:inherit"><input style="float:left;width:18px;margin-left:-25px;margin-top:-2px;padding:0px;height:18px;-webkit-appearance:radio;" name="qp_v2171043" type="radio" value="2" />b.wav<audio controls>
  <source src="audio/b.wav" type="audio/mpeg">
</audio>
</span></div><div style="display:block;font-family:Arial;font-size:12px;color:rgb(0,0,0);padding-top:5px;padding-bottom:5px;clear:both" class="qp_a" onClick="var c=this.getElementsByTagName('INPUT')[0]; if((!event.target?event.srcElement:event.target).tagName!='INPUT'){c.checked=(c.type=='radio'?true:!c.checked)};var i=this.parentNode.parentNode.parentNode.getElementsByTagName('INPUT');for(var k=0;k!=i.length;k=k+1){i[k].parentNode.parentNode.setAttribute('sel',i[k].checked?1:0)}"><span style="display:block;padding-left:30px;cursor:inherit"><input style="float:left;width:18px;margin-left:-25px;margin-top:-2px;padding:0px;height:18px;-webkit-appearance:radio;" name="qp_v2171043" type="radio" value="3" />c.wav
<audio controls>
  <source src="audio/c.wav" type="audio/mpeg">
</audio>
</span></div><div style="display:block;font-family:Arial;font-size:12px;color:rgb(0,0,0);padding-top:5px;padding-bottom:5px;clear:both" class="qp_a" onClick="var c=this.getElementsByTagName('INPUT')[0]; if((!event.target?event.srcElement:event.target).tagName!='INPUT'){c.checked=(c.type=='radio'?true:!c.checked)};var i=this.parentNode.parentNode.parentNode.getElementsByTagName('INPUT');for(var k=0;k!=i.length;k=k+1){i[k].parentNode.parentNode.setAttribute('sel',i[k].checked?1:0)}"><span style="display:block;padding-left:30px;cursor:inherit"><input style="float:left;width:18px;margin-left:-25px;margin-top:-2px;padding:0px;height:18px;-webkit-appearance:radio;" name="qp_v2171043" type="radio" value="4" />d.wav
<audio controls>
  <source src="audio/d.wav" type="audio/mpeg">
</audio>
</span></div></div><div style="clear:both;text-align:left;margin:0 auto 0em auto"><a style="text-decoration:none" class="qp_btna" href="#"><input name="qp_b2171043" style="min-width:7em;padding:0.5em;margin-top:5px;margin-right:5px;border-radius:10px;border:1px solid rgb(150,150,150);font-family:Arial;font-size:12px;font-weight:bold;color:rgb(0,0,0);cursor:pointer;cursor:hand;background:rgb(200,200,200)" type="submit" btype="v" value="Vote" /></a><a style="text-decoration:none" class="qp_btna" href="#"><input name="qp_b2171043" style="min-width:7em;padding:0.5em;margin-top:5px;margin-right:5px;border-radius:10px;border:1px solid rgb(150,150,150);font-family:Arial;font-size:12px;font-weight:bold;color:rgb(0,0,0);cursor:pointer;cursor:hand;background:rgb(200,200,200)" type="submit" btype="r" value="Results" /></a></div></form><div style="display:none"><div id="qp_rp2171043" style="font-size:11px;width:5ex;text-align:right;overflow:hidden;position:absolute;right:5px;height:1.5em;line-height:1.5em"></div><div id="qp_rv2171043" style="font-size:11px;width:0%;line-height:1.5em;text-align:right;color:#FFF;box-sizing:border-box;padding-right:3px"></div><div id="qp_rb2171043" style="font-size:12px;color:rgb(255,255,255);display:block;font-size:12px;padding-right:10px 5px"></div><div id="qp_rva2171043" style="background:#006FB9;border-color:#006FB9"></div><div id="qp_rvb2171043" style="background:#163463;border-color:#163463"></div><div id="qp_rvc2171043" style="background:#5BCFFC;border-color:#1481AB"></div></div></div></div><script src="//scripts.poll-maker.com/3012/scpolls.js" language="javascript"></script>


-->


<!--
<p>My interest lies in the modeling of the fundamental human learning capacity of cross-modality concept learning that is beyond the major scope of current methods in artificial intelligence. 
Concretely, I want to propose computational models in understanding how the processing of acoustic signals (especially music data) enhances the degree of visual perception, and vice versa. This is a joint area of
computer science and cognitive science where methods from both sides (Bayesian inference, deep learning tools, and cognitive modeling techniques) are applied.</p>
</p> -->


<!--<h3>Advisory committee </h3>
<p>
  <a href="http://www.indiana.edu/~pcl/rgoldsto/cv.pdf">Robert Goldstone</a>, <a href="https://www.cs.indiana.edu/~djcran/cv.pdf">David Crandall</a>, TBD, TBD

</p>

<h3>Publications </h3>
<h4>Posters </h4>
<ul> <p> <li><p><a href="pub/egocentric_topic_poster.pdf">Kai Zhen and David Crandall. Finding egocentric image topics through convolutional neural
network based representations. In IEEE Conference on Computer Vision and Pattern Recognition
Workshop on Egocentric Computer Vision, Las Vegas, 2016. (Poster).</a>

<h4>Drafts </h4>
<p> <li><p><a href="https://arxiv.org/abs/1703.05243">Kai Zhen, Mridul Birla, David Crandall, Bingjing Zhang, Judy Qiu. Hybrid Supervised-unsupervised Image Topic Visualization with Convolutional Neural Network and LDA. </a>

-->


<!--<h3>Keyboard </h3>
<ul> <p> <li><p><a href="piano/dev/index.html">Kai Zhen and David Crandall. Finding egocentric image topics through convolutional neural
network based representations. In IEEE Conference on Computer Vision and Pattern Recognition
Workshop on Egocentric Computer Vision, Las Vegas, 2016. (Poster).</a>
</p></ul> 

<h3>Talks</h3>
<ul> <p> <li><p> Models of learning in machines and humans. Seminar in Cognitive Psychology: New Approaches from Cognitive Science to Learning and Education. Room 115 Psychology Building, 1101 E. 10th St. Nov. 2nd, 2016
</p></ul> 
-->


<h3>In Submission</h3>
<ul>
<li><p>Kai Zhen, Jongmo Sung, Mi Suk Lee, Seungkwon Beack, and Minje Kim, "Efficient And Scalable Neural
Residual Waveform Coding with Collaborative Quantization" (submitted to ICASSP 2020).<br> <a href="speechcoding.html"> [Demo]</a>
 <li><p>Kai Zhen, Mi Suk Lee, Minje Kim, "A Dual-Staged Context Aggregation Method towards Efficient End-To-End Speech Enhancement" (submitted to ICASSP 2020). <br><a href="speechenhancement.html">  [Demo]</a>
</ul>

<h3> Peer Reviewed Conference Proceedings</h3>
<ul>
<li><p>Kai Zhen, Jongmo Sung, Mi Suk Lee, Seungkwon Beack, and Minje Kim, "Cascaded Cross-Module Residual Learning towards Lightweight End-to-End Speech Coding," In Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH'19), Graz, Austria, September 15-19, 2019.
 <br><a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1816.pdf"> [PDF]</a> <a href="pub/zhen2019cascaded.txt">[BibTex]</a> <a href="https://saige.sice.indiana.edu/research-projects/neural-audio-coding/">[Demo]</a>
</ul>
<h3> Patents</h3>
<ul>
<li><p>Minje Kim, Aswin Sivaraman, Kai Zhen , Jongmo Sung, et al, "Audio signal encoding method and apparatus
and audio signal decoding method and apparatus using psychoacoustic-based weighted error function". <br> <a href="https://patents.google.com/patent/US20190164052A1/en">[US Patent App. 16/122,708]</a>
<li><p>Minje Kim, Kai Zhen , Jongmo Sung, Mi Suk Lee, Seungkwon Beack, et al, "Method and Apparatus of
Cascaded Residual Learning Pipeline for Audio Coding," US Patent Application (pending), 2019
</ul>
<h3> Peer Reviewed Workshops And Forums</h3>
<ul>
  <li><p>Kai Zhen, Aswin Sivaraman, Jongmo Sung, Minje Kim. On Psychoacoustically Weighted Cost
Functions Towards Resource-efficient Deep Neural Networks for Speech Denoising. The 7th Annual Midwest Cognitive
Science Conference, 2018. <br> <a href="https://arxiv.org/pdf/1801.09774.pdf">[PDF]</a> <a href="pub/zhen2018psychoacoustically.txt">[BibTex]</a> 

<li><p><a href="http://cencia.gsu.edu/improvising-brain-iii/">Peter Miksza, Kevin Watson, Kai Zhen, Sanna Wager, Minje Kim. Relationships between experts'
subjective ratings of jazz improvisations and computational measures of melodic entropy. In data
analysis phase. Paper presented at the Improvising Brain III: Cultural Variation and Analytical
Techniques Symposium, Atlanta, GA, in Feb, 2017.</a>
<li><p><a href="pub/egocentric_topic_abstract.pdf">Kai Zhen and David Crandall. Finding egocentric image topics through convolutional neural
network based representations. In IEEE Conference on Computer Vision and Pattern Recognition
Workshop on Egocentric Computer Vision, Las Vegas, 2016. (Poster).</a>
</ul>
<!--<li><p><a href="https://cocosci.github.io/LDA/">Learning graphical model (probabilistic inference in Bayesian network) through Latent Dirichlet Allocation, 2016 Spring. </a><a href="https://cocosci.github.io/LDA_JAVA/">A Java version of naive LDA is also provided:)</a>
  
  <li><p><a href="https://cocosci.github.io/MPIPagerank/">PageRank algorithm in parallel via MPJ-Express open library, 2016 Fall.</a>
</ul>


 <li><p><a href="pub/egocentric_topic_abstract.pdf">Kai Zhen and David Crandall. Finding egocentric image topics through convolutional neural
network based representations. In IEEE Conference on Computer Vision and Pattern Recognition
Workshop on Egocentric Computer Vision, Las Vegas, 2016. (Poster).</a>
<li><p><a href="https://cocosci.github.io/LDA/">Learning graphical model (probabilistic inference in Bayesian network) through Latent Dirichlet Allocation, 2016 Spring. </a><a href="https://cocosci.github.io/LDA_JAVA/">A Java version of naive LDA is also provided:)</a>
  <li><p><a href="https://cocosci.github.io/MPIPagerank/">PageRank algorithm in parallel via MPJ-Express open library, 2016 Fall.</a>
</p> </ul>


<h3>Courses (AI and machine learning related) </h3>
<ul> 
  <li><p><a href="675/index.html">Statistical Learning by Prof. Trosset, 2015 Fall.</a>
  <li><p><a href="http://www.soic.indiana.edu/graduate/courses/index.html?number=b551&department=csci">Artificial Intelligence by Prof. Crandall, 2015 Fall.</a>
  <li><p><a href="http://mypage.iu.edu/~jbusemey/Q550/Q550.htm">Cognitive Modeling by Prof. Busemeyer, 2016 Fall.</a>
  <li><p><a href="http://salsahpc.indiana.edu/csci-b534-fall-2016/">Distributed System by Prof. Qiu, 2016 Fall.</a>
  <li><p><a href="http://www.soic.indiana.edu/graduate/courses/index.html?number=b565&department=csci">Data Mining by Prof. Raphael, 2017 Spring.</a>
  <li><p><a href="http://minjekim.com//">Machine Learning for Signal Processing by Prof. Minje Kim, 2017 Spring.</a>
  <li><p><a href="http://www.soic.indiana.edu/graduate/courses/index.html?number=b565&department=csci">Deep Learning System by Prof. Minje Kim, 2018 Spring.</a>
</p> </ul>
-->

<h3>Academic activities</h3>
<ul> 
  <li><p> I served as a reviewer of <a href="https://www.eurasip.org/"> ICASSP 2019, 2020</a>.
  <li><p> I served as a reviewer of <a href="https://www.eurasip.org/">EURASIP Journal on Advances in Signal Processing</a>.
  <li><p> I served as a sub-reviewer of <a href="https://www.aaai.org/Conferences/AAAI/aaai17.php">AAAI-2017, 2018</a>.
</p> </ul>



<!--
<h3>My Book Shelf </h3>
<h4>Nonparametric Bayesian methods </h4>
<ul> 

  <li><p><a href="http://ai.stanford.edu/~tadayuki/papers/miller-phd-dissertation11.pdf">Kurt Tadayuki Miller's dissertation with 175 pages on this topic, 2011.</a>
  <li><p><a href="http://mlg.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf">A tutorial by Zoubin Ghahramani, 2005.[slide]</a>
    <li><p><a href="http://gershmanlab.webfactional.com/pubs/GershmanBlei12.pdf">A tutorial of Bayesian Nonparametrics with CRP and IBP examples by David Blei, 2005.[paper]</a>
</p> </ul>
<h4>Graphical Models </h4>
<ul> 
<li><p><a href="http://www.cs.columbia.edu/~blei/fogm/2015F/notes/mixtures-and-gibbs.pdf">Bayesian Mixture Models and the Gibbs Sampler, David Blei, 2015.</a>

</p> </ul>
-->
<!--
<h3>Technical Interview Preparations </h3>
<ul> 

  <li><p><a href="https://cocosci.github.io/zanamo/">Feel hungry? Make your mouth occupied! Want an internship? Get your hands dirty!</a>
</p> </ul>

<h3>Body Weight </h3>
<ul> 

  <li><p><a href="index_bodyweight.html"> Stat'n'Curves  </a>
</p> </ul>
-->

<!--
<h3>I like singing.  </h3>
<ul> 
  <li><p> Feng Huang Yu Fei (a tragic love story in ancient China); recorded in Shuangjing, Beijing. 
</p> </ul>
<audio controls>
  <source src="horse.ogg" type="audio/ogg">
  <source src="audio/fenghuang_bj.wav" type="audio/mpeg">
</audio>

<ul> 
  <li><p> You and Me (2008 Olympic Theme Song); recorded in squash court 12 at SRSC. 
</p> </ul>
<audio controls>
  <source src="horse.ogg" type="audio/ogg">
  <source src="audio/youandme.wav" type="audio/mpeg">
</audio>


<ul> 
  <li><p> Blossom in Winter; recorded at home. 
</p> </ul>
<audio controls>
  <source src="horse.ogg" type="audio/ogg">
  <source src="audio/yijianmei.wav" type="audio/mpeg">
</audio>

<ul> 
  <li><p> New York, New York; performed at a thanksgiving party. 
</p> </ul>
<audio controls>
  <source src="horse.ogg" type="audio/ogg">
  <source src="audio/nyny.wav" type="audio/mpeg">
</audio>
<ul> 
  <li><p> Glory Fades (a song for the Young Marshall); recorded in squash court 12 at SRSC.
</p> </ul>
<audio controls>
  <source src="horse.ogg" type="audio/ogg">
  <source src="audio/zaicike.wav" type="audio/mpeg">
</audio>


<ul> 
  <li><p> A Blooming Flower (Youth); recorded at home.
</p> </ul>
<audio controls>
  <source src="horse.ogg" type="audio/ogg">
  <source src="audio/xiaohua.wav" type="audio/mpeg">
</audio>
</audio>

<ul> 
  <li><p> New Day Has Come (Xiang Lian, a pop song in 1980s when people started to embrace new lives); recorded at home.
</p> </ul>
<audio controls>
  <source src="horse.ogg" type="audio/ogg">
  <source src="audio/xianglian.wav" type="audio/mpeg">
</audio>


<h3>Just came back from Miami and almost missed the very moment of the year in IU.</h3>
<div style="width:500px;height:500px;text-align:center;margin:auto;" >
<a data-flickr-embed="true"  href="https://www.flickr.com/photos/160081194@N02/albums/72157700075376762" title="2018 Fall"><img src="https://farm5.staticflickr.com/4878/45717653552_e997ba0353_z.jpg" width="480" height="640" alt="2018 Fall"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>
-->
<!--<a data-flickr-embed="true"  href="https://www.flickr.com/photos/160081194@N02/albums/72157668400301738" title="2018&#x27; Spring"><img src="https://farm5.staticflickr.com/4524/24717907928_408105ac6e.jpg" width="375" height="500" alt="2017&#x27; Fall"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script></div>-->


<!--<h3>Audio </h3>
<ul> 

  <li><p><a href="audio.js"> VocalVocal!! </a>
</p> </ul>




<h3>IU Gallery </h3>
<p>IU soothes me in working hard to get closer to my heart. You may check the <a href="Gallery/index.html" target="_blank">IU Gallery</a> to see a small subset of my photo collections.</p>

<h3>Boost Your TOEFL Writing! </h3>
<html>
 <head>
  <script type="text/javascript">
function functionOne() { alert('You clicked the top text'); }
function functionTwo() { alert('You clicked the bottom text'); }
  </script>
 </head>
<body>
 <p><a href="#" onClick="functionOne();">Top Text</a></p>
 <p><a href="javascript:functionTwo();">Bottom Text</a></p>
 </body>
</html>

<html>
    <head>
        <script src="toefl.js"></script>
    </head>
</html>


-->
<h3>Visitor Stats  </h3>
<!-- GoStats Simple HTML Based Code <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=CED9&d=q-glD7r36S7FadlnyzQ4gHE0XpUsa2F29LAAnJ8bj0U"></script>-->

<!-- GoStats Simple HTML Based Code -->
<!--<a target="_blank" title="web site counter" href="http://gostats.com"><img alt="web site counter" 
src="http://monster.gostats.com/bin/count/a_479073/t_4/i_1/z_0/show_hits/counter.png" 
style="border-width:0" /></a>-->
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=jGdG1k5xplNC_7uIAOKPmrstD74VXasxYxZ3i0GfUlk'></script>
<!-- End GoStats Simple HTML Based Code 


<div id="clustrmaps-widget"></div><script type="text/javascript">var _clustrmaps = {'url' : 'http://pages.iu.edu/~zhenk/', 'user' : 1176291, 'server' : '3', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2015-08-12', 'lang' : 'en', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www3.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</script><noscript><a href="http://www3.clustrmaps.com/user/4fc11f2e3"><img src="http://www3.clustrmaps.com/stats/maps-no_clusters/pages.iu.edu-~zhenk--thumb.jpg" alt="Locations of visitors to this page" /></a></noscript>-->

</body>
</html>
