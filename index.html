



<!--<p><a href="home.html" target="_blank">Street view outside my home.</a></p>-->


<!-- <script type="text/python" src="hello.py"></script>


<p><a href="hello.py" target="_blank">hello</a></p>


<p><a href="http://pages.iu.edu/~zhenk/Snip20150809_5.png" target="_blank">imaggge</a></p>





<a href="http://shijue.me/zone/contest/formula/about" target="_blank">history</a>


-->


<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>


<script>
* {
  box-sizing: border-box;
}

.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</script>

<link rel="shortcut icon" href="images/pumpkin.jpg">
<!--<link rel="shortcut icon" href="favicon3.ico" type="image/x-icon" >-->
  <title>Kai Zhen - Ph.D. Candidate at Indiana University</title>

  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="description" content="Ph.D. Candidate in Computer Science and Cognitive Science at Indiana University. Future Applied Scientist at Amazon. Proud to Be A Hoosier.">
  <meta name="viewport" content="width=device-width, initial-scale=1">


	<link rel="stylesheet" href="assets/css/arlington.css" />
<link rel="stylesheet" href="assets/css/washingtonpost.css" />
<link rel="stylesheet" href="assets/css/sub.css" />






<!--
<link rel="stylesheet"  media="screen and (max-width: 1680px)" href="assets/css/sub.css" />
<link rel="stylesheet" href="css/skel.css" />
<link rel="stylesheet" href="css/style.css" />
<link rel="stylesheet" href="css/style-desktop.css" />
<link rel="stylesheet" type="text/css" href="assets/css/fibonacci.css">
<link rel="stylesheet" href="assets/css/sub.css" />


	<link rel="stylesheet"
	      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
</head>

-->
  <div align="center">
    <img src="head_2021.jpg" style="width: 55vw; min-width: 330px;">
     <SPAN STYLE="font-size:.0pt"><BR></SPAN>
    <b><font size="6">Kai Zhen</font></b>
  </div>






<br>
<div id="container">
<div id="home_target" style="position: relative; top: -50px;"></div>
<p>
	<b><font size="4">About Me</font></b><br><br>
	I'm finishing my PhD in computer science and cognitive science at Indiana University. My main focus is on <a href="http://www.google.com/search?q=Neural+Waveform+Coding"> <b>neural speech/audio coding</b></a>, aside from <b> monaural speech enhancement</b> and <b>model pruning</b> for ASR.<br>





	<!--People are less impressed than they used to be, if ever, by a new benchmark.

During my Ph.D. study with Minje's supervision, I proposed a very compact neural waveform codec operating at a wide bitrate range with comparable or superior performance
	to standard codecs. Comparing to auto-regressive neural decoders, our decoder, still a neural network though, is ~100X smaller. Oh, it functions almost in real-time even on a single-core CPU.



	Ph.D. Candidate at Indiana University-->
	<!--<font size="3">Research Interests:</font> My main focus is on <a href="http://www.google.com/search?q=Neural+Waveform+Coding"> <b>neural speech/audio coding</b></a>, aside from <b> monaural speech enhancement</b> and <b>model pruning</b> for ASR.<br>-->
	<font size="3">Research Group:</font> <a href="http://saige.sice.indiana.edu/"> <b>Signals & Artificial Intelligence Group in Engineering (SAIGE) </b></a><br>
  <font size="3">Dissertation Committee:</font> <a href="https://minjekim.com/">Minje Kim (Chair)</a>, <a href="https://psych.indiana.edu/directory/faculty/goldstone-robert.html"> Robert Goldstone (Co-chair)</a>, <a href="https://sphsc.washington.edu/content/yi-shen">Yi Shen</a>, <a href="http://homes.sice.indiana.edu/williads/">Donald Williamson</a><br>
<!--
	<blockquote>
	<i>
		<font size="3">Neural Waveform Coding: Scalability, Efficiency and Psychoacoustic Calibration</font><br><br>
	Waveform coding, an indispensable speech/audio processing component, serves a critical role in data storage and communication. A waveform codec converts a speech/audio waveform into a highly compact bitstream, to be transmitted to the receiver side where the waveform is reconstructed with the least possible perceptual discrepancy from the uncompressed counterpart.<br><br>
	Since waveform codecs are usually deployed on low-power devices for real-time communications, recent data-driven approaches may not be feasible in practice. For example, the user-perceived latency can go beyond tolerance along with the overwhelming runtime complexity, which can be energy-consuming. <br><br>
	The dissertation introduces a scalable and efficient methodology to compress speech and audio waveforms. While reflecting upon recent advancements in deep learning, it integrates knowledge and techniques from digital signal processing and psychoacoustics into the model design and training procedure: derived from multistage vector quantization (MSVQ), our method achieves scalability via cascaded cross-module residual learning (CMRL); it also leverages the speech production theory where linear predictive coding (LPC) is remodeled as a trainable and efficient preprocessor for spectral envelope estimation; we also bring psychoacoustics to the loss function leading to noticeable performance gains for audio coding. Our coding performance can be scaled up to a transparent level superior to that from conventional codecs. Compared with recent neural network based generative models, our model size is significantly smaller.
	</i>
	</blockquote>
	-->

	<font size="3">Email:</font> ZHenK.IU@EdU (2 characters are to be swapped)<br>
  <!--<font size="3"><a href="cv.pdf">Curriculum Vitae</a></font>-->
  
  <!--Mail: Department of Psychological and Brain Sciences, Room 601K. 1001 E 10th St, Bloomington, IN 47408
<br>
  <a href="Papers/KAI-NOV-2015.pdf">Curriculum Vitae (as of Nov 2015)</a> <br>-->
<!--<br>
  Currently, I am open to internship opportunities. <a href="Resume9516.pdf">Check out my resume (as of September 2016).</a> <br> -->
</p>

</div>
              <!--I'm in my final year of the Ph.D. program, advised by <a href="https://minjekim.com/">Prof. Minje Kim</a> at Indiana University.              
              My research is mainly on scalable and efficient speech and audio coding via techniques spanning the traditional digital signal processing domain and modern computational paradigm.
              <!--In my view, innovation does not only come from proposing brand new methodologies but reviving the conventional techniques as well.
              This is reflected in most of my projects as an joint effort spanning the traditional digital signal processing domain and modern computational paradigm.-->
              <!--The six-year journey has also endowed me with opportunities including working with great companies such as LinkedIn and Amazon. My next play will be at Amazon Alexa as an applied scientist.               
              <p>Check my <a href="http://kaizhen.us/cv.pdf">CV</a> for more details.</p>-->

<!--
<div id="Updates" style="position: relative; top: -50px;"></div>
<p>

<b><font size="4">Moment & Momentum (Archived)</font></b> <br>


<p align="justify">
 Ending yet
</p>


<p align="justify">
 Amazon adventure
</p>

<p align="justify">
 My qualifying project on psychoacoustics has turned into my first journal article.
</p>

<p align="justify">
 Return offer to the Bay area: the big tech hub.
</p>

<p align="justify">
 First accepted conference paper in speech coding/ Graz
</p>

<p align="justify">
 Qualifying exam
</p>

<p align="justify">
 Interned at LinkedIn Corporation which is hosted in the iconic Empire State Building.
</p>

<p align="justify">
 Landed on the United States of America.
</p>

</p>

</div>
-->


	<!--
<div style="margin-left: 350px;">
  <span class="dot" onclick="currentSlide(1)"></span>
  <span class="dot" onclick="currentSlide(2)"></span>
  <span class="dot" onclick="currentSlide(3)"></span>
</div>

<div class="slideshow-container">

  <div class="mySlides fade">
    <div class="numbertext">1 / 3</div>
	  <a href="default.asp">

		  <a href="dccrn.html"> <img src="assets/dccrn-2.png" style="width: 55vw; min-width: 130px;"></a>
		  <a href="dccrn.html"> <figcaption style="margin-left: 50px;">Context aggregation with dilated CNN is ubiquitous: check out our hybrid model with only 1.38M parameters.</figcaption></a>
	  </a>

  </div>

  <div class="mySlides fade">
    <div class="numbertext">2 / 3</div>
	  <a href="neural-audio-coding.html"> <img src="assets/audio_coding_artifact.gif" style="width: 55vw;min-width: 130px;">
    <figcaption style="margin-left: 50px;">Artifacts can be harmless if they are inaudible. Can psychoacoustics be leveraged in contemporary models? <br> Check out our  neural audio coding project!</figcaption></a>
  </div>

  <div class="mySlides fade">
    <div class="numbertext">3 / 3</div>
    <a href="default.asp">
		<a href="https://saige.sice.indiana.edu/research-projects/neural-audio-coding/"><img src="assets/nsc.png" style="width: 55vw;min-width: 130px;">
		 <figcaption style="margin-left: 50px;margin-top: 10px;">Efficient neural speech coding: see what we can do with a tiny neural codec with 0.35M parameters!</figcaption></a>
	</a>
    <div class="text">Caption Three</div>
	  <script>
currentSlide(2)
		</script>
</div>
<br>

<!-- The dots/circles -->

<div align="left">
	<a href="https://saige.sice.indiana.edu/research-projects/neural-audio-coding/"><img src="left.png" style="width: 35%;min-width: 330px"></a>
	<a href="http://www.kaizhen.us/neural-audio-coding.html"><img src="mid.png" style="width: 25%;min-width: 330px"></a>
	<a href="http://kaizhen.us/dccrn.html"><img src="right.png" style="width: 35%;min-width: 330px"></a>
  </div>


<b><font size="4">Publications</font></b> 
<br>
<em>
<b class="label label-journal">Journal Articles </b>&nbsp
<b class="label label-conference">Conference Proceedings </b>&nbsp
<b class="label label-patent">Patents </b>&nbsp
<b class="label label-workshop">Workshop Papers </b>
</em>
<br><br>

<b><font size="3">===2021===</font></b><br><br>

<p align="justify">

 <b class="label label-conference">C-005</b>  <b>Kai Zhen</b>, Hieu Duy Nguyen, Feng-Ju (Claire) Chang, Athanasios Mouchtaris, and Ariya Rastrow, "<b>Sparsification via Compressed Sensing for Automatic Speech Recognition</b>," <i>in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>,Toronto, ON, Canada, June 6-12, 2021. <br>
<!--<a href="http://kaizhen.us/pub/cs-poster.png">[poster]</a>-->
	<a href="https://assets.amazon.science/d1/3e/1301067541d0ac20abbcfe0d93d5/sparsification-via-compressed-sensing-for-automatic-speech-recognition.pdf">[pdf]</a> <br><i>&#42;from the summer internship with Amazon</i>
</p>

<b class="label label-conference">C-004</b>  Haici Yang, <b>Kai Zhen</b>, Seungkwon Beack, Minje Kim, "<b>Source-Aware Neural Speech Coding for Noisy Speech Compression</b>," <i>in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>,Toronto, ON, Canada, June 6-12, 2021. <br>
<a href="https://arxiv.org/pdf/2008.12889.pdf">[pdf]</a>
</p>

<br><b><font size="3">===2020===</font></b><br><br>

<p align="justify">
	 <b class="label label-journal">J-001</b> <b>Kai Zhen</b>, Mi Suk Lee, Jongmo Sung, Seungkwon Beack, and Minje Kim, "<b>Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding</b>," <i>IEEE Signal Processing Letters, vol. 27, pp. 2159-2163, 2020, doi: 10.1109/LSP.2020.3039765.</i>.
<br>
<a href="http://kaizhen.us/neural-audio-coding.html">[demo]</a>
<a href="http://kaizhen.us/pub/zhenk-spl.pdf">[pdf]</a>
<a href="https://github.com/cocosci/pam-nac">[code]</a></p>


<p align="justify">
 <b class="label label-conference">C-003</b> <b>Kai Zhen</b>, Mi Suk Lee, Jongmo Sung, Seungkwon Beack, and Minje Kim, "<b>Efficient and Scalable Neural Residual Waveform Coding with Collaborative Quantization</b>," <i>in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, Barcelona, Spain, May 4-8, 2020. <br>
	<a href="http://kaizhen.us/collaborative-quantization.html">[demo]</a>
	<a href="http://kaizhen.us/pub/zhenk2020cq.pdf">[pdf]</a>
	<!--<a href="http://kaizhen.us/pub/cq.txt">[bib]</a>-->
	<a href="https://github.com/cocosci/NSC">[code]</a></p>

<p align="justify">
 <b class="label label-conference">C-002</b> <b>Kai Zhen</b>, Mi Suk Lee, Minje Kim. "<b>A Dual-Staged Context Aggregation Method towards Efficient End-To-End Speech Enhancement</b>,"  <i>in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, Barcelona, Spain, May 4-8, 2020. <br>
	<a href="http://kaizhen.us/speechenhancement.html">[demo]</a>
	<a href="http://kaizhen.us/pub/zhenk2020dccrn.pdf">[pdf]</a>
<!--	<a href="http://kaizhen.us/pub/dccrn.txt">[bib]</a>-->
</p>



 <p align="justify">

 <b class="label label-patent">P-004</b>  Minje Kim, <b>Kai Zhen</b>, Seungkwon Beack, et al, "<b>Collaborative quantization for efficient and scalable neural waveform coding</b>," <i>US Patent Application, US 2020</i>.
</p>

<p align="justify">
 <b class="label label-patent">P-003</b> Minje Kim, <b>Kai Zhen</b>, Mi Suk Lee, "<b>Apparatus and Method for Speech Processing Using a Densely Connected Hybrid Neural Network</b>," <i>US Patent Application, 2020</i>.
</p>

<p align="justify">
 <b class="label label-patent">P-002</b> Minje Kim, <b>Kai Zhen</b>, Seungkwon Beack, et al, "<b>Audio Signal Encoding Method and Audio Signal Decoding Method, And Encoder And Decoder Performing the Same</b>," <i>US Patent Application, US20200135220A1</i>.
</p>

<p align="justify">
 <b class="label label-workshop">W-004</b> <b>Kai Zhen</b>, Hieu Duy Nguyen, Feng-Ju (Claire) Chang, Athanasios Mouchtaris. <b>Network Sparsification for On-Device ASR</b>. <i>Amazon Machine Learning Conference (AMLC) Workshop on Network Inference Optimization, 2020.</i></a>
</p>


<br><b><font size="3">===2019 and earlier===</font></b><br><br>

<p align="justify">
 <b class="label label-conference">C-001</b> <b>Kai Zhen</b>, Jongmo Sung, Mi Suk Lee, Seungkwon Beack, and Minje Kim, "<b>Cascaded Cross-Module Residual Learning towards Lightweight End-to-End Speech Coding</b>," <i>In Proc. Annual Conference of the International Speech Communication Association (Interspeech)</i>, Graz, Austria, September 15-19, 2019. <br>
	<a href="https://saige.sice.indiana.edu/research-projects/neural-audio-coding/">[demo]</a>
	<a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1816.pdf">[pdf]</a>
	<!--<a href="http://kaizhen.us/pub/cmrl.txt">[bib]</a>-->
 </p>


<p align="justify">
 <b class="label label-patent">P-001</b> Minje Kim, Aswin Sivaraman, <b>Kai Zhen</b>, Jongmo Sung, et al, "<b>Audio signal encoding method and apparatus and audio signal decoding method and apparatus using psychoacoustic-based weighted error function</b>," <i>US Patent Application, US20190164052A1</i>.
</p>


<p align="justify">
 <b class="label label-workshop">W-003</b> <b>Kai Zhen</b>, Aswin Sivaraman, Jongmo Sung, Minje Kim, "<b>On Psychoacoustically Weighted Cost Functions Towards Resource-efficient Deep Neural Networks for Speech Denoising</b>," <i>The 7th Annual Midwest Cognitive Science Conference</i>, Bloomington, IN, 2018.<br>
	<a href="https://arxiv.org/pdf/1801.09774.pdf">[pdf]</a>

</p>

<p align="justify">
 <b class="label label-workshop">W-002</b> Peter Miksza, Kevin Watson, <b>Kai Zhen</b>, Sanna Wager, Minje Kim, "<b>Relationships between experts' subjective ratings of jazz improvisations and computational measures of melodic entropy</b>," <i>The Improvising Brain III: Cultural Variation and Analytical Techniques Symposium</i>, Atlanta, GA, in Feb, 2017.
</p>

<p align="justify">
 <b class="label label-workshop">W-001</b> <b>Kai Zhen</b> and David Crandall, "<b>Finding egocentric image topics through convolutional neural network based representations (extended abstract)</b>," <i>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Egocentric Computer Vision</i>, Las Vegas, US, June 26 - July 1, 2016. 
</p>







<br>
<b><font size="4">Positions Held</font></b><br><br>

<div class="row">
<!-- <div class="0.3u"> </div> -->
<div class="3u" align="center">
	<img src="assets/iu.png" border="0" width="50%">
</div>
<div class="9u">

<ul style="list-style-type: disc; margin-left: -2em;"> 
	<li><a href="http://indiana.edu/" target="_blank">Indiana University</a>: <b>Research Assistant</b>
	<ul style="list-style-type: circle; margin-left: 2em;"> 
		<li>Jan. 2018 - present</li>
		<li>Project: efficient end-to-end neural audio coding system</li>
		<li>Research group: <a href="http://saige.sice.indiana.edu/" target="_blank">Signals and AI Group in Engineering (SAIGE)</a></li>
	</ul>
</li></ul>	

<ul style="list-style-type: disc; margin-left: -2em;"> 
<li><a href="http://indiana.edu/" target="_blank">Indiana University</a>: <b>Teaching Assistant</b>
	<ul style="list-style-type: circle; margin-left: 2em;"> 
		<li>Aug. 2015 - Dec. 2017</li>
		<li><a href="https://cs.indiana.edu/" target="_blank">Department of Computer Science</a> <br>(CSCI-C 343, CSCI-B 551, CSCI-B 657)</li>
		<li><a href="https://engineering.indiana.edu/" target="_blank">Intelligent Systems Engineering Department</a> <br>(ENGR E511, ENGR E533)</li>
	</ul>
</li></ul>
</div>
</div>



<div class="row">
<!-- <div class="0.3u"> </div> -->
<div class="3u" align="center">
	<img src="assets/amazon.png" border="0" width="50%">
</div>
<div class="9u">
<ul style="list-style-type: disc; margin-left: -2em;"> 
	<li><a href="https://developer.amazon.com/en-US/alexa/science" target="_blank">Amazon</a>: <b>Applied Scientist Intern</b></li>
	<ul style="list-style-type: circle; margin-left: 2em;"> 
		<li>May. 2020 - Aug. 2020</li>						
		<li>Alexa Edge ML team</li>
		<li>Mentor: Hieu Duy Nguyen, Feng-ju (Claire) Chang</li>
		<li>Manager: Athanasios Mouchtaris</li>
		<li>Project: network compression for on-device ASR solutions</li>	
</li>							
						
	</ul>
</ul>	
</div>
</div>


<div class="row">
<!-- <div class="0.3u"> </div> -->
<div class="3u" align="center">
	<img src="assets/linkedin.svg" border="0" width="50%">
</div>
<div class="9u">
<ul style="list-style-type: disc; margin-left: -2em;"> 
	<li><a href="https://economicgraph.linkedin.com/research#all" target="_blank">LinkedIn Corporation</a>: <b>Machine Learning & Relevance Intern</b>
	<ul style="list-style-type: circle; margin-left: 2em;"> 
		<li>May. 2019 - Aug. 2019, Mountain View, CA </li>						
		<li>Ads AI group</li>							
		<li>Mentor: Lijun Peng, Hiroto Udagawa</li>		
		<li>Manager: Sara Smoot</li>							
		<li>Project: ads response rate prediction in wide-n-deep estimators and BERT</li>	
	</ul>
</li></ul>	
</div>
</div>

<div class="row">
<!-- <div class="0.3u"> </div> -->
<div class="3u" align="center">
	<img src="assets/linkedin.svg" border="0" width="50%">
</div>
<div class="9u">
<ul style="list-style-type: disc; margin-left: -2em;"> 
	<li><a href="https://economicgraph.linkedin.com/research#all" target="_blank">LinkedIn Corporation</a>: <b>Machine Learning & Relevance Intern</b>
	<ul style="list-style-type: circle; margin-left: 2em;"> 
		<li>May. 2018 - Aug. 2018, New York City, NY </li>						
		<li>Company standardization group</li>							
		<li>Mentor: Deirdre Hogan</li>	
		<li>Manager: Xiaoqiang Luo</li>	
		<li>Project: relevance ranking for resume builder with deep neural networks</li>	
	</ul>
</li></ul>	
</div>
</div>

<hr>




<div align="center">
<section id="custom_html-7" class="widget_text widget widget_custom_html"><div class="widget_text widget-wrap"><b><font size="4">Visitor Statistics</font></b><div class="textwidget custom-html-widget">
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=jGdG1k5xplNC_7uIAOKPmrstD74VXasxYxZ3i0GfUlk"></script></div></div>
</section>
</div>
 <footer> <small>&copy; Copyright 2021, Kai Zhen</small> </footer>

<!--

<p>
  Email: zhenk@iu.edu  <br>
  Address: 700 N. Woodlawn Ave. Luddy Hall, Bloomington, Indiana, 47404.
</p>
	-->



</body>
</html>
