@INPROCEEDINGS{9054434,
author={H. {Helmholz} and J. {Ahrens} and D. L. {Alon} and S. V. {Amengual Garí} and R. {Mehra}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Evaluation of Sensor Self-Noise In Binaural Rendering of Spherical Microphone Array Signals},
year={2020},
volume={},
number={},
pages={161-165},
abstract={Spherical microphone arrays are used to capture spatial sound fields, which can then be rendered via headphones. We use the Real-Time Spherical Array Renderer (ReTiSAR) to analyze and auralize the propagation of sensor self-noise through the processing pipeline. An instrumental evaluation confirms a strong global influence of different array and rendering parameters on the spectral balance and the overall level of the rendered noise. The character of the noise is direction independent in the case of spatially uniformly distributed noise. However, timbre of the rendered self-noise changes with head orientation in the case of spatially non-uniform noise. We determine audibility thresholds of the coloration artifact during head rotations for different array configurations in a perceptual user study.},
keywords={Sensor self-noise;Spherical microphone arrays;Binaural rendering;Real-time signal processing},
doi={10.1109/ICASSP40776.2020.9054434},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053613,
author={A. {Kumar} and V. K. {Ithapu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={SeCoST:: Sequential Co-Supervision for Large Scale Weakly Labeled Audio Event Detection},
year={2020},
volume={},
number={},
pages={666-670},
abstract={Weakly supervised learning algorithms are critical for scaling audio event detection to several hundreds of sound categories. Such learning models should not only disambiguate sound events efficiently with minimal class-specific annotation but also be robust to label noise, which is more apparent with weak labels instead of strong annotations. In this work, we propose a new framework for designing learning models with weak supervision by bridging ideas from sequential learning and knowledge distillation. We refer to the proposed methodology as SeCoST (pronounced Sequest) — Sequential Co-supervision for training generations of Students. SeCoST incrementally builds a cascade of student-teacher pairs via a novel knowledge transfer method. Our evaluations on Audioset (the largest weakly labeled dataset available) show that SeCoST achieves a mean average precision of 0.383 while outperforming prior state of the art by a considerable margin.},
keywords={Audio Event Detection;Teacher Student Models;Weakly Labeled;Sequential Learning},
doi={10.1109/ICASSP40776.2020.9053613},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053652,
author={J. {Yang} and J. {Bingham}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Environment-Aware Reconfigurable Noise Suppression},
year={2020},
volume={},
number={},
pages={3042-3046},
abstract={The paper proposes an efficient, robust, and reconfigurable technique to suppress various types of noises for any sampling rate. The theoretical analyses, subjective and objective test results show that the proposed noise suppression (NS) solution significantly enhances the speech transmission index (STI), speech intelligibility (SI), signal-to-noise ratio (SNR), and subjective listening experience. The STI and SI consists of 5 levels, i.e., bad, poor, fair, good, and excellent. The most common noisy condition is of SNR ranging from -5 to 8 dB. For the input SNR between -5 and 2.5 dB, the proposed NS improves the STI and SI from "fair" to "good". For the input SNR between 2.5 to 8 dB, the STI and SI are improved from "good" to "excellent" by the proposed NS. The proposed NS can be adopted in both capture and playback paths for voice over internet protocol, voice-trigger, and automatic speech recognition applications.},
keywords={Noise suppression;noise estimation;speech intelligibility;voice over internet protocol;automatic speech recognition},
doi={10.1109/ICASSP40776.2020.9053652},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053451,
author={A. {Gamage} and E. {Chien} and J. {Peng} and O. {Milenkovic}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-MotifGAN (MMGAN): Motif-Targeted Graph Generation And Prediction},
year={2020},
volume={},
number={},
pages={4182-4186},
abstract={Generative graph models create instances of graphs that mimic the properties of real-world networks. Generative models are successful at retaining pairwise associations in the underlying networks but often fail to capture higher-order connectivity patterns known as network motifs. Different types of graphs contain different network motifs, an example of which are triangles that often arise in social and biological networks. It is hence vital to capture these higher-order structures to simulate real-world networks accurately. We propose Multi-MotifGAN (MMGAN), a motif-targeted Generative Adversarial Network (GAN) that generalizes the benchmark NetGAN approach. The generalization consists of combining multiple biased random walks, each of which captures a different motif structure. MMGAN outperforms NetGAN at creating new graphs that accurately reflect the network motif statistics of input graphs such as Citeseer, Cora and Facebook.},
keywords={Generative adversarial networks;Higher-order networks;Multi-view graphs;Network motifs},
doi={10.1109/ICASSP40776.2020.9053451},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053171,
author={R. {Cutler} and R. {Mehran} and S. {Johnson} and C. {Zhang} and A. {Kirk} and O. {Whyte} and A. {Kowdle}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multimodal Active Speaker Detection and Virtual Cinematography for Video Conferencing},
year={2020},
volume={},
number={},
pages={4527-4531},
abstract={Active speaker detection (ASD) and virtual cinematography (VC) can significantly improve the experience of a video conference by automatically panning, tilting and zooming of a camera: subjectively users rate an expert video cinematographer significantly higher than the unedited video. We describe a new automated ASD and VC that performs within 0.3 MOS of an expert cinematographer based on subjective ratings with a 1-5 scale. This system uses a 4K wide-FOV camera, a depth camera, and a microphone array, extracts features from each modality and trains an ASD using an AdaBoost machine learning system that is very efficient and runs in real-time. A VC is similarly trained using machine learning. To avoid distracting the room participants the system has no moving parts – the VC works by cropping and zooming the 4K wide-FOV video stream. The system was tuned and evaluated using extensive crowdsourcing techniques and evaluated on a system with N=100 meetings, each 25 minutes in length.},
keywords={Active speaker detection;virtual cinematography;video conferencing;machine learning;computer vision;sound source localization;multimodal fusion;crowdsourcing},
doi={10.1109/ICASSP40776.2020.9053171},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054607,
author={E. {Nachmani} and L. {Wolf}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Gated Hypernet Decoder for Polar Codes},
year={2020},
volume={},
number={},
pages={5210-5214},
abstract={Hypernetworks were recently shown to improve the performance of message passing algorithms for decoding error correcting codes. In this work, we demonstrate how hypernet-works can be applied to decode polar codes by employing a new formalization of the polar belief propagation decoding scheme. We demonstrate that our method improves the previous results of neural polar decoders and achieves, for large SNRs, the same bit-error-rate performances as the successive list cancellation method, which is known to be better than any belief propagation decoders and very close to the maximum likelihood decoder.},
keywords={Error correcting codes;polar codes;belief propagation;hypernetworks},
doi={10.1109/ICASSP40776.2020.9054607},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053872,
author={X. {Zhang} and D. {Povey} and S. {Khudanpur}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={OOV Recovery with Efficient 2nd Pass Decoding and Open-vocabulary Word-level RNNLM Rescoring for Hybrid ASR},
year={2020},
volume={},
number={},
pages={6334-6338},
abstract={In this paper, we investigate out-of-vocabulary (OOV) word recovery in hybrid automatic speech recognition (ASR) systems, with emphasis on dynamic vocabulary expansion for both Weight Finite State Transducer (WFST)-based decoding and word-level RNNLM rescoring. We first describe our OOV candidate generation method based on a hybrid lexical model (HLM) with phoneme-sequence constraints. Next, we introduce a framework for efficient second pass OOV recovery with a dynamically expanded vocabulary, showing that, by calibrating OOV candidates’ language model (LM) scores, it significantly improves OOV recovery and overall decoding performance compared to HLM-based first pass decoding. Finally we propose an open-vocabulary word-level recurrent neural network language model (RNNLM) re-scoring framework, making it possible to re-score ASR hypotheses containing recovered OOVs, using a single word-level RNNLM ignorant of OOVs when it was trained. By evaluating OOV recovery and overall decoding performance on Spanish/English ASR ‘tasks, we show the proposed OOV recovery pipeline has the potential of an efficient open-vocab word-based ASR decoding framework, with minimal extra computation versus a standard WFST based decoding and RNNLM rescoring pipeline.},
keywords={OOV recovery;RNNLM rescoring;dynamic vocabulary},
doi={10.1109/ICASSP40776.2020.9053872},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054257,
author={D. {Le} and T. {Koehler} and C. {Fliegen} and M. L. {Seltzer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={G2G: TTS-Driven Pronunciation Learning for Graphemic Hybrid ASR},
year={2020},
volume={},
number={},
pages={6869-6873},
abstract={Grapheme-based acoustic modeling has recently been shown to outperform phoneme-based approaches in both hybrid and end-to-end automatic speech recognition (ASR), even on non-phonemic languages like English. However, graphemic ASR still has problems with low-frequency words that do not follow the standard spelling conventions seen in training, such as entity names. In this work, we present a novel method to train a statistical grapheme-to-grapheme (G2G) model on text-to-speech data that can rewrite an arbitrary character sequence into more phonetically consistent forms. We show that using G2G to provide alternative pronunciations during decoding reduces Word Error Rate by 3% to 11% relative over a strong graphemic baseline and bridges the gap on rare name recognition with an equivalent phonetic setup. Unlike many previously proposed methods, our method does not require any change to the acoustic model training procedure. This work reaffirms the efficacy of grapheme-based modeling and shows that specialized linguistic knowledge, when available, can be leveraged to improve graphemic ASR.},
keywords={graphemic pronunciation learning;hybrid speech recognition;chenones;acoustic modeling},
doi={10.1109/ICASSP40776.2020.9054257},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054345,
author={Y. {Wang} and A. {Mohamed} and D. {Le} and C. {Liu} and A. {Xiao} and J. {Mahadeokar} and H. {Huang} and A. {Tjandra} and X. {Zhang} and F. {Zhang} and C. {Fuegen} and G. {Zweig} and M. L. {Seltzer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Transformer-Based Acoustic Modeling for Hybrid Speech Recognition},
year={2020},
volume={},
number={},
pages={6874-6878},
abstract={We propose and evaluate transformer-based acoustic models (AMs) for hybrid speech recognition. Several modeling choices are discussed in this work, including various positional embedding methods and an iterated loss to enable training deep transformers. We also present a preliminary study of using limited right context in transformer models, which makes it possible for streaming applications. We demonstrate that on the widely used Librispeech benchmark, our transformer-based AM outperforms the best published hybrid result by 19% to 26% relative when the standard n-gram language model (LM) is used. Combined with neural network LM for rescoring, our proposed approach achieves state-of-the-art results on Librispeech. Our findings are also confirmed on a much larger internal dataset.},
keywords={hybrid speech recognition;acoustic modeling;transformer;recurrent neural networks},
doi={10.1109/ICASSP40776.2020.9054345},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052964,
author={A. {Tjandra} and C. {Liu} and F. {Zhang} and X. {Zhang} and Y. {Wang} and G. {Synnaeve} and S. {Nakamura} and G. {Zweig}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={DEJA-VU: Double Feature Presentation and Iterated Loss in Deep Transformer Networks},
year={2020},
volume={},
number={},
pages={6899-6903},
abstract={Deep acoustic models typically receive features in the first layer of the network, and process increasingly abstract representations in the subsequent layers. Here, we propose to feed the input features at multiple depths in the acoustic model. As our motivation is to allow acoustic models to re-examine their input features in light of partial hypotheses we introduce intermediate model heads and loss function. We study this architecture in the context of deep Transformer networks, and we use an attention mechanism over both the previous layer activations and the input features. To train this model's intermediate output hypothesis, we apply the objective function at each layer right before feature re-use. We find that the use of such iterated loss significantly improves performance by itself, as well as enabling input feature re-use. We present results on both Librispeech, and a large scale video dataset, with relative improvements of 10 - 20% for Librispeech and 3.2 - 13% for videos.},
keywords={transformer;deep learning;CTC;hybrid ASR},
doi={10.1109/ICASSP40776.2020.9052964},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053098,
author={Y. {Chen} and Z. {Yang} and C. {Yeh} and M. {Jain} and M. L. {Seltzer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Aipnet: Generative Adversarial Pre-Training of Accent-Invariant Networks for End-To-End Speech Recognition},
year={2020},
volume={},
number={},
pages={6979-6983},
abstract={As one of the major sources in speech variability, accents have posed a grand challenge to the robustness of speech recognition systems. In this paper, our goal is to build a unified end-to-end speech recognition system that generalizes well across accents. For this purpose, we propose a novel pre-training framework AIPNetbased on generative adversarial nets (GAN) for accent-invariant representation learning: Accent Invariant Pre-training Networks. We pre-train AIPNetto disentangle accent-invariant and accent-specific characteristics from acoustic features through adversarial training on accented data for which transcriptions are not necessarily available. We further fine-tune AIPNetby connecting the accent-invariant module with an attention-based encoder-decoder model for multi-accent speech recognition. In the experiments, our approach is compared against four baselines including both accent-dependent and accent-independent models. Experimental results on 9 English accents show that the proposed approach outperforms all the baselines by 2.3 ~ 4.5% relative reduction on average WER when transcriptions are available in all accents and by 1.6 ~ 6.1% relative reduction when transcriptions are only available in US accent.},
keywords={Generative adversarial network;end-to-end speech recognition;accent-invariance},
doi={10.1109/ICASSP40776.2020.9053098},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054295,
author={J. {Kahn} and A. {Lee} and A. {Hannun}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Self-Training for End-to-End Speech Recognition},
year={2020},
volume={},
number={},
pages={7084-7088},
abstract={We revisit self-training in the context of end-to-end speech recognition. We demonstrate that training with pseudo-labels can substantially improve the accuracy of a baseline model. Key to our approach are a strong baseline acoustic and language model used to generate the pseudo-labels, filtering mechanisms tailored to common errors from sequence-to-sequence models, and a novel ensemble approach to increase pseudo-label diversity. Experiments on the LibriSpeech corpus show that with an ensemble of four models and label filtering, self-training yields a 33.9% relative improvement in WER compared with a baseline trained on 100 hours of labelled data in the noisy speech setting. In the clean speech setting, self-training recovers 59.3% of the gap between the baseline and an oracle model, which is at least 93.8% relatively higher than what previous approaches can achieve.},
keywords={speech recognition;semi-supervised;deep learning},
doi={10.1109/ICASSP40776.2020.9054295},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054548,
author={M. {Rivière} and A. {Joulin} and P. {Mazaré} and E. {Dupoux}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unsupervised Pretraining Transfers Well Across Languages},
year={2020},
volume={},
number={},
pages={7414-7418},
abstract={Cross-lingual and multi-lingual training of Automatic Speech Recognition (ASR) has been extensively investigated in the supervised setting. This assumes the existence of a parallel corpus of speech and orthographic transcriptions. Recently, contrastive predictive coding (CPC) algorithms have been proposed to pretrain ASR systems with unlabelled data. In this work, we investigate whether unsupervised pretraining transfers well across languages. We show that a slight modification of the CPC pretraining extracts features that transfer well to other languages, being on par or even outperforming supervised pretraining. This shows the potential of unsupervised methods for languages with few linguistic resources.},
keywords={Unsupervised pretraining;low resources;cross-lingual},
doi={10.1109/ICASSP40776.2020.9054548},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053439,
author={W. {He} and L. {Lu} and B. {Zhang} and J. {Mahadeokar} and K. {Kalgaonkar} and C. {Fuegen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Spatial Attention for Far-Field Speech Recognition with Deep Beamforming Neural Networks},
year={2020},
volume={},
number={},
pages={7499-7503},
abstract={In this paper, we introduce spatial attention for refining the information in multi-direction neural beamformer for far-field automatic speech recognition. Previous approaches of neural beamformers with multiple look directions, such as the factored complex linear projection, have shown promising results. However, the features extracted by such methods contain redundant information, as only the direction of the target speech is relevant. We propose using a spatial attention subnet to weigh the features from different directions, so that the subsequent acoustic model could focus on the most relevant features for the speech recognition. Our experimental results show that spatial attention achieves up to 9% relative word error rate improvement over methods without the attention.},
keywords={Deep beamforming networks;multi-channel far-field speech recognition;array signal processing;attention},
doi={10.1109/ICASSP40776.2020.9053439},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052942,
author={J. {Kahn} and M. {Rivière} and W. {Zheng} and E. {Kharitonov} and Q. {Xu} and P. E. {Mazaré} and J. {Karadayi} and V. {Liptchinsky} and R. {Collobert} and C. {Fuegen} and T. {Likhomanenko} and G. {Synnaeve} and A. {Joulin} and A. {Mohamed} and E. {Dupoux}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Libri-Light: A Benchmark for ASR with Limited or No Supervision},
year={2020},
volume={},
number={},
pages={7669-7673},
abstract={We introduce a new collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio, which is, to our knowledge, the largest freely-available corpus of speech. The audio has been segmented using voice activity detection and is tagged with SNR, speaker ID and genre descriptions. Additionally, we provide baseline systems and evaluation metrics working under three settings: (1) the zero resource/unsupervised setting (ABX), (2) the semi- supervised setting (PER, CER) and (3) the distant supervision setting (WER). Settings (2) and (3) use limited textual resources (10 minutes to 10 hours) aligned with the speech. Setting (3) uses large amounts of unaligned text. They are evaluated on the standard LibriSpeech dev and test sets for comparison with the supervised state-of-the-art.},
keywords={unsupervised and semi-supervised learning;distant supervision;dataset;zero- and low resource ASR.},
doi={10.1109/ICASSP40776.2020.9052942},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054224,
author={A. {Baevski} and A. {Mohamed}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Effectiveness of Self-Supervised Pre-Training for ASR},
year={2020},
volume={},
number={},
pages={7694-7698},
abstract={We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] self-supervision approach to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on test-clean, while achieving a 25% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054224},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053527,
author={K. {Singh} and D. {Okhonko} and J. {Liu} and Y. {Wang} and F. {Zhang} and R. {Girshick} and S. {Edunov} and F. {Peng} and Y. {Saraf} and G. {Zweig} and A. {Mohamed}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Training ASR Models By Generation of Contextual Information},
year={2020},
volume={},
number={},
pages={7864-7868},
abstract={Supervised ASR models have reached unprecedented levels of accuracy, thanks in part to ever-increasing amounts of labelled training data. However, in many applications and locales, only moderate amounts of data are available, which has led to a surge in semi- and weakly-supervised learning research. In this paper, we conduct a large-scale study evaluating the effectiveness of weakly-supervised learning for speech recognition by using loosely related contextual information as a surrogate for ground-truth labels. For weakly supervised training, we use 50k hours of public English social media videos along with their respective titles and post text to train an encoder-decoder transformer model. Our best encoder-decoder models achieve an average of 20.8% WER reduction over a 1000 hours supervised baseline, and an average of 13.4% WER reduction when using only the weakly supervised encoder for CTC fine-tuning. Our results show that our setup for weak supervision improved both the encoder acoustic representations as well as the decoder language generation abilities.},
keywords={End-to-end ASR;Weak-supervision},
doi={10.1109/ICASSP40776.2020.9053527},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053406,
author={A. D. {McCarthy} and L. {Puzon} and J. {Pino}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Skinaugment: Auto-Encoding Speaker Conversions for Automatic Speech Translation},
year={2020},
volume={},
number={},
pages={7924-7928},
abstract={We propose autoencoding speaker conversion for training data augmentation in automatic speech translation. This technique directly transforms an audio sequence, resulting in audio thesized to resemble another speaker’s voice. Our method compares favorably to SpecAugment on English–French and English–Romanian automatic speech translation (AST) tasks as well as on a low-resource English automatic speech recognition (ASR) task. Further, in ablations, we show the benefits of both quantity and diversity in augmented data. Finally, we show that we can combine our approach with augmentation by machine-translated transcripts to obtain a competitive end-to-end AST model that outperforms a very strong cascade model on an English–French AST task. Our method is sufficiently general that it can be applied to other speech generation and analysis tasks.},
keywords={automatic speech translation;end-to-end speech translation;data augmentation;speaker normalization},
doi={10.1109/ICASSP40776.2020.9053406},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053399,
author={K. {Li} and Z. {Liu} and T. {He} and H. {Huang} and F. {Peng} and D. {Povey} and S. {Khudanpur}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Empirical Study of Transformer-Based Neural Language Model Adaptation},
year={2020},
volume={},
number={},
pages={7934-7938},
abstract={We explore two adaptation approaches of deep Transformer based neural language models (LMs) for automatic speech recognition. The first approach is a pretrain-finetune framework, where we first pretrain a Transformer LM on a large-scale text corpus from scratch and then adapt it to relatively small target domains via finetuning. The second approach is a mixer of dynamically weighted models that are separately trained on source and target domains, aiming to improve simple linear interpolation with dynamic weighting. We compare the two approaches with three baselines - without adaptation, merging data, and simple interpolation - on Switchboard (SWBD) and Wall Street Journal (WSJ). Experiments show that the mixer model generally performs better than baselines and finetuning. Compared with no adaptation, finetuning and the mixer approach obtain up to relative 11.5% and 14.1% WER reductions on SWBD, respectively. The mixer model also outperforms linear interpolation and merging data. On WSJ, the mixer approach achieves a new state-of-the-art WER result.},
keywords={neural language model;language model adaptation;Transformer;linear interpolation;automatic speech recognition},
doi={10.1109/ICASSP40776.2020.9053399},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053053,
author={F. {Kreuk} and Y. {Sheena} and J. {Keshet} and Y. {Adi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Phoneme Boundary Detection Using Learnable Segmental Features},
year={2020},
volume={},
number={},
pages={8089-8093},
abstract={Phoneme boundary detection plays an essential first step for a variety of speech processing applications such as speaker diarization, speech science, keyword spotting, etc. In this work, we propose a neural architecture coupled with a parameterized structured loss function to learn segmental representations for the task of phoneme boundary detection. First, we evaluated our model when the spoken phonemes were not given as input. Results on the TIMIT and Buckeye corpora suggest that the proposed model is superior to the baseline models and reaches state-of-the-art performance in terms of F1 and R-value. We further explore the use of phonetic transcription as additional supervision and show this yields minor improvements in performance but substantially better convergence rates. We additionally evaluate the model on a He-brew corpus and demonstrate such phonetic supervision can be beneficial in a multi-lingual setting.},
keywords={Sequence segmentation;recurrent neural networks (RNNs);structured prediction;phoneme boundary detection},
doi={10.1109/ICASSP40776.2020.9053053},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054240,
author={A. {Tyagi} and V. {Sharma} and R. {Gupta} and L. {Samson} and N. {Zhuang} and Z. {Wang} and B. {Campbell}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast Intent Classification for Spoken Language Understanding Systems},
year={2020},
volume={},
number={},
pages={8119-8123},
abstract={Spoken Language Understanding (SLU) systems consist of several machine learning components operating together (e.g. intent classification, named entity recognition and resolution). Deep learning models have obtained state of the art results on several of these tasks, largely attributed to their better modeling capacity. However, an increase in modeling capacity comes with added costs of higher latency and energy usage, particularly when operating on low complexity devices. To address the latency and computational complexity issues, we explore a BranchyNet scheme on an intent classification scheme within SLU systems. The BranchyNet scheme when applied to a high complexity model, adds exit points at various stages in the model allowing early decision making for a set of queries to the SLU model. We conduct experiments on the Facebook Semantic Parsing dataset with two candidate model architectures for intent classification. Our experiments show that the BranchyNet scheme provides gains in terms of computational complexity without compromising model accuracy. We also conduct analytical studies regarding the improvements in the computational cost, distribution of utterances that egress from various exit points and the impact of adding more complexity to models with the BranchyNet scheme.},
keywords={Spoken Language Understanding;BranchyNet;Intent Classification},
doi={10.1109/ICASSP40776.2020.9054240},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054158,
author={A. S. {Timmaraju} and A. {Liu} and P. {Tripathi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Addressing Challenges in Building Web-Scale Content Classification Systems},
year={2020},
volume={},
number={},
pages={8134-8138},
abstract={Understanding the semantic meaning of content on the web through the lens of a taxonomy has many practical advantages. However, when building large-scale content classification systems, practitioners are faced with unique challenges involving finding the best ways to leverage the scale and variety of data available on internet platforms. We present learnings from our efforts in building a content classification system for multiple document types at Facebook using Multi-modal Transformers. We empirically demonstrate the effectiveness of multi-lingual, multi-modal and cross-document type learning. We describe effective strategies for exploiting weakly supervised signals as a pre-training step and show that they lead to significant gains in downstream classification accuracy. We also discuss label collection schemes that help minimize the amount of noise in collected data.},
keywords={self-attention;weakly supervised pre-training;multi-lingual classification;cross-document transfer;multi-modality},
doi={10.1109/ICASSP40776.2020.9054158},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054014,
author={J. {Wang} and V. {Tantia} and N. {Ballas} and M. {Rabbat}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Lookahead Converges to Stationary Points of Smooth Non-convex Functions},
year={2020},
volume={},
number={},
pages={8604-8608},
abstract={The Lookahead optimizer [Zhang et al., 2019] was recently proposed and demonstrated to improve performance of stochastic first-order methods for training deep neural networks. Lookahead can be viewed as a two time-scale algorithm, where the fast dynamics (inner optimizer) determine a search direction and the slow dynamics (outer optimizer) perform updates by moving along this direction. We prove that, with appropriate choice of step-sizes, Lookahead converges to a stationary point of smooth non-convex functions. Although Lookahead is described and implemented as a serial algorithm, our analysis is based on viewing Lookahead as a multi-agent optimization method with two agents communicating periodically.},
keywords={Lookahead optimizer;deep learning;stochastic non-convex optimization},
doi={10.1109/ICASSP40776.2020.9054014},
ISSN={2379-190X},
month={May},}