@INPROCEEDINGS{9054369,
author={},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={[Front cover]},
year={2020},
volume={},
number={},
pages={c1-c1},
abstract={Presents the front cover or splash screen of the proceedings record.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054369},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054730,
author={},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={[ICASSP 2020 Title Page]},
year={2020},
volume={},
number={},
pages={i-i},
abstract={Presents the title page of the proceedings record.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054730},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054742,
author={},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={[Copyright notice]},
year={2020},
volume={},
number={},
pages={i-i},
abstract={Presents the copyright information for the conference. May include reprint permission information.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054742},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053672,
author={},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={ICASSP 2020 Organizing Committee},
year={2020},
volume={},
number={},
pages={i-i},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053672},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054320,
author={},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={ICASSP 2020 Technical Program Committee},
year={2020},
volume={},
number={},
pages={iv-xxx},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054320},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054406,
author={},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={ICASSP 2020 Table of Contents},
year={2020},
volume={},
number={},
pages={xxxi-clxxx},
abstract={Presents the table of contents/splash page of the proceedings record.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054406},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054069,
author={T. {Hsieh} and K. {Cheng} and Z. {Fan} and Y. {Yang} and Y. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Addressing The Confounds Of Accompaniments In Singer Identification},
year={2020},
volume={},
number={},
pages={1-5},
abstract={Identifying singers is an important task with many applications. However, the task remains challenging due to many issues. One major issue is related to the confounding factors from the background instrumental music that is mixed with the vocals in music production. A singer identification model may learn to extract non-vocal related features from the instrumental part of the songs, if a singer only sings in certain musical contexts (e.g., genres). The model cannot therefore generalize well when the singer sings in unseen contexts. In this paper, we attempt to address this issue. Specifically, we employ open-unmix, an open source tool with state-of-the-art performance in source separation, to separate the vocal and instrumental tracks of music. We then investigate two means to train a singer identification model: by learning from the separated vocal only, or from an augmented set of data where we "shuffle-and-remix" the separated vocal tracks and instrumental tracks of different songs to artificially make the singers sing in different contexts. We also incorporate melodic features learned from the vocal melody contour for better performance. Evaluation results on a benchmark dataset called the artist20 shows that this data augmentation method greatly improves the accuracy of singer identification.},
keywords={Signer identification;singing voice separation;melody extraction;data augmentation},
doi={10.1109/ICASSP40776.2020.9054069},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053442,
author={J. {Lee} and N. J. {Bryan} and J. {Salomon} and Z. {Jin} and J. {Nam}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Disentangled Multidimensional Metric Learning for Music Similarity},
year={2020},
volume={},
number={},
pages={6-10},
abstract={Music similarity search is useful for a variety of creative tasks such as replacing one music recording with another recording with a similar "feel", a common task in video editing. For this task, it is typically necessary to define a similarity metric to compare one recording to another. Music similarity, however, is hard to define and depends on multiple simultaneous notions of similarity (i.e. genre, mood, instrument, tempo). While prior work ignore this issue, we embrace this idea and introduce the concept of multidimensional similarity and unify both global and specialized similarity metrics into a single, semantically disentangled multidimensional similarity metric. To do so, we adapt a variant of deep metric learning called conditional similarity networks to the audio domain and extend it using track-based information to control the specificity of our model. We evaluate our method and show that our single, multidimensional model outperforms both specialized similarity spaces and alternative baselines. We also run a user-study and show that our approach is favored by human annotators as well.},
keywords={multidimensional music similarity;metric learning;disentangled representation;query-by-example},
doi={10.1109/ICASSP40776.2020.9053442},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053644,
author={V. {Lostanlen} and S. {Sridhar} and B. {McFee} and A. {Farnsworth} and J. P. {Bello}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning the Helix Topology of Musical Pitch},
year={2020},
volume={},
number={},
pages={11-15},
abstract={To explain the consonance of octaves, music psychologists represent pitch as a helix where azimuth and axial coordinate correspond to pitch class and pitch height respectively. This article addresses the problem of discovering this helical structure from unlabeled audio data. We measure Pearson correlations in the constant-Q transform (CQT) domain to build a K-nearest neighbor graph between frequency subbands. Then, we run the Isomap manifold learning algorithm to represent this graph in a three-dimensional space in which straight lines approximate graph geodesics. Experiments on isolated musical notes demonstrate that the resulting manifold resembles a helix which makes a full turn at every octave. A circular shape is also found in English speech, but not in urban noise. We discuss the impact of various design choices on the visualization: instrumentarium, loudness mapping function, and number of neighbors K.},
keywords={Continuous wavelet transforms;distance learning;music;pitch control (audio);shortest path problem},
doi={10.1109/ICASSP40776.2020.9053644},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054352,
author={K. M. {Ibrahim} and J. {Royo-Letelier} and E. V. {Epure} and G. {Peeters} and G. {Richard}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Audio-Based Auto-Tagging With Contextual Tags for Music},
year={2020},
volume={},
number={},
pages={16-20},
abstract={Music listening context such as location or activity has been shown to greatly influence the users’ musical tastes. In this work, we study the relationship between user context and audio content in order to enable context-aware music recommendation agnostic to user data. For that, we propose a semi-automatic procedure to collect track sets which leverages playlist titles as a proxy for context labelling. Using this, we create and release a dataset of ~50k tracks labelled with 15 different contexts. Then, we present benchmark classification results on the created dataset using an audio auto-tagging model. As the training and evaluation of these models are impacted by missing negative labels due to incomplete annotations, we propose a sample-level weighted cross entropy loss to account for the confidence in missing labels and show improved context prediction results.},
keywords={music auto-tagging;user context;dataset collection;multi-label classification;missing labels},
doi={10.1109/ICASSP40776.2020.9054352},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053793,
author={F. {Yesiler} and J. {Serrà} and E. {Gómez}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Accurate and Scalable Version Identification Using Musically-Motivated Embeddings},
year={2020},
volume={},
number={},
pages={21-25},
abstract={The version identification (VI) task deals with the automatic detection of recordings that correspond to the same underlying musical piece. Despite many efforts, VI is still an open problem, with much room for improvement, specially with regard to combining accuracy and scalability. In this paper, we present MOVE, a musically-motivated method for accurate and scalable version identification. MOVE achieves state-of-the-art performance on two publicly-available benchmark sets by learning scalable embeddings in an Euclidean distance space, using a triplet loss and a hard triplet mining strategy. It improves over previous work by employing an alternative input representation, and introducing a novel technique for temporal content summarization, a standardized latent space, and a data augmentation strategy specifically designed for VI. In addition to the main results, we perform an ablation study to highlight the importance of our design choices, and study the relation between embedding dimensionality and model performance.},
keywords={Cover song identification;deep learning;music embedding;network encoder},
doi={10.1109/ICASSP40776.2020.9053793},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053257,
author={C. {Jiang} and D. {Yang} and X. {Chen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Similarity Learning For Cover Song Identification Using Cross-Similarity Matrices of Multi-Level Deep Sequences},
year={2020},
volume={},
number={},
pages={26-30},
abstract={In recent years, several deep learning models have been proposed for cover song identification and they have been designed to learn fixed-length feature vectors for music tracks. However, the aspect of temporal progression of music, which is important for measuring the melody similarity between two tracks, is not well represented by fixed-length vectors. In this paper, we propose a new Siamese network architecture for music melody similarity metric learning. The architecture consists of two parts. One part is a network for learning the deep sequence representation of music tracks, and the other is a similarity estimation network which takes as input the cross-similarity matrices calculated from the deep sequences of a pair of tracks. The two networks are jointly trained and optimized to achieve high melody similarity prediction accuracy. Experiments conducted on several public datasets demonstrate the superiority of the proposed architecture.},
keywords={similarity learning;cover song identification;cross-similarity matrices;Siamese network;multi-level deep sequences},
doi={10.1109/ICASSP40776.2020.9053257},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054172,
author={E. {Tzinis} and S. {Venkataramani} and Z. {Wang} and C. {Subakan} and P. {Smaragdis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Two-Step Sound Source Separation: Training On Learned Latent Targets},
year={2020},
volume={},
number={},
pages={31-35},
abstract={In this paper, we propose a two-step training procedure for source separation via a deep neural network. In the first step we learn a transform (and it’s inverse) to a latent space where masking-based separation performance using oracles is optimal. For the second step, we train a separation module that operates on the previously learned space. In order to do so, we also make use of a scale-invariant signal to distortion ratio (SI-SDR) loss function that works in the latent space, and we prove that it lower-bounds the SI-SDR in the time domain. We run various sound separation experiments that show how this approach can obtain better performance as compared to systems that learn the transform and the separation module jointly. The proposed methodology is general enough to be applicable to a large class of neural network end-to-end separation systems.},
keywords={Audio source separation;signal representation;cost function;deep learning},
doi={10.1109/ICASSP40776.2020.9054172},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053602,
author={D. {Ditter} and T. {Gerkmann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Multi-Phase Gammatone Filterbank for Speech Separation Via Tasnet},
year={2020},
volume={},
number={},
pages={36-40},
abstract={In this work, we investigate if the learned encoder of the end-to-end convolutional time domain audio separation network (Conv-TasNet) is the key to its recent success, or if the encoder can just as well be replaced by a deterministic hand-crafted filterbank. Motivated by the resemblance of the trained encoder of Conv-TasNet to auditory filterbanks, we propose to employ a deterministic gammatone filterbank. In contrast to a common gammatone filterbank, our filters are restricted to 2 ms length to allow for low-latency processing. Inspired by the encoder learned by Conv-TasNet, in addition to the logarithmically spaced filters, the proposed filterbank holds multiple gammatone filters at the same center frequency with varying phase shifts. We show that replacing the learned encoder with our proposed multi-phase gammatone filterbank (MP-GTF) even leads to a scale-invariant source-to-noise ratio (SI-SNR) improvement of 0.7 dB. Furthermore, in contrast to using the learned encoder we show that the number of filters can be reduced from 512 to 128 without loss of performance.},
keywords={Speech Separation;Auditory Filterbank;End-To-End Learning;TasNet},
doi={10.1109/ICASSP40776.2020.9053602},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053845,
author={N. {Takahashi} and M. K. {Singh} and S. {Basak} and P. {Sudarsanam} and S. {Ganapathy} and Y. {Mitsufuji}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Voice Separation by Incorporating End-To-End Speech Recognition},
year={2020},
volume={},
number={},
pages={41-45},
abstract={Despite recent advances in voice separation methods, many challenges remain in realistic scenarios such as noisy recording and the limits of available data. In this work, we propose to explicitly incorporate the phonetic and linguistic nature of speech by taking a transfer learning approach using an end-to-end automatic speech recognition (E2EASR) system. The voice separation is conditioned on deep features extracted from E2EASR to cover the long-term dependence of phonetic aspects. Experimental results on speech separation and enhancement task on the AVSpeech dataset show that the proposed method significantly improves the signal-to-distortion ratio over the baseline model and even outperforms an audio visual model, that utilizes visual information of lip movements.},
keywords={Speech separation;Singing voice separation;End-to-end ASR},
doi={10.1109/ICASSP40776.2020.9053845},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054266,
author={Y. {Luo} and Z. {Chen} and T. {Yoshioka}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Dual-Path RNN: Efficient Long Sequence Modeling for Time-Domain Single-Channel Speech Separation},
year={2020},
volume={},
number={},
pages={46-50},
abstract={Recent studies in deep learning-based speech separation have proven the superiority of time-domain approaches to conventional time-frequency-based methods. Unlike the time-frequency domain approaches, the time-domain separation systems often receive input sequences consisting of a huge number of time steps, which introduces challenges for modeling extremely long sequences. Conventional recurrent neural networks (RNNs) are not effective for modeling such long sequences due to optimization difficulties, while one-dimensional convolutional neural networks (1-D CNNs) cannot perform utterance-level sequence modeling when its receptive field is smaller than the sequence length. In this paper, we propose dual-path recurrent neural network (DPRNN), a simple yet effective method for organizing RNN layers in a deep structure to model extremely long sequences. DPRNN splits the long sequential input into smaller chunks and applies intra- and inter-chunk operations iteratively, where the input length can be made proportional to the square root of the original sequence length in each operation. Experiments show that by replacing 1-D CNN with DPRNN and apply sample-level modeling in the time-domain audio separation network (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved with a 20 times smaller model than the previous best system.},
keywords={Speech separation;deep learning;time domain;recurrent neural networks},
doi={10.1109/ICASSP40776.2020.9054266},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053789,
author={C. {Uhle} and M. {Torcoliv} and J. {Paulus}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Controlling the Perceived Sound Quality for Dialogue Enhancement With Deep Learning},
year={2020},
volume={},
number={},
pages={51-55},
abstract={Speech enhancement attenuates interfering sounds in speech signals but may introduce artifacts that perceivably deteriorate the output signal. We propose a method for controlling the trade-off between the attenuation of the interfering background signal and the loss of sound quality. A deep neural network estimates the attenuation of the separated background signal such that the sound quality, quantified using the Artifact-related Perceptual Score, meets an adjustable target. Subjective evaluations indicate that consistent sound quality is obtained across various input signals. Our experiments show that the proposed method is able to control the tradeoff with an accuracy that is adequate for real-world dialogue enhancement applications.},
keywords={Speech Enhancement;Dialogue Enhancement;Deep Learning;Artifact-related Perceptual Score},
doi={10.1109/ICASSP40776.2020.9053789},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054171,
author={M. {Togami} and Y. {Masuyama} and T. {Komatsu} and Y. {Nakagome}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unsupervised Training for Deep Speech Source Separation with Kullback-Leibler Divergence Based Probabilistic Loss Function},
year={2020},
volume={},
number={},
pages={56-60},
abstract={In this paper, we propose a multi-channel speech source separation method with a deep neural network (DNN) which is trained under the condition that no clean signal is available. As an alternative to a clean signal, the proposed method adopts an estimated speech signal by an unsupervised speech source separation method which leverages a statistical model. As a statistical model of microphone input signal, we adopts a time-varying spatial covariance matrix (SCM) model which includes reverberation and background noise submodels so as to achieve robustness against reverberation and background noise. The DNN infers intermediate variables which are needed for constructing the time-varying SCM. Separation is performed in a probabilistic manner so as to avoid overfitting to separation error. Since there are multiple intermediate variables, a loss function which evaluates a single intermediate variable is not applicable. Instead, the proposed method adopts a loss function which evaluates the output probabilistic signal directly based on Kullback-Leibler Divergence (KLD). The gradient of the loss function can be back-propagated into the DNN through all the intermediate variables. Experimental results under reverberant conditions show that the proposed method achieves better results than the conventional methods.},
keywords={Unsupervised learning;local Gaussian modeling;dereverberation;denoising;Kullback-Leibler Divergence},
doi={10.1109/ICASSP40776.2020.9054171},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052995,
author={Ç. {Bilen} and G. {Ferroni} and F. {Tuveri} and J. {Azcarreta} and S. {Krstulović}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Framework for the Robust Evaluation of Sound Event Detection},
year={2020},
volume={},
number={},
pages={61-65},
abstract={This work defines a new framework for performance evaluation of polyphonic sound event detection (SED) systems, which overcomes the limitations of the conventional collar-based event decisions, event F-scores and event error rates. The proposed framework introduces a definition of event detection that is more robust against labelling subjectivity. It also resorts to polyphonic receiver operating characteristic (ROC) curves to deliver more global insight into system performance than F1-scores, and proposes a reduction of these curves into a single polyphonic sound detection score (PSDS), which allows system comparison independently from operating points (OPs). The presented method also delivers better insight into data biases and classification stability across sound classes. Furthermore, it can be tuned to varying applications in order to match a variety of user experience requirements. The benefits of the proposed approach are demonstrated by re-evaluating the baseline and two of the top-performing systems from DCASE 2019 Task 4.},
keywords={Sound event detection;SED;evaluation metrics;sound recognition;polyphonic sound detection score;PSDS},
doi={10.1109/ICASSP40776.2020.9052995},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053609,
author={K. {Miyazaki} and T. {Komatsu} and T. {Hayashi} and S. {Watanabe} and T. {Toda} and K. {Takeda}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Weakly-Supervised Sound Event Detection with Self-Attention},
year={2020},
volume={},
number={},
pages={66-70},
abstract={In this paper, we propose a novel sound event detection (SED) method that incorporates a self-attention mechanism of the Transformer for a weakly-supervised learning scenario. The proposed method utilizes the Transformer encoder, which consists of multiple self-attention modules, allowing to take both local and global context information of the input feature sequence into account. Furthermore, inspired by the great success of BERT in the natural language processing field, the proposed method introduces a special tag token into the input sequence for weak label prediction, which enables the aggregation of the whole sequence information. To demonstrate the performance of the proposed method, we conduct the experimental evaluation using the DCASE2019 Task4 dataset. The experimental results demonstrate that the proposed method outperforms the DCASE2019 Task4 baseline method, which is based on the convolutional recurrent neural network, and the self-attention mechanism effectively works for SED.},
keywords={sound event detection;weakly-supervised learning;self-attention;Transformer},
doi={10.1109/ICASSP40776.2020.9053609},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053045,
author={T. N. {Tho Nguyen} and D. L. {Jones} and W. {Gan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Sequence Matching Network for Polyphonic Sound Event Localization and Detection},
year={2020},
volume={},
number={},
pages={71-75},
abstract={Polyphonic sound event detection and direction-of-arrival estimation require different input features from audio signals. While sound event detection mainly relies on time-frequency patterns, direction-of-arrival estimation relies on magnitude or phase differences between microphones. Previous approaches use the same input features for sound event detection and direction-of-arrival estimation, and train the two tasks jointly or in a two-stage transfer-learning manner. We propose a two-step approach that decouples the learning of the sound event detection and directional-of-arrival estimation systems. In the first step, we detect the sound events and estimate the directions-of-arrival separately to optimize the performance of each system. In the second step, we train a deep neural network to match the two output sequences of the event detector and the direction-of-arrival estimator. This modular and hierarchical approach allows the flexibility in the system design, and increase the performance of the whole sound event localization and detection system. The experimental results using the DCASE 2019 sound event localization and detection dataset show an improved performance compared to the previous state-of-the-art solutions.},
keywords={sound event detection;direction-of-arrival estimation;deep neural network;sequence matching},
doi={10.1109/ICASSP40776.2020.9053045},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053336,
author={B. {Shi} and M. {Sun} and K. C. {Puvvada} and C. {Kao} and S. {Matsoukas} and C. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Few-Shot Acoustic Event Detection Via Meta Learning},
year={2020},
volume={},
number={},
pages={76-80},
abstract={We study few-shot acoustic event detection (AED) in this paper. Few-shot learning enables detection of new events with very limited labeled data. Compared to other research areas like computer vision, few-shot learning for audio recognition has been under-studied. We formulate few-shot AED problem and explore different ways of utilizing traditional supervised methods for this setting as well as a variety of meta-learning approaches, which are conventionally used to solve few-shot classification problem. Compared to supervised baselines, meta-learning models achieve superior performance, thus showing its effectiveness on generalization to new audio events. Our analysis including impact of initialization and domain discrepancy further validate the advantage of meta-learning approaches in few-shot AED.},
keywords={Acoustic event detection;few-shot learning;meta learning},
doi={10.1109/ICASSP40776.2020.9053336},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054708,
author={Y. {Wang} and J. {Salomon} and N. J. {Bryan} and J. {Pablo Bello}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Few-Shot Sound Event Detection},
year={2020},
volume={},
number={},
pages={81-85},
abstract={Locating perceptually similar sound events within a continuous recording is a common task for various audio applications. However, current tools require users to manually listen to and label all the locations of the sound events of interest, which is tedious and time-consuming. In this work, we (1) adapt state-of-the-art metric-based few-shot learning methods to automate the detection of similar-sounding events, requiring only one or few examples of the target event, (2) develop a method to automatically construct a partial set of labeled examples (negative samples) to reduce user labeling effort, and (3) develop an inference-time data augmentation method to increase detection accuracy. To validate our approach, we perform extensive comparative analysis of few-shot learning methods for the task of keyword detection in speech. We show that our approach successfully adapts closed-set few-shot learning approaches to an open-set sound event detection problem.},
keywords={Few-shot learning;sound event detection;keyword detection;keyword spotting;speech},
doi={10.1109/ICASSP40776.2020.9054708},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054478,
author={R. {Serizel} and N. {Turpault} and A. {Shah} and J. {Salamon}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sound Event Detection in Synthetic Domestic Environments},
year={2020},
volume={},
number={},
pages={86-90},
abstract={We present a comparative analysis of the performance of state-of-the-art sound event detection systems. In particular, we study the robustness of the systems to noise and signal degradation, which is known to impact model generalization. Our analysis is based on the results of task 4 of the DCASE 2019 challenge, where submitted systems were evaluated on, in addition to real-world recordings, a series of synthetic soundscapes that allow us to carefully control for different soundscape characteristics. Our results show that while overall systems exhibit significant improvements compared to previous work, they still suffer from biases that could prevent them from generalizing to real-world scenarios.},
keywords={Sound event detection;synthetic data;weakly labeled data;semi-supervised learning},
doi={10.1109/ICASSP40776.2020.9054478},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053055,
author={F. {Pishdadian} and G. {Wichern} and J. L. {Roux}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning to Separate Sounds from Weakly Labeled Scenes},
year={2020},
volume={},
number={},
pages={91-95},
abstract={Deep learning models for monaural audio source separation are typically trained on large collections of isolated sources, which may not be available in domains such as environmental monitoring. We propose objective functions and network architectures that enable training a source separation system with weak labels. In contrast with strong time-frequency (TF) labels, weak labels only indicate the time periods where different sources are active in this scenario. We train a separator that outputs a TF mask for each type of sound event, using a classifier to pool label estimates across frequency. Our objective function requires the classifier applied to a separated source to output weak labels for the class corresponding to that source and zeros for all other classes. The objective function also enforces that the separated sources sum to the mixture. We benchmark performance using synthetic mixtures of overlapping sound events recorded in urban environments. Compared to training on mixtures and their isolated sources, our model still achieves significant SDR improvement.},
keywords={audio source separation;semi-supervised classification;weakly-labeled data},
doi={10.1109/ICASSP40776.2020.9053055},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053921,
author={E. {Tzinis} and S. {Wisdom} and J. R. {Hershey} and A. {Jansen} and D. P. W. {Ellis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Universal Sound Separation Using Sound Classification},
year={2020},
volume={},
number={},
pages={96-100},
abstract={Deep learning approaches have recently achieved impressive performance on both audio source separation and sound classification. Most audio source separation approaches focus only on separating sources belonging to a restricted domain of source classes, such as speech and music. However, recent work has demonstrated the possibility of "universal sound separation", which aims to separate acoustic sources from an open domain, regardless of their class. In this paper, we utilize the semantic information learned by sound classifier networks trained on a vast amount of diverse sounds to improve universal sound separation. In particular, we show that semantic embeddings extracted from a sound classifier can be used to condition a separation network, providing it with useful additional information. This approach is especially useful in an iterative setup, where source estimates from an initial separation stage and their corresponding classifier-derived embeddings are fed to a second separation network. By performing a thorough hyperparameter search consisting of over a thousand experiments, we find that classifier embeddings from oracle clean sources provide nearly one dB of SNR gain, and our best iterative models achieve a significant fraction of this oracle performance, establishing a new state-of-the-art for universal sound separation.},
keywords={Audio source separation;deep learning;semantic audio representations;sound classification},
doi={10.1109/ICASSP40776.2020.9053921},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053396,
author={Q. {Kong} and Y. {Wang} and X. {Song} and Y. {Cao} and W. {Wang} and M. D. {Plumbley}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Source Separation with Weakly Labelled Data: an Approach to Computational Auditory Scene Analysis},
year={2020},
volume={},
number={},
pages={101-105},
abstract={Source separation is the task of separating an audio recording into individual sound sources. Source separation is fundamental for computational auditory scene analysis. Previous work on source separation has focused on separating particular sound classes such as speech and music. Much previous work requires mixtures and clean source pairs for training. In this work, we propose a source separation framework trained with weakly labelled data. Weakly labelled data only contains the tags of an audio clip, without the occurrence time of sound events. We first train a sound event detection system with AudioSet. The trained sound event detection system is used to detect segments that are most likely to contain a target sound event. Then a regression is learnt from a mixture of two randomly selected segments to a target segment conditioned on the audio tagging prediction of the target segment. Our proposed system can separate 527 kinds of sound classes from AudioSet within a single system. A U-Net is adopted for the separation system and achieves an average SDR of 5.67 dB over 527 sound classes in AudioSet.},
keywords={Source separation;weakly labelled data;computational auditory scene analysis;AudioSet},
doi={10.1109/ICASSP40776.2020.9053396},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053052,
author={S. {Kim} and H. {Yang} and M. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Boosted Locality Sensitive Hashing: Discriminative Binary Codes for Source Separation},
year={2020},
volume={},
number={},
pages={106-110},
abstract={Speech enhancement tasks have seen significant improvements with the advance of deep learning technology, but with the cost of increased computational complexity. In this study, we propose an adaptive boosting approach to learning locality sensitive hash codes, which represent audio spectra efficiently. We use the learned hash codes for single-channel speech denoising tasks as an alternative to a complex machine learning model, particularly to address the resource-constrained environments. Our adaptive boosting algorithm learns simple logistic regressors as the weak learners. Once trained, their binary classification results transform each spectrum of test noisy speech into a bit string. Simple bitwise operations calculate Hamming distance to find the $\mathcal{K}$-nearest matching frames in the dictionary of training noisy speech spectra, whose associated ideal binary masks are averaged to estimate the denoising mask for that test mixture. Our proposed learning algorithm differs from AdaBoost in the sense that the projections are trained to minimize the distances between the self-similarity matrix of the hash codes and that of the original spectra, rather than the misclassification rate. We evaluate our discriminative hash codes on the TIMIT corpus with various noise types, and show comparative performance to deep learning methods in terms of denoising performance and complexity.},
keywords={Speech Enhancement;Locality Sensitive Hashing;AdaBoost},
doi={10.1109/ICASSP40776.2020.9053052},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053757,
author={S. {Emura} and H. {Sawada} and S. {Araki} and N. {Harada}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Frequency-Domain BSS Method Based on ℓ1 Norm, Unitary Constraint, and Cayley Transform},
year={2020},
volume={},
number={},
pages={111-115},
abstract={We propose a frequency-domain blind source separation method that uses (a) the ℓ1 norm of orthonormal vectors of estimated source signals as a sparsity measure and (b) Cayley transform for optimizing the objective function under the unitary constraint in the Riemannian geometry approach. The orthonormal vectors of estimated source signals, obtained by the sphering of observed mixed signals and the unitary constraint on the separation filters, enables us to use the ℓ1 norm properly as a sparsity measure. The Cayley transform enables us to handle the geometrical aspects of the unitary constraint efficiently. According to the simulation of a two-channel case, the proposed method achieved a 20-dB improvement in the source-to-interference ratio in a room with a reverberation time of T60 = 300ms.},
keywords={frequency-domain blind source separation;sparse;unitary constraint;Cayley transform;FastICA},
doi={10.1109/ICASSP40776.2020.9053757},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053588,
author={S. {Venkataramani} and E. {Tzinis} and P. {Smaragdis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={End-To-End Non-Negative Autoencoders for Sound Source Separation},
year={2020},
volume={},
number={},
pages={116-120},
abstract={Discriminative models for source separation have recently been shown to produce impressive results. However, when operating on sources outside of the training set, these models can not perform as well and are cumbersome to update. Classical methods like Nonnegative Matrix Factorization (NMF) provide modular approaches to source separation that can be easily updated to adapt to new mixture scenarios. In this paper, we generalize NMF to develop end-to-end non-negative auto-encoders and demonstrate how they can be used for source separation. Our experiments indicate that these models deliver comparable separation performance to discriminative approaches, while retaining the modularity of NMF and the modeling flexibility of neural networks.},
keywords={Non-negative autoencoder;non-negative matrix factorization;source separation;single-channel audio separation;end-to-end;deep learning},
doi={10.1109/ICASSP40776.2020.9053588},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054137,
author={A. {Jansen} and D. P. W. {Ellis} and S. {Hershey} and R. C. {Moore} and M. {Plakal} and A. C. {Popat} and R. A. {Saurous}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Coincidence, Categorization, and Consolidation: Learning to Recognize Sounds with Minimal Supervision},
year={2020},
volume={},
number={},
pages={121-125},
abstract={Humans do not acquire perceptual abilities in the way we train machines. While machine learning algorithms typically operate on large collections of randomly-chosen, explicitly-labeled examples, human acquisition relies more heavily on multimodal unsupervised learning (as infants) and active learning (as children). With this motivation, we present a learning framework for sound representation and recognition that combines (i) a self-supervised objective based on a general notion of unimodal and cross-modal coincidence, (ii) a clustering objective that reflects our need to impose categorical structure on our experiences, and (iii) a cluster-based active learning procedure that solicits targeted weak supervision to consolidate categories into relevant semantic classes. By training a combined sound embedding/clustering/classification network according to these criteria, we achieve a new state-of-the-art unsupervised audio representation and demonstrate up to a 20-fold reduction in the number of labels required to reach a desired classification performance.},
keywords={Sound classification;self-supervised learning;multimodal models;clustering;active learning},
doi={10.1109/ICASSP40776.2020.9054137},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053582,
author={T. {Nguyen} and F. {Pernkopf} and M. {Kosmider}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Acoustic Scene Classification for Mismatched Recording Devices Using Heated-Up Softmax and Spectrum Correction},
year={2020},
volume={},
number={},
pages={126-130},
abstract={Deep neural networks (DNNs) are successful in applications with matching inference and training distributions. In realworld scenarios, DNNs have to cope with truly new data samples during inference, potentially coming from a shifted data distribution. This usually causes a drop in performance. Acoustic scene classification (ASC) with different recording devices is one of this situation. Furthermore, an imbalance in quality and amount of data recorded by different devices causes severe challenges. In this paper, we introduce two calibration methods to tackle these challenges. In particular, we applied scaling of the features to deal with varying frequency response of the recording devices. Furthermore, to account for the shifted data distribution, a heated-up softmax is embedded to calibrate the predictions of the model. We use robust and resource-efficient models, and show the efficiency of heated-up softmax. Our ASC system reaches state-of-the-art performance on the development set of DCASE challenge 2019 task 1B with only ~70K parameters. It achieves 70.1% average classification accuracy for device B and device C. It performs on par with the best single model system of the DCASE 2019 challenge and outperforms the baseline system by 28.7% (absolute).},
keywords={Acoustic scene classification;spectrum correction;heated-up softmax;temperature scaling;calibration of confidence prediction},
doi={10.1109/ICASSP40776.2020.9053582},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053160,
author={N. {Turpault} and R. {Serizel} and E. {Vincent}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Limitations of Weak Labels for Embedding and Tagging},
year={2020},
volume={},
number={},
pages={131-135},
abstract={Many datasets and approaches in ambient sound analysis use weakly labeled data. Weak labels are employed because annotating every data sample with a strong label is too expensive. Yet, their impact on the performance in comparison to strong labels remains unclear. Indeed, weak labels must often be dealt with at the same time as other challenges, namely multiple labels per sample, unbalanced classes and/or overlapping events. In this paper, we formulate a supervised learning problem which involves weak labels. We create a dataset that focuses on the difference between strong and weak labels as opposed to other challenges. We investigate the impact of weak labels when training an embedding or an end-to-end classifier. Different experimental scenarios are discussed to provide insights into which applications are most sensitive to weakly labeled data.},
keywords={Weak labels;triplet loss;prototypical network;audio tagging;audio embedding.},
doi={10.1109/ICASSP40776.2020.9053160},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053065,
author={H. {Shrivaslava} and Y. {Yin} and R. R. {Shah} and R. {Zimmermann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Mt-Gcn For Multi-Label Audio Tagging With Noisy Labels},
year={2020},
volume={},
number={},
pages={136-140},
abstract={Multi-label audio tagging is the task of predicting the types of sounds occurring in an audio clip. Recently, large-scale audio datasets such as Google’s AudioSet, have allowed researchers to use deep learning techniques for this task but this comes at the cost of label noise in the datasets. Audio datasets such as the AudioSet are usually built following a hierarchical structure known as ontology which captures the relationships between different sound events with domain knowledge. However, existing methods for audio tagging failed to utilize this domain knowledge about label relationships in their models, resulting in models being sensitive to label noise. We therefore present MT-GCN, a Multi-task Learning based Graph Convolutional Network that learns domain knowledge from ontology. The relationships between sound events in our proposed method are described by a graph. We propose two ontology-based graph construction methods, and conduct extensive experiments on the FSDKaggle2019 dataset. The experimental results show that our approach outperforms the baseline methods by a significant margin.},
keywords={Audio Tagging;Graph Convolutional Networks;Multi-task Learning},
doi={10.1109/ICASSP40776.2020.9053065},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053274,
author={M. D. {McDonnell} and W. {Gao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Acoustic Scene Classification Using Deep Residual Networks with Late Fusion of Separated High and Low Frequency Paths},
year={2020},
volume={},
number={},
pages={141-145},
abstract={We investigate the problem of acoustic scene classification, using a deep residual network applied to log-mel spectrograms complemented by log-mel deltas and delta-deltas. We design the network to take into account that the temporal and frequency axes in spectrograms represent fundamentally different information. In particular, we use two pathways in the residual network: one for high frequencies and one for low frequencies, that were fused just two convolutional layers prior to the network output. We conduct experiments using two public 2019 DCASE datasets for acoustic scene classification; the first with binaural audio inputs recorded by a single device, and the second with single-channel audio inputs recorded through various devices. We show the performance of our models are significantly enhanced by the use of log-mel deltas, and that overall our approach is capable of training strong single models, without use of any supplementary data from outside the official challenge dataset, with excellent generalization to unknown devices. In particular, our approach achieved second place in 2019 DCASE Task 1b (0.4% behind the winning entry), and the best Task 1B evaluation results (by a large margin of over 5%) on test data from a device not used to record any training data.},
keywords={acoustic scene classification;deep residual network;log-mel spectrograms;deltas;delta-deltas},
doi={10.1109/ICASSP40776.2020.9053274},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054725,
author={M. {Ebrahimpour} and T. {Shea} and A. {Danielescu} and D. {Noelle} and C. {Kello}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={End-To-End Auditory Object Recognition Via Inception Nucleus},
year={2020},
volume={},
number={},
pages={146-150},
abstract={Machine learning approaches to auditory object recognition are traditionally based on engineered features such as those derived from the spectrum or cepstrum. More recently, end- to-end classification systems in image and auditory recognition systems have been developed to learn features jointly with classification and result in improved classification accuracy. In this paper, we propose a novel end-to-end deep neural network to map the raw waveform inputs to sound class labels. Our network includes an "inception nucleus" that optimizes the size of convolutional filters on the fly that results in reducing engineering efforts dramatically. Classification results compared favorably against current state-of-the-art approaches, besting them by 10.4 percentage points on the Ur- bansound8k dataset. Analyses of learned representations revealed that filters in the earlier hidden layers learned waveletlike transforms to extract features that were informative for classification.},
keywords={End-to-End Learning;Auditory Object Recognition;Inception Nucleus;Deep Convolutional Neural Networks;Sound Event Classification},
doi={10.1109/ICASSP40776.2020.9054725},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054414,
author={M. {Kentgens} and A. {Behler} and P. {Jax}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Translation of a Higher Order Ambisonics Sound Scene Based on Parametric Decomposition},
year={2020},
volume={},
number={},
pages={151-155},
abstract={This paper presents a novel 3DoF+ system that allows to navigate, i.e., change position, in scene-based spatial audio content beyond the sweet spot of a Higher Order Ambisonics recording. It is one of the first such systems based on sound capturing at a single spatial position. The system uses a parametric decomposition of the recorded sound field. For the synthesis, only coarse distance information about the sources is needed as side information but not the exact number of them. In an experiment with multiple sources and reverberation, the proposed method shows good performance in terms of an instrumental measure and excellent performance in a subjective evaluation based on a MUSHRA listening test. A variant of the analysis stage increases localization performance and scores median subject ratings between 91 % and 99 % compared to the reference.},
keywords={Higher Order Ambisonics;parametric decomposition;soundfield navigation;3DoF+;scene-based virtual reality},
doi={10.1109/ICASSP40776.2020.9054414},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054647,
author={D. D. {Carlo} and C. {Elvira} and A. {Deleforge} and N. {Bertin} and R. {Gribonval}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Blaster: An Off-Grid Method for Blind and Regularized Acoustic Echoes Retrieval},
year={2020},
volume={},
number={},
pages={156-160},
abstract={Acoustic echoes retrieval is a research topic that is gaining importance in many speech and audio signal processing applications such as speech enhancement, source separation, dereverberation and room geometry estimation. This work proposes a novel approach to blindly retrieve the off-grid timing of early acoustic echoes from a stereophonic recording of an unknown sound source such as speech. It builds on the recent framework of continuous dictionaries. In contrast with existing methods, the proposed approach does not rely on parameter tuning nor peak picking techniques by working directly in the parameter space of interest. The accuracy and robustness of the method are assessed on challenging simulated setups with varying noise and reverberation levels and are compared to two state-of-the-art methods.},
keywords={Blind Channel Identification;Super Resolution;Sparsity;Acoustic Impulse Response},
doi={10.1109/ICASSP40776.2020.9054647},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054434,
author={H. {Helmholz} and J. {Ahrens} and D. L. {Alon} and S. V. {Amengual Garí} and R. {Mehra}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Evaluation of Sensor Self-Noise In Binaural Rendering of Spherical Microphone Array Signals},
year={2020},
volume={},
number={},
pages={161-165},
abstract={Spherical microphone arrays are used to capture spatial sound fields, which can then be rendered via headphones. We use the Real-Time Spherical Array Renderer (ReTiSAR) to analyze and auralize the propagation of sensor self-noise through the processing pipeline. An instrumental evaluation confirms a strong global influence of different array and rendering parameters on the spectral balance and the overall level of the rendered noise. The character of the noise is direction independent in the case of spatially uniformly distributed noise. However, timbre of the rendered self-noise changes with head orientation in the case of spatially non-uniform noise. We determine audibility thresholds of the coloration artifact during head rotations for different array configurations in a perceptual user study.},
keywords={Sensor self-noise;Spherical microphone arrays;Binaural rendering;Real-time signal processing},
doi={10.1109/ICASSP40776.2020.9054434},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053715,
author={K. {Ariga} and T. {Nishida} and S. {Koyama} and N. {Ueno} and H. {Saruwatari}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Mutual-Information-Based Sensor Placement for Spatial Sound Field Recording},
year={2020},
volume={},
number={},
pages={166-170},
abstract={A sensor (microphone) placement method based on mutual information for spatial sound field recording is proposed. The sound field recording methods using distributed sensors enable the estimation of the sound field inside a target region of arbitrary shape; however, it is a difficult task to find the best placement of sensors. We focus on the mutual-information-based sensor placement method in which spatial phenomena are modeled as a Gaussian process (GP). We propose the use of the sound-field-interpolation kernel for the covariance of measurements in a GP model to obtain the sensor placement suitable for sound field recording. We also extend the method to treat broadband signals and derive an efficient algorithm based on block matrix inversion. Numerical simulation results indicated that the proposed method achieves accurate sound field estimation compared with a method using the generally used Gaussian kernel.},
keywords={sensor placement;sound field recording;interpolation;mutual information;microphone array},
doi={10.1109/ICASSP40776.2020.9053715},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054091,
author={Z. {Fan} and V. {Vineet} and H. {Gamper} and N. {Raghuvanshi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast Acoustic Scattering Using Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={171-175},
abstract={Diffracted scattering and occlusion are important acoustic effects in interactive auralization and noise control applications, typically requiring expensive numerical simulation. We propose training a convolutional neural network to map from a convex scatterer’s crosssection to a 2D slice of the resulting spatial loudness distribution. We show that employing a full-resolution residual network for the resulting image-to-image regression problem yields spatially detailed loudness fields with a root-mean-squared error of less than 1 dB, at over 100x speedup compared to full wave simulation.},
keywords={Diffraction;occlusion;scattering;convolutional neural network;wave simulation},
doi={10.1109/ICASSP40776.2020.9054091},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054717,
author={B. {Alary} and A. {Politis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Frequency-Dependent Directional Feedback Delay Network},
year={2020},
volume={},
number={},
pages={176-180},
abstract={A recent publication introduced the Directional Feedback Delay Network, a parametric artificial reverberation algorithm capable of producing direction-dependent energy decay. This method extends the capabilities of Feedback Delay Networks by using multichannel delay-line groups and a spatial transform to produce direction-dependent reverberation in the Ambisonics domain. In this paper, we present a modified formulation of the Directional Feedback Delay Network method that allows both frequency-and direction-dependent reverberation. Multichannel delay-line groups are used to manipulate signals incident on a spherical grid through independent recursive signal paths, while an early reflection module gives a physically-motivated spatial distribution of input signals in the system. The overall number of delay lines is reduced from the previous formulation, and the design allows flexibility to favor a lower computational cost over accuracy.},
keywords={Artificial reverberation;multichannel sound reproduction;spatial audio},
doi={10.1109/ICASSP40776.2020.9054717},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053214,
author={Y. {Koizumi} and K. {Yaiabe} and M. {Delcroix} and Y. {Maxuxama} and D. {Takeuchi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Speech Enhancement Using Self-Adaptation and Multi-Head Self-Attention},
year={2020},
volume={},
number={},
pages={181-185},
abstract={This paper investigates a self-adaptation method for speech enhancement using auxiliary speaker-aware features; we extract a speaker representation used for adaptation directly from the test utterance. Conventional studies of deep neural network (DNN)-based speech enhancement mainly focus on building a speaker independent model. Meanwhile, in speech applications including speech recognition and synthesis, it is known that model adaptation to the target speaker improves the accuracy. Our research question is whether a DNN for speech enhancement can be adopted to unknown speakers without any auxiliary guidance signal in test-phase. To achieve this, we adopt multi-task learning of speech enhancement and speaker identification, and use the output of the final hidden layer of speaker identification branch as an auxiliary feature. In addition, we use multi-head self-attention for capturing long-term dependencies in the speech and noise. Experimental results on a public dataset show that our strategy achieves the state-of-the-art performance and also outperform conventional methods in terms of subjective quality.},
keywords={Speech enhancement;auxiliary information;multi-task learning;and multi-head self-attention},
doi={10.1109/ICASSP40776.2020.9053214},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053666,
author={V. W. {Neo} and C. {Evers} and P. A. {Naylor}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={PEVD-Based Speech Enhancement in Reverberant Environments},
year={2020},
volume={},
number={},
pages={186-190},
abstract={The enhancement of noisy speech is important for applications involving human-to-human interactions, such as telecommunications and hearing aids, as well as human-to-machine interactions, such as voice-controlled systems and robot audition. In this work, we focus on reverberant environments. It is shown that, by exploiting the lack of correlation between speech and the late reflections, further noise reduction can be achieved. This is verified using simulations involving actual acoustic impulse responses and noise from the ACE corpus. The simulations show that even without using a noise estimator, our proposed method simultaneously achieves noise reduction, and enhancement of speech quality and intelligibility, in reverberant environments over a wide range of SNRs. Furthermore, informal listening examples highlight that our approach does not introduce any significant processing artefacts such as musical noise.},
keywords={Speech enhancement;polynomial matrix eigenvalue decomposition;microphone array;noise reduction;broadband signal processing.},
doi={10.1109/ICASSP40776.2020.9053666},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054196,
author={M. {Tammen} and D. {Fischer} and B. T. {Meyer} and S. {Doclo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={DNN-Based Speech Presence Probability Estimation for Multi-Frame Single-Microphone Speech Enhancement},
year={2020},
volume={},
number={},
pages={191-195},
abstract={Multi-frame approaches for single-microphone speech enhancement, e.g., the multi-frame minimum-power-distortionless-response (MFMPDR) filter, are able to exploit speech correlations across neighboring time frames. In contrast to single-frame approaches such as the Wiener gain, it has been shown that multi-frame approaches achieve a substantial noise reduction with hardly any speech distortion, provided that an accurate estimate of the correlation matrices and especially the speech interframe correlation (IFC) vector is available. Typical estimation procedures of the IFC vector require an estimate of the speech presence probability (SPP) in each time-frequency (TF) bin. In this paper, we propose to use a bi-directional long short-term memory deep neural network (DNN) to estimate the SPP for each TF bin. Aiming at achieving a robust performance, the DNN is trained for various noise types and within a large signal-to-noise-ratio range. Experimental results show that the MFMPDR in combination with the proposed data-driven SPP estimator yields an increased speech quality compared to a state-of-the-art model-based SPP estimator. Furthermore, it is confirmed that exploiting interframe correlations in the MFMPDR is beneficial when compared to the Wiener gain especially in adverse scenarios.},
keywords={Speech Presence Probability;Deep Neural Network;Single-Microphone Speech Enhancement;Multi-Frame Filtering},
doi={10.1109/ICASSP40776.2020.9054196},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053210,
author={K. {Tesch} and T. {Gerkmann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Nonlinear Spatial Filtering for Multichannel Speech Enhancement in Inhomogeneous Noise Fields},
year={2020},
volume={},
number={},
pages={196-200},
abstract={A common processing pipeline for multichannel speech enhancement is to combine a linear spatial filter with a single-channel postfilter. In fact, it can be shown that such a combination is optimal in the minimum mean square error (MMSE) sense if the noise follows a multivariate Gaussian distribution. However, for non-Gaussian noise, this serial concatenation is generally suboptimal and may thus also lead to suboptimal results. For instance, in our previous work, we showed that a joint spatial-spectral nonlinear estimator achieves a performance gain of 2.6 dB segmental signal-to-noise ratio (SNR) improvement for heavy-tailed large-kurtosis multivariate noise compared to the traditional combination of a linear spatial beamformer and a postfilter.In this paper, we show that a joint spatial-spectral nonlinear filter is not only advantageous for noise distributions that are significantly more heavy-tailed than a Gaussian but also for distributions that model inhomogeneous noise fields while having rather low kurtosis. In experiments with artificially created noise we measure a gain of 1 dB for inhomogenous noise with low kurtosis and up to 2 dB for inhomogeneous noise fields with moderate kurtosis.},
keywords={Multichannel;speech enhancement;nonlinear filtering;acoustic beamforming},
doi={10.1109/ICASSP40776.2020.9053210},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054470,
author={H. W. {Löllmann} and A. {Brendel} and W. {Kellermann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Generalized Coherence-Based Signal Enhancement},
year={2020},
volume={},
number={},
pages={201-205},
abstract={This contribution presents a novel approach for coherence-based signal enhancement. An estimator for the coherent-to-diffuse ratio (CDR) is devised, which exploits the concept of generalized magnitude coherence and thus, unlike common state-of-the-art schemes, can simultaneously take advantage of more than two microphones. Moreover, the speech enhancement by CDR-based spectral weighting is not performed as a post-filtering step, but by enhancing the most appropriate microphone signal. This signal is implicitly determined as part of the CDR estimation such that the presented technique does not depend on an estimation of the direction-of-arrival (DOA) or similar side-information about the desired source.The application of the new approach to binaural hearings aids shows that it achieves a consistently better speech enhancement performance than comparable state-of-the-art approaches.},
keywords={CDR estimation;generalized coherence;multichannel signal enhancement;dereverberation;hearing aids},
doi={10.1109/ICASSP40776.2020.9054470},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053296,
author={S. {Maiti} and M. I. {Mandel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Speaker Independence of Neural Vocoders and Their Effect on Parametric Resynthesis Speech Enhancement},
year={2020},
volume={},
number={},
pages={206-210},
abstract={Traditional speech enhancement systems produce speech with compromised quality. Here we propose to use the high quality speech generation capability of neural vocoders for better quality speech enhancement. We term this parametric resynthesis (PR). In previous work, we showed that PR systems generate high quality speech for a single speaker using two neural vocoders, WaveNet and WaveGlow. Both these vocoders are traditionally speaker dependent. Here we first show that when trained on data from enough speakers, these vocoders can generate speech from unseen speakers, both male and female, with similar quality as seen speakers in training. Next using these two vocoders and a new vocoder LPCNet, we evaluate the noise reduction quality of PR on unseen speakers and show that objective signal and overall quality is higher than the state-of-the-art speech enhancement systems Wave-U-Net, Wavenet-denoise, and SEGAN. Moreover, in subjective quality, multiple-speaker PR out-performs the oracle Wiener mask.},
keywords={Speech enhancement;Neural vocoders;analysis-by-synthesis;enhancement-by-synthesis},
doi={10.1109/ICASSP40776.2020.9053296},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052988,
author={G. {Huang} and J. {Benesty} and J. {Chen} and I. {Cohen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust and steerable kronecker product differential beamforming With rectangular microphone arrays},
year={2020},
volume={},
number={},
pages={211-215},
abstract={Differential microphone arrays (DMAs), a class of welldesigned small-size arrays combined with differential beamforming, are very useful for processing broadband acoustic, audio, and speech signals in a wide range of applications. However, most efforts in the literature so far have been devoted to linear, circular, and spherical arrays. In this paper, we consider rectangular shapes of planar microphone arrays. Instead of adopting the traditional differential beamforming methods developed in the literature, we present a differential beamforming method based on the so-called Kronecker product. We first decompose the entire rectangular array into two virtual rectangular sub-arrays so that the steering vector of the entire array is the Kronecker product of the steering vectors of the two smaller virtual rectangular sub-arrays. We use the first virtual rectangular array, which is much smaller in size than the entire array but well satisfies the basic requirements for differential beamforming, to design a steerable differential beamformer. For the second virtual rectangular array, we can design either the delay-and-sum (DS) beamformer, which helps to improve the robustness of the global differential beamformer, or an adaptive beamformer, which makes the global differential beamformer adaptive. This method has many interesting properties, particularly the designed beamformer is fully steerable, and its robustness and the array gain can be easily controlled.},
keywords={Microphone arrays;Kronecker product;differential beamforming;adaptive beamforming},
doi={10.1109/ICASSP40776.2020.9052988},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054393,
author={C. {Boeddeker} and T. {Nakatani} and K. {Kinoshita} and R. {Haeb-Umbach}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Jointly Optimal Dereverberation and Beamforming},
year={2020},
volume={},
number={},
pages={216-220},
abstract={We previously proposed an optimal (in the maximum likelihood sense) convolutional beamformer that can perform simultaneous denoising and dereverberation, and showed its superiority over the widely used cascade of a Weighted Prediction Error (WPE) dereverberation filter and a conventional Minimum-Power Distortionless Response (MPDR) beamformer. However, it has not been fully investigated which components in the convolutional beamformer yield such superiority. To this end, this paper presents a new derivation of the convolutional beamformer that allows us to factorize it into a WPE dereverberation filter, and a special type of a (non-convolutional) beamformer, referred to as a weighted MPDR (wM-PDR) beamformer, without loss of optimality. With experiments, we show that the superiority of the convolutional beamformer in fact comes from its wMPDR part.},
keywords={Dereverberation;beamforming;speech enhancement},
doi={10.1109/ICASSP40776.2020.9054393},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054752,
author={S. {Woźniak} and K. {Kowalczyk}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Exploiting Rays in Blind Localization of Distributed Sensor Arrays},
year={2020},
volume={},
number={},
pages={221-225},
abstract={Many signal processing algorithms for distributed sensors are capable of improving their performance if the positions of sensors are known. In this paper, we focus on estimators for inferring the relative geometry of distributed arrays and sources, i.e. the setup geometry up to a scaling factor. Firstly, we present the Maximum Likelihood estimator derived under the assumption that the Direction of Arrival measurements follow the von Mises-Fisher distribution. Secondly, using unified notation, we show the relations between the cost functions of a number of state-of-the-art relative geometry estimators. Thirdly, we derive a novel estimator that exploits the concept of rays between the arrays and source event positions. Finally, we show the evaluation results for the presented estimators in various conditions, which indicate that major improvements in the probability of convergence to the optimum solution over the existing approaches can be achieved by using the proposed ray-based estimator.},
keywords={array processing;distributed sensor networks;geometry calibration;maximum likelihood;least squares},
doi={10.1109/ICASSP40776.2020.9054752},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054728,
author={N. {Akbar} and G. {Dickins} and M. R. P. {Thomas} and P. {Samarasinghe} and T. {Abhayapala}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Novel Method for Obtaining Diffuse Field Measurements for Microphone Calibration},
year={2020},
volume={},
number={},
pages={226-230},
abstract={We propose a straightforward and cost-effective method to perform diffuse soundfield measurements for calibrating the magnitude response of a microphone array. Typically, such calibration is performed in a diffuse soundfield created in reverberation chambers, an expensive and time-consuming process. A method is proposed for obtaining diffuse field measurements in untreated environments. First, a closed-form expression for the spatial correlation of a wideband signal in a diffuse field is derived. Next, we describe a practical procedure for obtaining the diffuse field response of a microphone array in the presence of a non-diffuse soundfield by the introduction of random perturbations in the microphone location. Experimental spatial correlation data obtained is compared with the theoretical model, confirming that it is possible to obtain diffuse field measurements in untreated environments with relatively few loudspeakers. A 30 second test signal played from 4–8 loudspeakers is shown to be sufficient in obtaining a diffuse field measurement using the proposed method. An Eigenmike® is then successfully calibrated at two different geographical locations.},
keywords={Spherical arrays;microphone calibration;diffuse field measurements;and spatial correlation},
doi={10.1109/ICASSP40776.2020.9054728},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054766,
author={M. {Togami}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Channel Speech Source Separation and Dereverberation With Sequential Integration of Determined and Underdetermined Models},
year={2020},
volume={},
number={},
pages={231-235},
abstract={In this paper, we propose a joint multi-channel speech source separation and dereverberation method in which multiple speech sources and late reverberation are separated in an unsupervised manner. The proposed method jointly optimizes an auto-regressive (AR) model based speech dereverberation and a time-varying multichannel Wiener filtering (MWF) based speech source separation. So as to increase separation and dereverberation performance and to overcome the inter-frequency permutation problem, the proposed method adopts a sequential parameter optimization strategy. At first, the parameter is updated based on a determined model, and the permutation problem can be solved based on the non-negative matrix factorization. The determined model reduces reverberation by only the AR model and residual reverberation remains. Inspired by the fact that a parameter of a determined model can be converted into a parameter of a underdetermined model, the proposed method regards residual reverberation as an additional source and reduces residual reverberation with the converted parameter based on the underdetermined model. We further propose additional update of the parameter based on the underdetermined model. Experimental results show that the proposed method outperforms the conventional method based on only the determined model and the proposed additional parameter update based on the underdetermined model is effective.},
keywords={Statistical modeling;determined speech source separation;underdetermined speech source separation;local Gaussian modeling},
doi={10.1109/ICASSP40776.2020.9054766},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053556,
author={R. {Scheibler} and N. {Ono}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast and Stable Blind Source Separation with Rank-1 Updates},
year={2020},
volume={},
number={},
pages={236-240},
abstract={We propose a new algorithm for the blind source separation of acoustic sources. This algorithm is an alternative to the popular auxiliary function based independent vector analysis using iterative projection (AuxIVA-IP). It optimizes the same cost function, but instead of alternate updates of the rows of the demixing matrix, we propose a sequence of rank-1 updates. Remarkably, and unlike the previous method, the resulting updates do not require matrix inversion. Moreover, their computational complexity is quadratic in the number of microphones, rather than cubic in AuxIVA-IP. In addition, we show that the new method can be derived as alternate updates of the steering vectors of sources. Accordingly, we name the method iterative source steering (AuxIVA-ISS). Finally, we confirm in simulated experiments that the proposed algorithm separates sources just as well as AuxIVA-IP, at a lower computational cost.},
keywords={Blind Source Separation;Auxiliary Function Optimization;Independent Vector Analysis;Mixing Matrix;Fast algorithm},
doi={10.1109/ICASSP40776.2020.9053556},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053093,
author={M. A. {Martínez Ramírez} and E. {Benetos} and J. D. {Reiss}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Modeling Plate and Spring Reverberation Using A Dsp-Informed Deep Neural Network},
year={2020},
volume={},
number={},
pages={241-245},
abstract={Plate and spring reverberators are electromechanical systems first used and researched as means to substitute real room reverberation. Currently, they are often used in music production for aesthetic reasons due to their particular sonic characteristics. The modeling of these audio processors and their perceptual qualities is difficult since they use mechanical elements together with analog electronics resulting in an extremely complex response. Based on digital reverberators that use sparse FIR filters, we propose a signal processing-informed deep learning architecture for the modeling of artificial reverberators. We explore the capabilities of deep neural networks to learn such highly nonlinear electromechanical responses and we perform modeling of plate and spring reverberators. In order to measure the performance of the model, we conduct a perceptual evaluation experiment and we also analyze how the given task is accomplished and what the model is actually learning.},
keywords={artificial reverberation;audio effects modeling;deep learning;sparse FIR},
doi={10.1109/ICASSP40776.2020.9053093},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054308,
author={S. {Wager} and G. {Tzanetakis} and C. {Wang} and M. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Autotuner: A Pitch Correcting Network for Singing Performances},
year={2020},
volume={},
number={},
pages={246-250},
abstract={We introduce a data-driven approach to automatic pitch correction of solo singing performances. The proposed approach predicts note-wise pitch shifts from the relationship between the respective spectrograms of the singing and accompaniment. This approach differs from commercial systems, where vocal track notes are usually shifted to be centered around pitches in a user-defined score, or mapped to the closest pitch among the twelve equal-tempered scale degrees. The proposed system treats pitch as a continuous value rather than relying on a set of discretized notes found in musical scores, thus allowing for improvisation and harmonization in the singing performance. We train our neural network model using a dataset of 4,702 amateur karaoke performances selected for good intonation. Our model is trained on both incorrect intonation, for which it learns a correction, and intentional pitch variation, which it learns to preserve. The proposed deep neural network with gated recurrent units on top of convolutional layers shows promising performance on the real-world score-free singing pitch correction task—autotuning.},
keywords={music information retrieval;singing voice;automatic pitch correction;deep learning;autotuning},
doi={10.1109/ICASSP40776.2020.9054308},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052944,
author={A. {Wright} and V. {Välimäki}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Perceptual loss function for neural modeling of audio systems},
year={2020},
volume={},
number={},
pages={251-255},
abstract={This work investigates alternate pre-emphasis filters used as part of the loss function during neural network training for nonlinear audio processing. In our previous work, the errorto-signal ratio loss function was used during network training, with a first-order high-pass pre-emphasis filter applied to both the target signal and neural network output. This work considers more perceptually relevant pre-emphasis filters, which include low-pass filtering at high frequencies. We conducted listening tests to determine whether they offer an improvement to the quality of a neural network model of a guitar tube amplifier. Listening test results indicate that the use of an A-weighting pre-emphasis filter offers the best improvement among the tested filters. The proposed perceptual loss function improves the sound quality of neural network models in audio processing without affecting the computational cost.},
keywords={Audio systems;deep learning;digital filters;nonlinear distortion;psychoacoustics},
doi={10.1109/ICASSP40776.2020.9052944},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054108,
author={S. I. {Mimilakis} and N. J. {Bryan} and P. {Smaragdis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={One-Shot Parametric Audio Production Style Transfer with Application to Frequency Equalization},
year={2020},
volume={},
number={},
pages={256-260},
abstract={Audio production is a difficult process for many people], [and properly manipulating sound to achieve a certain effect is non-trivial. In this paper], [we present a method that facilitates this process by inferring appropriate audio effect parameters in order to make an input recording sound similar to an unrelated reference recording. We frame our work as a form of parametric style transfer that], [by design], [leverages existing audio production semantics and manipulation algorithms], [avoiding several issues that have plagued audio style transfer algorithms in the past. To demonstrate our approach], [we consider the task of controlling a parametric], [four-band infinite impulse response equalizer and show that we are able to predict the parameters necessary to transform the equalization style of one recording to another. The framework we present], [however], [is applicable to a wider range of parametric audio effects.},
keywords={Parametric style transfer;one-shot learning;deep learning;parametric equalization},
doi={10.1109/ICASSP40776.2020.9054108},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054473,
author={J. {Parekh} and P. {Rao} and Y. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Speech-To-Singing Conversion in an Encoder-Decoder Framework},
year={2020},
volume={},
number={},
pages={261-265},
abstract={In this paper our goal is to convert a set of spoken lines into sung ones. Unlike previous signal processing based methods, we take a learning based approach to the problem. This allows us to automatically model various aspects of this transformation, thus overcoming dependence on specific inputs such as high quality singing templates or phoneme-score synchronization information. Specifically, we propose an encoder–decoder framework for our task. Given time-frequency representations of speech and a target melody contour, we learn encodings that enable us to synthesize singing that preserves the linguistic content and timbre of the speaker while adhering to the target melody. We also propose a multi-task learning based objective to improve lyric intelligibility. We present a quantitative and qualitative analysis of our framework.},
keywords={Speech-to-singing transformation;style transfer;machine learning;multi-task learning},
doi={10.1109/ICASSP40776.2020.9054473},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054688,
author={P. {Alonso-Jiménez} and D. {Bogdanov} and J. {Pons} and X. {Serra}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Tensorflow Audio Models in Essentia},
year={2020},
volume={},
number={},
pages={266-270},
abstract={Essentia is a reference open-source C++/Python library for audio and music analysis. In this work, we present a set of algorithms that employ TensorFlow in Essentia, allow predictions with pre-trained deep learning models, and are designed to offer flexibility of use, easy extensibility, and real-time inference. To show the potential of this new interface with TensorFlow, we provide a number of pre-trained state-of-the-art music tagging and classification CNN models. We run an extensive evaluation of the developed models. In particular, we assess the generalization capabilities in a cross-collection evaluation utilizing both external tag datasets as well as manual annotations tailored to the taxonomies of our models.},
keywords={music information retrieval;music tagging;deep learning;transfer learning;audio analysis software},
doi={10.1109/ICASSP40776.2020.9054688},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054344,
author={K. {Suefusa} and T. {Nishida} and H. {Purohit} and R. {Tanabe} and T. {Endo} and Y. {Kawaguchi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Anomalous Sound Detection Based on Interpolation Deep Neural Network},
year={2020},
volume={},
number={},
pages={271-275},
abstract={As the labor force decreases, the demand for labor-saving automatic anomalous sound detection technology that conducts maintenance of industrial equipment has grown. Conventional approaches detect anomalies based on the reconstruction errors of an autoencoder. However, when the target machine sound is non-stationary, a reconstruction error tends to be large independent of an anomaly, and its variations increased because of the difficulty of predicting the edge frames. To solve the issue, we propose an approach to anomalous detection in which the model utilizes multiple frames of a spectrogram whose center frame is removed as an input, and it predicts an interpolation of the removed frame as an output. Rather than predicting the edge frames, the proposed approach makes the reconstruction error consistent with the anomaly. Experimental results showed that the proposed approach achieved 27% improvement based on the standard AUC score, especially against non-stationary machinery sounds.},
keywords={Machine health monitoring;Anomaly detection;DNN;Autoencoder},
doi={10.1109/ICASSP40776.2020.9054344},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054248,
author={W. {Wei} and H. {Zhu} and E. {Benetos} and Y. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A-CRNN: A Domain Adaptation Model for Sound Event Detection},
year={2020},
volume={},
number={},
pages={276-280},
abstract={This paper presents a domain adaptation model for sound event detection. A common challenge for sound event detection is how to deal with the mismatch among different datasets. Typically, the performance of a model will decrease if it is tested on a dataset which is different from the one that the model is trained on. To address this problem, based on convolutional recurrent neural networks (CRNNs), we propose an adapted CRNN (A-CRNN) as an unsupervised adversarial domain adaptation model for sound event detection. We have collected and annotated a dataset in Singapore with two types of recording devices to complement existing datasets in the research community, especially with respect to domain adaptation. We perform experiments on recordings from different datasets and from different recordings devices. Our experimental results show that the proposed A-CRNN model can achieve a better performance on an unseen dataset in comparison with the baseline non-adapted CRNN model.},
keywords={sound event detection;domain adaptation;computational sound scene analysis;CRNNs},
doi={10.1109/ICASSP40776.2020.9054248},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053620,
author={Y. {Koizumi} and M. {Yasuda} and S. {Murata} and S. {Saito} and H. {Uematsu} and N. {Harada}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={SPIDERnet: Attention Network For One-Shot Anomaly Detection In Sounds},
year={2020},
volume={},
number={},
pages={281-285},
abstract={We propose a similarity function for one-shot anomaly detection in sounds (ADS) called SPecific anomaly IDentifiER network (SPI- DERnet). In ADS systems, since overlooking an anomaly may re- sult in serious incidents, we need to update such systems using an (often only one) overlooked anomalous sample. A previous study proposed the use of memory-based one-shot learning. A problem with this previous method is that it can detect only short anomalous sounds such as collision sounds because its similarity function is based on a naive mean-squared-error between the input and memo- rized spectrogram. To detect various anomalous sounds, SPIDERnet consists of (i) a neural network-based feature extractor for measur- ing similarity in embedded space and (ii) attention mechanisms for absorbing time-frequency stretching. Experimental results on two public datasets indicate that SPIDERnet outperforms conventional methods and robustly detects various anomalous sounds.},
keywords={Anomaly detection in sounds;acoustic condition monitoring;one-shot learning;and multi-head attention},
doi={10.1109/ICASSP40776.2020.9053620},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054433,
author={Y. {Li} and M. {Liu} and K. {Drossos} and T. {Virtanen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sound Event Detection Via Dilated Convolutional Recurrent Neural Networks},
year={2020},
volume={},
number={},
pages={286-290},
abstract={Convolutional recurrent neural networks (CRNNs) have achieved state-of-the-art performance for sound event detection (SED). In this paper, we propose to use a dilated CRNN, namely a CRNN with a dilated convolutional kernel, as the classifier for the task of SED. We investigate the effectiveness of dilation operations which provide a CRNN with expanded receptive fields to capture long temporal context without increasing the amount of CRNN’s parameters. Compared to the classifier of the baseline CRNN, the classifier of the dilated CRNN obtains a maximum increase of 1.9%, 6.3% and 2.5% at F1 score and a maximum decrease of 1.7%, 4.1% and 3.9% at error rate (ER), on the publicly available audio corpora of the TUTSED Synthetic 2016, the TUT Sound Event 2016 and the TUT Sound Event 2017, respectively.},
keywords={Sound event detection;dilated convolutional recurrent neural network;temporal context},
doi={10.1109/ICASSP40776.2020.9054433},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054208,
author={M. {Mulimani} and A. B. {Kademani} and S. G. {Koolagudi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Deep Neural Network-Driven Feature Learning Method for Polyphonic Acoustic Event Detection from Real-Life Recordings},
year={2020},
volume={},
number={},
pages={291-295},
abstract={In this paper, a Deep Neural Network (DNN)-driven feature learning method for polyphonic Acoustic Event Detection (AED) is proposed. The proposed DNN is a combination of different layers used to characterize multiple overlapped acoustic events in the mixture. During training, DNN is able to learn the optimal set of discriminative spectral characteristics of the overlapped (polyphonic) acoustic events. The performance of the proposed method is evaluated on the TUT Sound Event 2016 (TUT-SED 2016) real-life dataset and joint Acoustic Scene Classification (ASC) and polyphonic AED dataset. Results show that proposed approach outperforms the state-of-the-art methods.},
keywords={Polyphonic Acoustic Event Detection (AED);Deep Neural Network (DNN)-driven feature learning method;Detection and Classification of Acoustic Scenes and Events (DCASE);Convolutional Neural Networks (CNN)},
doi={10.1109/ICASSP40776.2020.9054208},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053427,
author={S. {Hong} and Y. {Zou} and W. {Wang} and M. {Cao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Weakly Labelled Audio Tagging Via Convolutional Networks with Spatial and Channel-Wise Attention},
year={2020},
volume={},
number={},
pages={296-300},
abstract={Multiple instance learning (MIL) with convolutional neural networks (CNNs) has been proposed recently for weakly labelled audio tagging. However, features from the various CNN filtering channels and spatial regions are often treated equally, which may limit its performance in event prediction. In this paper, we propose a novel attention mechanism, namely, spatial and channel-wise attention (SCA). For spatial attention, we divide it into global and local submodules with the former to capture the event-related spatial regions and the latter to estimate the onset and offset of the events. Considering the variations in CNN channels, channel-wise attention is also exploited to recognize different sound scenes. The proposed SCA can be employed into any CNNs seamlessly with affordable overheads and is end-to-end trainable fashion. Extensive experiments on weakly labelled dataset Audioset show that the proposed SCA with CNNs achieves a state-of-the-art mean average precision (mAP) of 0.390.},
keywords={Audio tagging;weakly labelled data;multiple instance learning;spatial attention;channel-wise attention},
doi={10.1109/ICASSP40776.2020.9053427},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054445,
author={V. {Subramanian} and A. {Pankajakshan} and E. {Benetos} and N. {Xu} and S. {McDonald} and M. {Sandler}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Study on the Transferability of Adversarial Attacks in Sound Event Classification},
year={2020},
volume={},
number={},
pages={301-305},
abstract={An adversarial attack is an algorithm that perturbs the input of a machine learning model in an intelligent way in order to change the output of the model. An important property of adversarial attacks is transferability. According to this property, it is possible to generate adversarial perturbations on one model and apply it the input to fool the output of a different model. Our work focuses on studying the transferability of adversarial attacks in sound event classification. We are able to demonstrate differences in transferability properties from those observed in computer vision. We show that dataset normalization techniques such as z-score normalization does not affect the transferability of adversarial attacks and we show that techniques such as knowledge distillation do not increase the transferability of attacks.},
keywords={Adversarial attacks;transferability;audio tagging;sound event classification},
doi={10.1109/ICASSP40776.2020.9054445},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054024,
author={M. {Thomas} and F. {Lionel} and D. {Laurent}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Propeller Noise Detection with Deep Learning},
year={2020},
volume={},
number={},
pages={306-310},
abstract={Due to the complexity of environment and source modelling, underwater target detection is a rather challenging task. In the Deep Learning community, many attempts were made to deal with this problem, mainly through expert features, but few assessed the benefit of using raw acoustic signals. In this paper, we propose a new model of underwater propeller noise as well as its optimal statistical detector for detecting the presence of propeller in acoustic signal. We then design a deep learning architecture which approximates this optimal detector using only some training signal samples. Numerical simulations confirm the efficiency of the approach.},
keywords={Underwater acoustic signal;Amplitude modulation;Bayes detection;CNN},
doi={10.1109/ICASSP40776.2020.9054024},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053459,
author={H. {Dinkel} and K. {Yu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Duration Robust Weakly Supervised Sound Event Detection},
year={2020},
volume={},
number={},
pages={311-315},
abstract={Task 4 of the DCASE2018 challenge demonstrated that substantially more research is needed for a real-world application of sound event detection. Analyzing the challenge results it can be seen that most successful models are biased towards predicting long (e.g., over 5s) clips. This work aims to investigate the performance impact of fixed-sized window median filter post-processing and advocate the use of double thresholding as a more robust and predictable post-processing method. Further, four different temporal subsampling methods within the CRNN framework are proposed: mean-max, α-mean-max, Lp-norm and convolutional. We show that for this task subsampling the temporal resolution by a neural network enhances the F1 score as well as its robustness towards short, sporadic sound events. Our best single model achieves 30.1% F1 on the evaluation set and the best fusion model 32.5%, while being robust to event length variations.},
keywords={weakly supervised sound event detection;convolutional neural networks;recurrent neural networks;semi-supervised duration estimation},
doi={10.1109/ICASSP40776.2020.9053459},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053150,
author={C. {Kao} and M. {Sun} and W. {Wang} and C. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Comparison of Pooling Methods on LSTM Models for Rare Acoustic Event Classification},
year={2020},
volume={},
number={},
pages={316-320},
abstract={Acoustic event classification (AEC) and acoustic event detection (AED) refer to the task of detecting whether specific target events occur in audios. As long short-term memory (LSTM) leads to state-of-the-art results in various speech related tasks, it is employed as a popular solution for AEC as well. This paper focuses on investigating the dynamics of LSTM model on AEC tasks. It includes a detailed analysis on LSTM memory retaining, and a benchmarking of nine different pooling methods on LSTM models using 1.7M generated mixture clips of multiple events with different signal-to-noise ratios. This paper focuses on understanding: 1) utterance-level classification accuracy; 2) sensitivity to event position within an utterance. The analysis is done on the dataset for the detection of rare sound events from DCASE 2017 Challenge. We find max pooling on the prediction level to perform the best among the nine pooling approaches in terms of classification accuracy and insensitivity to event position within an utterance. To authors’ best knowledge, this is the first kind of such work focused on LSTM dynamics for AEC tasks.},
keywords={Long short-term memory (LSTM);acoustic event classification and detection;pooling functions},
doi={10.1109/ICASSP40776.2020.9053150},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053389,
author={Y. {Sun} and S. {Ghaffarzadegan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Ontology-Aware Framework for Audio Event Classification},
year={2020},
volume={},
number={},
pages={321-325},
abstract={Recent advancements in audio event classification often ignore the structure and relation between the label classes available as prior information. This structure can be defined by ontology and augmented in the classifier as a form of domain knowledge. To capture such dependencies between the labels, we propose an ontology-aware neural network containing two components: feed-forward ontology layers and graph convolutional networks (GCN). The feed-forward ontology layers capture the intra-dependencies of labels between different levels of ontology. On the other hand, GCN mainly models interdependency structure of labels within an ontology level. The framework is evaluated on two benchmark datasets for single-label and multi-label audio event classification tasks. The results demonstrate the proposed solutions efficacy to capture and explore the ontology relations and improve the classification performance.},
keywords={Audio event classification;Ontology structure;Graph convolutional networks;Ontology layers},
doi={10.1109/ICASSP40776.2020.9053389},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053073,
author={J. {Yan} and Y. {Song} and L. {Dai} and I. {McLoughlin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Task-Aware Mean Teacher Method for Large Scale Weakly Labeled Semi-Supervised Sound Event Detection},
year={2020},
volume={},
number={},
pages={326-330},
abstract={Weakly labeled semi-supervised learning methods have recently drawn increasing attention from the research community for sound event detection tasks. Due to the weakness of the labelling, neural networks are often designed to perform sound event detection (SED) and audio tagging (AT) at the same time. In this paper, we propose a task-aware mean teacher method using a convolutional recurrent neural network (CRNN) with multi-branch structure to solve the SED and AT tasks differently. Specifically, a branch with coarse-level temporal resolution is designed for the AT task, while a branch with fine-level temporal resolution is designed for the SED task. The mean teacher based semi-supervised learning method is first adopted to improve the performance of the coarse-level AT branch by exploiting unlabeled data. Then the coarse-level AT branch is introduced as a teacher to guide the aggregated AT output of the fine-level SED branch, yielding an improvement in the SED performance. To further improve the AT and SED performance, information from multiple layers is exploited in the form of a multi-resolution feature. Experimental results on Task4 of the DCASE2018 challenge demonstrate the superiority of the proposed method, achieving 37.7% F1-score, which outperforms the winning system’s 32.4%.},
keywords={weakly labeled;semi-supervised learning;sound event detection;audio tagging},
doi={10.1109/ICASSP40776.2020.9053073},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054204,
author={A. A. {Catellier} and S. D. {Voran}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Wawenets: A No-Reference Convolutional Waveform-Based Approach to Estimating Narrowband and Wideband Speech Quality},
year={2020},
volume={},
number={},
pages={331-335},
abstract={Building on prior work we have developed a no-reference (NR) waveform-based convolutional neural network (CNN) architecture that can accurately estimate speech quality or intelligibility of narrowband and wideband speech segments. These Wideband Audio Waveform Evaluation Networks, or WAWEnets, achieve very high per-speech-segment correlation (ρseg ≥ 0.92, RMSE ≤ 0.38) to established full-reference quality and intelligibility estimators (PESQ, POLQA, PEMO, STOI) based on over 17 hours of speech from 127 previously unseen talkers speaking in 13 different languages; just 10% of our total data. NR correlations at this level across such a broad scope are unprecedented. This achievement was made possible by using full-reference estimates as training targets so that WAWEnets could learn implicit undistorted speech models and exploit them to produce accurate NR estimates.},
keywords={convolutional neural net;no-reference;speech intelligibility;speech quality;wideband},
doi={10.1109/ICASSP40776.2020.9054204},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052949,
author={M. B. {Pedersen} and A. {Heidemann Andersen} and S. H. {Jensen} and J. {Jensen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Neural Network for Monaural Intrusive Speech Intelligibility Prediction},
year={2020},
volume={},
number={},
pages={336-340},
abstract={Monaural intrusive speech intelligibility prediction (SIP) methods aim to predict the speech intelligibility (SI) of a single-microphone noisy and/or processed speech signal using the underlying clean speech signal. In the present work, we propose a neural network for monaural intrusive SIP. The proposed network is trained on data from multiple listening tests to predict SI. In the interest of using the available listening test data as efficiently as possible and to facilitate SI prediction of short duration speech signals, training is based on a local-time intelligibility curve derived from the listening test data. The trained neural network is evaluated, in terms of rank order correlation, against the classical monaural intrusive predictors STOI and ESTOI. The network is found to perform the best overall with a Kendall’s tau of 0.825 measured over long duration, i.e. speech signals up to several minutes in duration. For short-term prediction using short speech signals of 1 - 10 seconds the network also shows better performance and smaller prediction variance.},
keywords={speech;intelligibility;prediction;monaural;intrusive},
doi={10.1109/ICASSP40776.2020.9052949},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053220,
author={R. {Fejgin} and J. {Klejsa} and L. {Villemoes} and C. {Zhou}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Source Coding of Audio Signals with a Generative Model},
year={2020},
volume={},
number={},
pages={341-345},
abstract={We consider source coding of audio signals with the help of a generative model. We use a construction where a waveform is first quantized, yielding a finite bitrate representation. The waveform is then reconstructed by random sampling from a model conditioned on the quantized waveform. The proposed coding scheme is theoretically analyzed. Using SampleRNN as the generative model, we demonstrate that the proposed coding structure provides performance competitive with state-of-the-art source coding tools for specific categories of audio signals.},
keywords={audio coding;source coding;deep learning},
doi={10.1109/ICASSP40776.2020.9053220},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053951,
author={G. {Mittag} and S. {Möller}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Full-Reference Speech Quality Estimation with Attentional Siamese Neural Networks},
year={2020},
volume={},
number={},
pages={346-350},
abstract={In this paper, we present a full-reference speech quality prediction model with a deep learning approach. The model determines a feature representation of the reference and the degraded signal through a Siamese recurrent convolutional network that shares the weights for both signals as input. The resulting features are then used to align the signals with an attention mechanism and are finally combined to estimate the overall speech quality. The proposed network architecture represents a simple solution for the time-alignment problem that occurs for speech signals transmitted through Voice-Over-IP networks and shows how the clean reference signal can be incorporated into speech quality models that are based on end-to-end trained neural networks.},
keywords={speech quality;deep learning;Siamese networks;attention;CNN-LSTM},
doi={10.1109/ICASSP40776.2020.9053951},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054409,
author={S. {Shin} and S. K. {Beack} and W. {Lim} and H. {Park}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Enhanced Method of Audio Coding Using CNN-Based Spectral Recovery with Adaptive Structure},
year={2020},
volume={},
number={},
pages={351-355},
abstract={A process of spectral recovery can enhance the performance of transform-based audio coding by transmitting only a portion of spectral data and recovering the missing spectral data in the decoder. This study proposes an enhanced method of audio coding based on spectral recovery with an adaptive structure that yields improved sound quality compared with the previous method. The spectral data to be recovered are arranged in an adaptive pattern depending on the difficulty of recovery. In addition, according to the spectral characteristics, prior information associated with these spectral data is selectively transmitted that helps a neural network improve the performance of magnitude recovery. Prior information also provides the signs of recovered magnitudes. A subjective performance evaluation shows that, for mono coding without window switching at 40 kbps, the proposed coding method provides better sound quality than the conventional method on average.1},
keywords={adaptive structure;audio coding;autoencoder;convolutional neural network;spectral recovery},
doi={10.1109/ICASSP40776.2020.9054409},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053113,
author={A. {Biswas} and D. {Jia}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Audio Codec Enhancement with Generative Adversarial Networks},
year={2020},
volume={},
number={},
pages={356-360},
abstract={Audio codecs are typically transform-domain based and efficiently code stationary audio signals, but they struggle with speech and signals containing dense transient events such as applause. Specifically, with these two classes of signals as examples, we demonstrate a technique for restoring audio from coding noise based on generative adversarial networks (GAN). A primary advantage of the proposed GAN-based coded audio enhancer is that the method operates end-to-end directly on decoded audio samples, eliminating the need to design any manually-crafted frontend. Furthermore, the enhancement approach described in this paper can improve the sound quality of low-bit rate coded audio without any modifications to the existent standard-compliant encoders. Subjective tests illustrate that the proposed enhancer improves the quality of speech and difficult to code applause excerpts significantly.},
keywords={Audio coding;coded audio enhancement;generative adversarial networks;convolutional neural networks},
doi={10.1109/ICASSP40776.2020.9053113},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054347,
author={K. {Zhen} and M. S. {Lee} and J. {Sung} and S. {Beack} and M. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Efficient and Scalable Neural Residual Waveform Coding with Collaborative Quantization},
year={2020},
volume={},
number={},
pages={361-365},
abstract={Scalability and efficiency are desired in neural speech codecs, which supports a wide range of bitrates for applications on various devices. We propose a collaborative quantization (CQ) scheme to jointly learn the codebook of LPC coefficients and the corresponding residuals. CQ does not simply shoehorn LPC to a neural network, but bridges the computational capacity of advanced neural network models and traditional, yet efficient and domain-specific digital signal processing methods in an integrated manner. We demonstrate that CQ achieves much higher quality than its predecessor at 9 kbps with even lower model complexity. We also show that CQ can scale up to 24 kbps where it outperforms AMR-WB and Opus. As a neural waveform codec, CQ models are with less than 1 million parameters, significantly less than many other generative models.},
keywords={Speech coding;linear predictive coding;deep neural network;residual learning;model complexity},
doi={10.1109/ICASSP40776.2020.9054347},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054499,
author={K. {Zhen} and M. S. {Lee} and M. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Dual-Staged Context Aggregation Method towards Efficient End-to-End Speech Enhancement},
year={2020},
volume={},
number={},
pages={366-370},
abstract={In speech enhancement, an end-to-end deep neural network converts a noisy speech signal to a clean speech directly in the time domain without time-frequency transformation or mask estimation. However, aggregating contextual information from a high-resolution time domain signal with an affordable model complexity still remains challenging. In this paper, we propose a densely connected convolutional and recurrent network (DCCRN), a hybrid architecture, to enable dual-staged temporal context aggregation. With the dense connectivity and cross-component identical shortcut, DCCRN consistently outperforms competing convolutional baselines with an average STOI improvement of 0.23 and PESQ of 1.38 at three SNR levels. The proposed method is computationally efficient with only 1.38 million parameters. The generalizability performance on the unseen noise types is still decent considering its low complexity, although it is relatively weaker comparing to Wave-U-Net with 7.25 times more parameters.},
keywords={End-to-end;speech enhancement;context aggregation;residual learning;dilated convolution;recurrent network},
doi={10.1109/ICASSP40776.2020.9054499},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053164,
author={S. {Leglaive} and X. {Alameda-Pineda} and L. {Girin} and R. {Horaud}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Recurrent Variational Autoencoder for Speech Enhancement},
year={2020},
volume={},
number={},
pages={371-375},
abstract={This paper presents a generative approach to speech enhancement based on a recurrent variational autoencoder (RVAE). The deep generative speech model is trained using clean speech signals only, and it is combined with a nonnegative matrix factorization noise model for speech enhancement. We propose a variational expectation-maximization algorithm where the encoder of the RVAE is finetuned at test time, to approximate the distribution of the latent variables given the noisy speech observations. Compared with previous approaches based on feed-forward fully-connected architectures, the proposed recurrent deep generative speech model induces a posterior temporal dynamic over the latent variables, which is shown to improve the speech enhancement results.},
keywords={Speech enhancement;recurrent variational autoencoders;nonnegative matrix factorization;variational inference},
doi={10.1109/ICASSP40776.2020.9053164},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054222,
author={S. {He} and H. {Li} and X. {Zhang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Speakerfilter: Deep Learning-Based Target Speaker Extraction Using Anchor Speech},
year={2020},
volume={},
number={},
pages={376-380},
abstract={Speaker extraction aims to separate a target speaker from multiple voices which is useful for applications, e.g. teleconference. In many practical cases, it has an opportunity to get a piece voice of the target speaker in advance, which provides useful information for speaker extraction. This paper addresses the problem of extracting the target speaker from the mixture using a short piece of anchor speech. To effectively utilize anchor speech, we propose a multi-level feature extraction and seamlessly integrate the features into a speech separation model. Experiments are conducted on the two-speaker dataset (WSJ0-mix2) which is widely used for speaker extraction. The systematic evaluation shows that the proposed method significantly outperforms the previous methods and achieves a signal-to-distortion ratio (SDR) improvement of 11.3 dB on the unprocessed mixture.},
keywords={Speaker extraction;neural network;anchor information;deepfilter},
doi={10.1109/ICASSP40776.2020.9054222},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054577,
author={K. {Kinoshita} and M. {Delcroix} and S. {Araki} and T. {Nakatani}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Tackling Real Noisy Reverberant Meetings with All-Neural Source Separation, Counting, and Diarization System},
year={2020},
volume={},
number={},
pages={381-385},
abstract={Automatic meeting analysis is an essential fundamental technology required to let, e.g. smart devices follow and respond to our conversations. To achieve an optimal automatic meeting analysis, we previously proposed an all-neural approach that jointly solves source separation, speaker diarization and source counting problems in an optimal way (in a sense that all the 3 tasks can be jointly optimized through error back-propagation). It was shown that the method could well handle simulated clean (noiseless and anechoic) dialog-like data, and achieved very good performance in comparison with several conventional methods. However, it was not clear whether such all-neural approach would be successfully generalized to more complicated real meeting data containing more spontaneously-speaking speakers, severe noise and reverberation, and how it performs in comparison with the state-of-the-art systems in such scenarios. In this paper, we first consider practical issues required for improving the robustness of the all-neural approach, and then experimentally show that, even in real meeting scenarios, the all-neural approach can perform effective speech enhancement, and simultaneously outperform state-of-the-art systems.},
keywords={Diarization;separation;source counting},
doi={10.1109/ICASSP40776.2020.9054577},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053934,
author={T. {Nakamura} and H. {Saruwatari}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Time-Domain Audio Source Separation Based on Wave-U-Net Combined with Discrete Wavelet Transform},
year={2020},
volume={},
number={},
pages={386-390},
abstract={We propose a time-domain audio source separation method using down-sampling (DS) and up-sampling (US) layers based on a discrete wavelet transform (DWT). The proposed method is based on one of the state-of-the-art deep neural networks, Wave-U-Net, which successively down-samples and up-samples feature maps. We find that this architecture resembles that of multiresolution analysis, and reveal that the DS layers of Wave-U-Net cause aliasing and may discard information useful for the separation. Although the effects of these problems may be reduced by training, to achieve a more reliable source separation method, we should design DS layers capable of overcoming the problems. With this belief, focusing on the fact that the DWT has an anti-aliasing filter and the perfect reconstruction property, we design the proposed layers. Experiments on music source separation show the efficacy of the proposed method and the importance of simultaneously considering the anti-aliasing filters and the perfect reconstruction property.},
keywords={Time-Domain Audio Source Separation;Wave-U-Net;Discrete Wavelet Transform;Deep Neural Networks},
doi={10.1109/ICASSP40776.2020.9053934},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053360,
author={S. {Spagnol}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Auditory Model Based Subsetting of Head-Related Transfer Function Datasets},
year={2020},
volume={},
number={},
pages={391-395},
abstract={The rising availability of public head-related transfer function (HRTF) data, measured on hundreds of different individuals, offers a user the possibility to select the best matching non-individual HRTF from a wide catalogue. To this end, reducing the number of alternatives to a small subset of candidate HRTFs is the first step towards an efficient selection process. In this article a novel HRTF subset selection algorithm based on auditory-model vertical localization predictions and a greedy heuristic is outlined, designed to identify a representative HRTF subset from a catalogue including the three biggest public datasets currently available (373 HRTFs overall). The so-resulting subset (6 HRTFs) is then evaluated on a fourth independent dataset. Auditory model predictions show that for over 95% of the subjects of this dataset there exists at least one HRTF out of the representative subset scoring minimal vertical localization error deviations compared to the best available non-individual HRTF out of the catalogue.},
keywords={Auditory model;binaural;HRTF selection;sound localization},
doi={10.1109/ICASSP40776.2020.9053360},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052970,
author={N. J. {Bryan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Impulse Response Data Augmentation and Deep Neural Networks for Blind Room Acoustic Parameter Estimation},
year={2020},
volume={},
number={},
pages={1-5},
abstract={The reverberation time (T60) and the direct-to-reverberant ratio (DRR) are commonly used to characterize room acoustic environments. Both parameters can be measured from an acoustic impulse response (AIR) or using blind estimation methods that perform estimation directly from speech. When neural networks are used for blind estimation, however, a large realistic dataset is needed, which is expensive and time consuming to collect. To address this, we propose an AIR augmentation method that can parametrically control the T60 and DRR, allowing us to expand a small dataset of real AIRs into a balanced dataset orders of magnitude larger. Using this method, we train a previously proposed convolutional neural network (CNN) and show we can outperform past single-channel state-of-the-art methods. We then propose a more efficient, straightforward baseline CNN that is 4-5x faster, which provides an additional improvement and is better or comparable to all previously reported single- and multi-channel state-of-the-art methods.},
keywords={Blind acoustic parameter estimation;data augmentation;reverberation time;direct-to-reverberant ratio},
doi={10.1109/ICASSP40776.2020.9052970},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052928,
author={M. {Zhang} and X. {Wu} and T. {Qu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Individual Distance-Dependent HRTFS Modeling Through A Few Anthropometric Measurements},
year={2020},
volume={},
number={},
pages={401-405},
abstract={The lack of data is a major problem in individual HRTF modeling. There are many HRTF databases, but each database only has limited HRTFs with different characteristics, such as distance-dependent HRTFs or individual HRTFs. How to effectively model HRTFs through several different databases is an important task. In this paper, a method for predicting individual distance-dependent HRTFs using a few anthropometric parameters is proposed. By modeling the HRTFs in CIPIC database, which contains individual HRTFs in 1 meter, and the PKU&IOA database, which contains KEMAR HRTFs in eight distances, we predict the individual HRTFs in arbitrary directions and distances. The objective experiments show that the proposed model has less spectral distortions than distance variation function model. The subjective experiments show that the proposed model can predict the individual HRTFs in arbitrary directions and distances.},
keywords={HRTF;SPCA;DNN;anthropometric parameters},
doi={10.1109/ICASSP40776.2020.9052928},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054610,
author={A. {Ivry} and I. {Cohen} and B. {Berdugo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Evaluation of Deep-Learning-Based Voice Activity Detectors and Room Impulse Response Models in Reverberant Environments},
year={2020},
volume={},
number={},
pages={406-410},
abstract={State-of-the-art deep-learning-based voice activity detectors (VADs) are often trained with anechoic data. However, real acoustic environments are generally reverberant, which causes the performance to significantly deteriorate. To mitigate this mismatch between training data and real data, we simulate an augmented training set that contains nearly five million utterances. This extension comprises of anechoic utterances and their reverberant modifications, generated by convolutions of the anechoic utterances with a variety of room impulse responses (RIRs). We consider five different models to generate RIRs, and five different VADs that are trained with the augmented training set. We test all trained systems in three different real reverberant environments. Experimental results show 20% increase on average in accuracy, precision and recall for all detectors and response models, compared to anechoic training. Furthermore, one of the RIR models consistently yields better performance than the other models, for all the tested VADs. Additionally, one of the VADs consistently outperformed the other VADs in all experiments.},
keywords={Voice activity detection;reverberation;room impulse response;deep learning},
doi={10.1109/ICASSP40776.2020.9054610},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053873,
author={M. {Geronazzo} and J. Y. {Tissieres} and S. {Serafin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Minimal Personalization of Dynamic Binaural Synthesis with Mixed Structural Modeling and Scattering Delay Networks},
year={2020},
volume={},
number={},
pages={411-415},
abstract={This paper provides a small set of essential parameters for a personalized and effective real-time auralization with headphones. An image-guided procedure with two 2D images of the user’s head guides the mixed structural modeling of head-related transfer function (HRTF), combining a spherical head model with ear displacement with the HRTF high-frequency magnitude selected from a database according to ear anthropometry. Room acoustics phenomena are simplified following the scattering delay network (SDN) approach which allows an accurate spatialization of first order reflections. Finally, statically significant improvements in localization performances within a virtual reality (VR) test allow to identify some benefits of the proposed customized auralization model compared to the widely used higher-order ambisonics (HOA) rendering with generic HRTFs.},
keywords={binaural synthesis;auralization;personalization;head-related transfer function;sound localization;mixed structural modeling;room acoustics;scattering delay network},
doi={10.1109/ICASSP40776.2020.9053873},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053376,
author={H. {Caracalla} and A. {Roebel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sound Texture Synthesis Using RI Spectrograms},
year={2020},
volume={},
number={},
pages={416-420},
abstract={This article introduces a new parametric synthesis method for sound textures based on existing works in visual and sound texture synthesis. Starting from a base sound signal, an optimization process is performed until the cross-correlations between the feature-maps of several untrained 2D Convolutional Neural Networks (CNN) resemble those of an original sound texture. We use compressed Real-Imaginary (RI) spectrograms as input to the CNN: this time-frequency representation is the stacking of the real and imaginary part of the Short Time Fourier Transform (STFT) and thus implicitly contains both the magnitude and phase information, allowing for convincing syntheses of various audio events. The optimization is however performed directly on the time signal to avoid any STFT consistency issue. The results of an online perceptual evaluation are also detailed, and show that this method achieves results that are more realistic-sounding than existing parametric methods on a wide array of textures.},
keywords={Sound texture;CNN;RI spectrograms},
doi={10.1109/ICASSP40776.2020.9053376},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054561,
author={J. {Daniel} and S. {Kitić}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Time Domain Velocity Vector for Retracing the Multipath Propagation},
year={2020},
volume={},
number={},
pages={421-425},
abstract={We propose a conceptually and computationally simple form of sound velocity that offers a readable view of the interference between direct and indirect sound waves. Unlike most approaches in the literature, it jointly exploits both active and reactive sound intensity measurements, as typically derived from a first order ambisonics recording. This representation has a potential both as a valuable tool for directly analyzing sound multipath propagation, as well as being a new spatial feature format for machine learning algorithms in audio and acoustics. As a showcase, we demonstrate that the Direction-of-Arrival and the range of a sound source can be estimated as a development of this approach. To the best knowledge of the authors, this is the first time that range is estimated from an ambisonics recording.},
keywords={Ambisonics;intensity;localization;DoA;distance},
doi={10.1109/ICASSP40776.2020.9054561},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054701,
author={J. {Su} and Z. {Jin} and A. {Finkelstein}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Acoustic Matching By Embedding Impulse Responses},
year={2020},
volume={},
number={},
pages={426-430},
abstract={The goal of acoustic matching is to transform an audio recording made in one acoustic environment to sound as if it had been recorded in a different environment, based on reference audio from the target environment. This paper introduces a deep learning solution for two parts of the acoustic matching problem. First, we characterize acoustic environments by mapping audio into a low-dimensional embedding invariant to speech content and speaker identity. Next, a waveform-to-waveform neural network conditioned on this embedding learns to transform an input waveform to match the acoustic qualities encoded in the target embedding. Listening tests on both simulated and real environments show that the proposed approach improves on state-of-the-art baseline methods.},
keywords={Acoustic Matching;Acoustic Impulse Response;Equalization Matching;Embedding;Reverberation},
doi={10.1109/ICASSP40776.2020.9054701},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054532,
author={D. {Looney} and N. D. {Gaubitch}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Joint Estimation Of Acoustic Parameters From Single-Microphone Speech Observations},
year={2020},
volume={},
number={},
pages={431-435},
abstract={Key parameters that characterise the degree of degradation in an acoustic environment are reverberation time (RT), direct-to-reverberant ratio (DRR) and signal-to-noise ratio (SNR). To address the inherent interplay that exists between these parameters, which can hinder existing methods designed to estimate only a single parameter, we propose a data-driven solution to jointly estimate all three parameters using a convolutional neural network. To facilitate robustness to unseen acoustic conditions, the method is trained using a large set of simulated acoustic impulse responses that have been carefully selected so as to mitigate interplays that exist between RT and DRR. In this work we evaluate the performance of the proposed estimator with respect to reverberation only. Results show the estimator compares favourably with respect to the state-of-the-art for unseen and real acoustic scenarios.},
keywords={direct-to-reverberant ratio;reverberation time;convolutional neural network},
doi={10.1109/ICASSP40776.2020.9054532},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054461,
author={L. {Shi} and T. {Lee} and L. {Zhang} and J. K. {Nielsen} and M. {Grœsbøll Christensen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Fast Reduced-Rank Sound Zone Control Algorithm Using The Conjugate Gradient Method},
year={2020},
volume={},
number={},
pages={436-440},
abstract={Sound zone control enables different users to enjoy different audio contents in the same acoustic environment. Generalized eigenvalue decomposition (GEVD)-based methods allow us to control the tradeoff between the acoustic contrast (AC) and signal distortion (SD). However, such methods have a high computational complexity. In this paper, we propose a fast reduced-rank sound zone control algorithm using the conjugate gradient (CG) method. Instead of using the eigenvectors as the basis for the solution space, the search directions in the CG method are used to reduce the computational complexity. Then, a low dimensional EVD is applied to obtain the sub-optimal control filter coefficients. The dark zone power can be adjusted by a parameter, which implicitly controls the trade-off between the AC and SD. Compared with GEVD-based methods, experimental results show that the proposed algorithm has a degradation of performance (4–5 dB) in terms of AC or SD but a high improvement on computational efficiency.},
keywords={Computational complexity;conjugate gradient;reduced-rank;sound zone control;variable span trade-off filter},
doi={10.1109/ICASSP40776.2020.9054461},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054366,
author={M. {Guo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Empirical Study on Acoustic Feedback Path Across Hearing Aid Users},
year={2020},
volume={},
number={},
pages={441-445},
abstract={Acoustic feedback is one of the major problems in hearing aid applications. During a fitting session of a modern hearing aid, typically a feedback path prediction or an in situ measurement of feedback path is used as part of the gain and earpiece prescription to minimize the risk of feedback problems. It is well known that there are a lot of variations in feedback paths across users due to ear differences, however, there is limited knowledge from published studies to actually quantify these variations, especially for the earpiece types as domes and micro molds which became popular since the introduction of receiver-in-the-ear (RITE) style hearing aids. In this empirical study, we measured feedback paths on different users wearing a RITE style hearing aid fitted with different domes and micro molds. Our results confirmed that there are large variations across users/ears, and magnitude differences in measured feedback paths can be more than 60 dB for otherwise identical hearing aid and earpiece type.},
keywords={Hearing aids;earpiece;acoustic feedback;feedback path;measurements},
doi={10.1109/ICASSP40776.2020.9054366},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054110,
author={O. {Schwartz} and E. A. P. {Habets} and S. {Gannot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low Complexity NLMS for Multiple Loudspeaker Acoustic ECHO Canceller Using Relative Loudspeaker Transfer Functions},
year={2020},
volume={},
number={},
pages={446-450},
abstract={Speech signals captured by a microphone mounted to a smart soundbar or speaker are inherently contaminated by echos. Modern smart devices are usually characterized by low computational capabilities and low memory resources; in these cases, a low-complexity acoustic echo canceller (AEC) may be preferred even though a tolerable degradation in the cancellation occurs. In principle, devices with multiple loudspeakers need an individual AEC for each loudspeaker because the transfer function (TF) from each loudspeaker to the microphone must be estimated. In this paper, we present an normalized least mean square (NLMS) algorithm for a multi-loudspeaker case using relative loudspeaker transfer functions (RLTFs). In each iteration, the RLTFs between each loudspeaker and the reference loudspeaker are estimated first, and then the primary TF between the reference loudspeaker and the microphone. Assuming loudspeakers that are close to each other, the RLTFs can be estimated using fewer coefficients w.r.t. the primary TF, yielding a reduction of 3:4 in computational complexity and 1:2 in memory usage. The algorithm is evaluated using both simulated and real room impulse responses (RIRs) of two loudspeakers with a reverberation time set to 0.3 s and several distances between the loudspeakers.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054110},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054482,
author={P. {Meyer} and S. {Elshamy} and T. {Fingscheidt}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Multichannel Kalman-Based Wiener Filter Approach for Speaker Interference Reduction in Meetings},
year={2020},
volume={},
number={},
pages={451-455},
abstract={Recording a meeting and obtaining clean speech signals of each speaker is a challenging task. Even with a multichannel recording, in which all speakers are equipped with a close-talk microphone, speech of an active speaker still couples not only into his dedicated microphone, but also into all other microphone channels at a certain level. This is denoted as crosstalk and requires a multichannel speaker interference reduction to enhance the microphone channels for further processing. To solve this issue, we use a Wiener filter which is based on all individual microphone channels. We extend an existing approach by integrating methods from acoustic echo cancellation to improve the estimation of the interferer (noise) components of the filter, which leads to an improved signal-to-interferer ratio by up to 2.1 dB absolute at constant speech component quality.},
keywords={speaker interference reduction;Kalman filter;Wiener filter;meeting;crosstalk},
doi={10.1109/ICASSP40776.2020.9054482},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053601,
author={J. {Fabry} and P. {Jax}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Primary Path Estimator Based on Individual Secondary Path for ANC Headphones},
year={2020},
volume={},
number={},
pages={456-460},
abstract={Active noise cancellation (ANC) technology is a valuable asset for hearables. For a well performing and robust ANC system precise knowledge of the relevant acoustic paths is vital. It is feasible to individually measure the user’s secondary path by using the inner loudspeaker and microphone that are already built in into most ANC headphones. For measuring the primary path, however, an external loudspeaker as well as a suitable environment for acoustic measurements is required.We exploit that changes in the primary and secondary path appear jointly, based on the headphone fitting and the physiology of the user’s ear. In this contribution, we propose an estimator for the individual primary path based on a measurement of the individual secondary path. We evaluate the estimator by comparing the ANC performance to conventional fixed filter design methods for a large data set of real acoustic paths of an in-ear ANC headphone.},
keywords={Active noise cancellation;individualization;estimation},
doi={10.1109/ICASSP40776.2020.9053601},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054541,
author={M. M. {Halimeh} and W. {Kellermann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Efficient Multichannel Nonlinear Acoustic Echo Cancellation Based on a Cooperative Strategy},
year={2020},
volume={},
number={},
pages={461-465},
abstract={While a common approach to address nonlinear distortions, emitted by multiple loudspeakers and observed by multiple microphones, is to use post-filtering techniques, this paper proposes a cooperative strategy to rather model and then cancel such distortions. In this approach, the overall problem of modeling distortions emitted by a number of loudspeakers is divided into multiple simpler and easier tasks of estimating distortions emitted by subsets of loudspeakers. This approach allows also the exploitation of the physical configuration of the loudspeakers and microphones to select certain microphone signals for estimating the nonlinearity of loudspeakers that contribute the predominant part of the acoustic echo to this microphone signal. The proposed strategy is realized using the elitist resampling particle filter and the Gaussian particle filter. Both variants are evaluated and compared to a linear approach using synthesized and real recordings.},
keywords={Multichannel NAEC;Mutlichannel nonlinear system identification;Particle filtering},
doi={10.1109/ICASSP40776.2020.9054541},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054620,
author={M. {Hu} and J. {Lu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Active Control of Line Spectral Noise with Simultaneous Secondary Path Modeling Without Auxiliary Noise},
year={2020},
volume={},
number={},
pages={466-470},
abstract={Online secondary path modeling is appealing for most active noise control systems due to its benefit of effective tracking of the varying acoustic environment and possible variation of the control sources and sensors. However, the usually utilized additive noise method inevitably leads to the increase of residual noise. Recently we have found that it is possible to model the secondary path without any auxiliary noise as long as the noise to be controlled is not of line spectral property. In this paper, we further extend the analysis and theoretically prove that even for line spectral noise, it is feasible to realize online secondary path modeling using only the output of the control source while exerting active control simultaneously. Simulations using measured impulse responses validate the effectiveness of the proposed simultaneous control and modeling scheme.},
keywords={ANC;SAEC;line spectral process;online secondary path modeling},
doi={10.1109/ICASSP40776.2020.9054620},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054296,
author={H. {He} and J. {Chen} and J. {Benesty} and Y. {Yu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Frequency-Domain Recursive Least M-Estimate Adaptive Filter For Acoustic System Identification},
year={2020},
volume={},
number={},
pages={471-475},
abstract={To identify acoustic systems in non-Gaussian and Gaussian noises, a robust frequency-domain recursive least M-estimate (FRLM) adaptive filtering algorithm is proposed. The cost function of the adaptive filter is defined by using a robust time-domain M-estimator, while its update equation is derived from the normal equation in the frequency domain. As compared to the frequency-domain recursive least-squares adaptive filter, the FRLM algorithm obtains the robustness to non-Gaussian and Gaussian noises. The performance of the proposed algorithm is validated in simulated acoustic environments.},
keywords={Acoustic system identification;frequency-domain adaptive filter;recursive least M-estimate;robustness},
doi={10.1109/ICASSP40776.2020.9054296},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053421,
author={S. S. {Bhattacharjee} and N. V. {George}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Nearest Kronecker Product Decomposition Based Normalized Least Mean Square Algorithm},
year={2020},
volume={},
number={},
pages={476-480},
abstract={Recently, nearest Kronecker product (NKP) decomposition based Wiener filter and Recursive Least Squares (RLS) have been proposed and was found to be a good candidate for system identification and echo cancellation and was shown to offer better tracking performance along with lower computational complexity, especially for identification of low-rank systems. In this paper, we derive the Least Mean Square (LMS) versions of adaptive algorithms which take advantage of NKP decomposition, namely NKP-LMS and NKP Normalized LMS (NKP-NLMS) algorithms. We compare the convergence and tracking performance along with computational complexity between standard NLMS, standard RLS, NKP based RLS (RLS-NKP), the standard Affine Projection Algorithm (APA) and NKP-NLMS algorithm, to evaluate the efficacy of NKP-NLMS algorithm in the context of system identification. Simulation results show that NKP-NLMS can be a good candidate for system identification, especially for sparse/low rank systems.},
keywords={System identification;nearest Kronecker product;Adaptive filter;Least mean square;Low rank approximation},
doi={10.1109/ICASSP40776.2020.9053421},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053785,
author={S. {Hashemgeloogerdi} and S. {Braun}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Joint Beamforming and Reverberation Cancellation Using a Constrained Kalman Filter With Multichannel Linear Prediction},
year={2020},
volume={},
number={},
pages={481-485},
abstract={The performance of speech processing systems degrades significantly in far-field scenarios where the distance between the user and microphones increases, leading to low signal-to-noise and signal-to-reverberation ratios. To address this challenge, combining the denoising and dereverberation techniques in both parallel and cascade configurations has been widely studied. However, a parallel or cascade combination may not be efficient while imposing a large computational complexity. We propose a constrained Kalman filter based multichannel linear prediction method to jointly perform denoising and dereverberation efficiently using an online processing algorithm. In contrast to previously proposed methods which utilize steering vectors based on the relative early transfer function, our algorithm is implemented using a direct relative transfer function based steering vector, which aims at extracting the direct sound as opposed to preserving the early reflections. We show that the proposed algorithm outperforms existing online implementations of integrated beamformer and linear prediction methods on the REVERB challenge speech enhancement task while being computationally less complex.},
keywords={Dereverberation;denoising;constrained Kalman filter;multichannel linear prediction},
doi={10.1109/ICASSP40776.2020.9053785},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053610,
author={Z. {Wang} and D. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Microphone Complex Spectral Mapping for Speech Dereverberation},
year={2020},
volume={},
number={},
pages={486-490},
abstract={This study proposes a multi-microphone complex spectral mapping approach for speech dereverberation on a fixed array geometry. In the proposed approach, a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of direct sound from the stacked reverberant (and noisy) RI components of multiple microphones. We also investigate the integration of multi-microphone complex spectral mapping with beamforming and post-filtering. Experimental results on multi-channel speech dereverberation demonstrate the effectiveness of the proposed approach.},
keywords={Beamforming;complex spectral mapping;speech dereverberation;microphone array processing;deep learning},
doi={10.1109/ICASSP40776.2020.9053610},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053025,
author={H. {Gamper} and D. {Emmanouilidou} and S. {Braun} and I. J. {Tashev}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Predicting Word Error Rate for Reverberant Speech},
year={2020},
volume={},
number={},
pages={491-495},
abstract={Reverberation negatively impacts the performance of automatic speech recognition (ASR). Prior work on quantifying the effect of reverberation has shown that clarity (C50), a parameter that can be estimated from the acoustic impulse response, is correlated with ASR performance. In this paper we propose predicting ASR performance in terms of the word error rate (WER) directly from acoustic parameters via a polynomial, sigmoidal, or neural network fit, as well as blindly from reverberant speech samples using a convolutional neural network (CNN). We carry out experiments on two state-of-the-art ASR models and a large set of acoustic impulse responses (AIRs). The results confirm C50 and C80 to be highly correlated with WER, allowing WER to be predicted with the proposed fitting approaches. The proposed non-intrusive CNN model outperforms C50-based WER prediction, indicating that WER can be estimated blindly, i.e., directly from the reverberant speech samples without knowledge of the acoustic parameters.},
keywords={Distant speech recognition;ASR;reverberation;T60;C50},
doi={10.1109/ICASSP40776.2020.9053025},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054567,
author={C. {Gupta} and E. {Yılmaz} and H. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Automatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?},
year={2020},
volume={},
number={},
pages={496-500},
abstract={Automatic lyrics alignment and transcription in polyphonic music are challenging tasks because the singing vocals are corrupted by the background music. In this work, we propose to learn music genre-specific characteristics to train polyphonic acoustic models. We first compare several automatic speech recognition pipelines for the application of lyrics transcription. We then present the lyrics alignment and transcription performance of music-informed acoustic models for the best-performing pipeline, and systematically study the impact of music genre and language model on the performance. With such genre-based approach, we explicitly model the music without removing it during acoustic modeling. The proposed approach outperforms all competing systems in the lyrics alignment and transcription tasks on well-known polyphonic test datasets.},
keywords={Lyrics transcription;lyrics alignment;acoustic modeling;music genre;automatic speech recognition},
doi={10.1109/ICASSP40776.2020.9054567},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054642,
author={H. {Schreiber} and C. {Weiß} and M. {Müller}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Local Key Estimation In Classical Music Recordings: A Cross-Version Study on Schubert’s Winterreise},
year={2020},
volume={},
number={},
pages={501-505},
abstract={While global key and chord estimation for both popular and classical music recordings have received a lot of attention, little research has been devoted to estimating the local key for classical music. In this work, we approach local key estimation on a unique cross-version dataset comprising nine performances (versions) of Schubert’s song cycle Winterreise—a challenging scenario of high musical ambiguity and subjectivity. We compare an HMM-based system with a CNN-based approach. For both models, we employ a similar training procedure including the optimization of hyperparameters on a validation split. We systematically evaluate the model predictions and provide musical explanations for key confusions. As our main contribution, we explore how different training–test splits affect the models’ efficacy. Splitting along the song axis, we find that both methods perform similarly well. Splitting along the version axis leads to clearly higher results, especially for the CNN, which seems to effectively learn the harmonic progressions of the songs ("cover song effect") and successfully generalizes to unseen versions.},
keywords={music information retrieval;local key estimation;harmony analysis;evaluation;deep neural networks},
doi={10.1109/ICASSP40776.2020.9054642},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052987,
author={F. {Pedersoli} and G. {Tzanetakis} and K. M. {Yi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Music Transcription by Pre-Stacking A U-Net},
year={2020},
volume={},
number={},
pages={506-510},
abstract={We propose to pre-stack a U-Net as a way of improving the polyphonic music transcription performance of various baseline Convolutional Neural Networks (CNNS). The U-Net, a network architecture based on skip-connections between layers acts as a transformation network followed by a transcription network. Notably, we do not introduce any additional loss terms specific to the transformation network, but instead, jointly train the entire combined model with the original loss function that was designed for the back-end transcription network. We argue that this U-Net network transforms the input signal into a representation that is more effective for transcription, and thus enables the observed improvements in accuracy. We empirically confirm with several experiments using the MusicNet dataset, that the proposed configuration consistently improves the accuracy of transcription networks. This enhancement cannot be achieved by simply introducing more neurons or more layers to the baseline CNNs. Moreover, we show that using the proposed architecture we can go beyond general music transcription and perform transcription in an instrument-specific fashion. By doing so, the original general transcription performance is also increased.},
keywords={Automatic Music Transcription;Deep Learning;Deep Architecture;U-Net},
doi={10.1109/ICASSP40776.2020.9052987},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053135,
author={L. {Prétet} and G. {Richard} and G. {Peeters}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning to Rank Music Tracks Using Triplet Loss},
year={2020},
volume={},
number={},
pages={511-515},
abstract={Most music streaming services rely on automatic recommendation algorithms to exploit their large music catalogs. These algorithms aim at retrieving a ranked list of music tracks based on their similarity with a target music track. In this work, we propose a method for direct recommendation based on the audio content without explicitly tagging the music tracks. To that aim, we propose several strategies to perform triplet mining from ranked lists. We train a Convolutional Neural Network to learn the similarity via triplet loss. These different strategies are compared and validated on a large-scale experiment against an auto-tagging based approach. The results obtained highlight the efficiency of our system, especially when associated with an Auto-pooling layer.},
keywords={audio music similarity;deep learning;triplet loss;triplet mining},
doi={10.1109/ICASSP40776.2020.9053135},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054554,
author={J. {Jiang} and G. G. {Xia} and D. B. {Carlton} and C. N. {Anderson} and R. H. {Miyakawa}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Transformer VAE: A Hierarchical Model for Structure-Aware and Interpretable Music Representation Learning},
year={2020},
volume={},
number={},
pages={516-520},
abstract={Structure awareness and interpretability are two of the most desired properties of music generation algorithms. Structure-aware models generate more natural and coherent music with long-term dependencies, while interpretable models are more friendly for human-computer interaction and co-creation. To achieve these two goals simultaneously, we designed the Transformer Variational AutoEncoder, a hierarchical model that unifies the efforts of two recent breakthroughs in deep music generation: 1) the Music Transformer and 2) Deep Music Analogy. The former learns long-term dependencies using attention mechanism, and the latter learns interpretable latent representations using a disentangled conditional-VAE. We showed that Transformer VAE is essentially capable of learning a context-sensitive hierarchical representation, regarding local representations as the context and the dependencies among the local representations as the global structure. By interacting with the model, we can achieve context transfer, realizing the imaginary situation of "what if" a piece is developed following the music flow of another piece.},
keywords={Representation learning;VAE;Transformer;music structure},
doi={10.1109/ICASSP40776.2020.9054554},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052994,
author={J. {Fan} and Y. {Yang} and K. {Dong} and P. {Pasquier}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Comparative Study of Western and Chinese Classical Music Based on Soundscape Models},
year={2020},
volume={},
number={},
pages={521-525},
abstract={Whether literally or suggestively, the concept of soundscape is alluded in both modern and ancient music. In this study, we examine whether we can analyze and compare Western and Chinese classical music based on soundscape models. We addressed this question through a comparative study. Specifically, corpora of Western classical music excerpts (WCMED) and Chinese classical music excerpts (CCMED) were curated and annotated with emotional valence and arousal through a crowdsourcing experiment. We used a sound event detection (SED) and soundscape emotion recognition (SER) models with transfer learning to predict the perceived emotion of WCMED and CCMED. The results show that both SER and SED models could be used to analyze Chinese and Western classical music. The fact that SER and SED work better on Chinese classical music emotion recognition provides evidence that certain similarities exist between Chinese classical music and soundscape recordings, which permits transferability between machine learning models.},
keywords={Soundscape;Comparative Study;Crowdsourcing;Classical Music;Transfer Learning},
doi={10.1109/ICASSP40776.2020.9052994},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054278,
author={A. {Vaglio} and R. {Hennequin} and M. {Moussallam} and G. {Richard} and F. {d’Alché-Buc}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Audio-Based Detection of Explicit Content in Music},
year={2020},
volume={},
number={},
pages={526-530},
abstract={We present a novel automatic system for performing explicit content detection directly on the audio signal. Our modular approach uses an audio-to-character recognition model, a keyword spotting model associated with a dictionary of carefully chosen keywords, and a Random Forest classification model for the final decision. To the best of our knowledge, this is the first explicit content detection system based on audio only. We demonstrate the individual relevance of our modules on a set of sub-tasks and compare our approach to a lyrics-informed oracle and an end-to-end naive architecture. The results obtained are encouraging with a F1-score of 67% on a industrial scale explicit content dataset.},
keywords={Explicit content detection;keyword spotting;lyrics transcription;CTC training;music information retrieval},
doi={10.1109/ICASSP40776.2020.9054278},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052951,
author={J. {Devaney}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={New Metrics for Evaluating the Accuracy of Fundamental Frequency Estimation Approaches in Musical Signals},
year={2020},
volume={},
number={},
pages={531-535},
abstract={This paper demonstrates the importance of assessing the performance of fundamental frequency estimation algorithms with metrics that capture the temporal characteristics of f0 traces, particularly those that are calculated at the note level. Capturing temporal characteristics is particularly important for tasks that model human engagement with music, such as the study of expressive music performance. Note-level descriptors provide a better description of the human experience of listening to music and thus a more perceptually-relevant evaluation of algorithms than frame-level metrics. This paper evaluates the magnitude of accuracy differences between a simple mean-based frame-level accuracy measurement and four metrics that capture more perceptually-relevant aspects of the evolution of f0 traces over time (perceived pitch, vibrato rate, vibrato depth, and jitter) for two score-informed f0 estimation algorithms. The algorithms' accuracies are compared on multi-track recordings of either four vocalists or four instrumentalists (violin, saxophone, clarinet, and bassoon) both on the original anechoic recordings and mixes with artificial reverberation added. The algorithms performed within the margin of error of each other for frame-level accuracy but had significantly different accuracies on all of perceptually-relevant metrics. This paper concludes by proposing new evaluation metrics that capture temporal characteristics of fundamental frequency traces.},
keywords={evaluation;metrics;fundamental frequency;pitch;expressive performance},
doi={10.1109/ICASSP40776.2020.9052951},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053669,
author={M. {Won} and S. {Chun} and O. {Nieto} and X. {Serrc}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Data-Driven Harmonic Filters for Audio Representation Learning},
year={2020},
volume={},
number={},
pages={536-540},
abstract={We introduce a trainable front-end module for audio representation learning that exploits the inherent harmonic structure of audio signals. The proposed architecture, composed of a set of filters, compels the subsequent network to capture harmonic relations while preserving spectro-temporal locality. Since the harmonic structure is known to have a key role in human auditory perception, one can expect these harmonic filters to yield more efficient audio representations. Experimental results show that a simple convolutional neural network back-end with the proposed front-end outperforms state-of-the-art baseline methods in automatic music tagging, keyword spotting, and sound event tagging tasks.},
keywords={Harmonic filters;audio representation learning;deep learning},
doi={10.1109/ICASSP40776.2020.9053669},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053839,
author={Z. {Yu} and X. {Xu} and X. {Chen} and D. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning a Representation for Cover Song Identification Using Convolutional Neural Network},
year={2020},
volume={},
number={},
pages={541-545},
abstract={Cover song identification is a challenging task in the field of Music Information Retrieval (MIR) due to complex musical variations between query tracks and cover versions. Previous works typically utilize hand-crafted features and alignment algorithms for the task. More recently, further breakthroughs are achieved by employing neural network approaches. In this paper, we propose a novel Convolutional Neural Network (CNN) towards cover song identification. We train the network through classification criteria. Having been trained, the network is used to extract music representation for cover song identification. A training scheme is designed to train robust models against tempo changes. Experimental results show that our approach outperforms state-of-the-art methods on several public datasets with low time complexity.},
keywords={Music Information Retrieval;Cover Song Identification},
doi={10.1109/ICASSP40776.2020.9053839},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053815,
author={T. {Tsai}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Towards Linking the Lakh and IMSLP Datasets},
year={2020},
volume={},
number={},
pages={546-550},
abstract={This paper investigates the problem of matching a MIDI file against a large database of piano sheet music images. Previous sheet–audio and sheet–MIDI alignment approaches have primarily focused on a 1-to-1 alignment task, which is not a scalable solution for retrieval from large databases. We propose a method for scalable cross-modal retrieval that might be used to link the Lakh MIDI dataset with IM-SLP sheet music data. Our approach is to modify a previously proposed feature representation called a symbolic bootleg score to be suitable for hashing. On a database of 5,000 piano scores containing 55,000 individual sheet music images, our system achieves a mean reciprocal rank of 0.84 and an average retrieval time of 25.4 seconds.},
keywords={sheet music;MIDI;retrieval;cross-modal;search},
doi={10.1109/ICASSP40776.2020.9053815},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053059,
author={P. {Gao} and C. {You} and T. {Chi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Multi-Dilation and Multi-Resolution Fully Convolutional Network for Singing Melody Extraction},
year={2020},
volume={},
number={},
pages={551-555},
abstract={Each human cognitive function involves bottom-up and top-down processes. Several methods have been proposed for singing melody extraction by emphasizing either the bottom-up or top-down processes. For hearing, the bottom-up processes include spectral and spectro-temporal decomposition of the sound by the cochlea and the auditory cortex. In this paper, we propose a neural network, which includes spectro-temporal multi-resolution decomposition of the log-spectrogram of the sound and a semantic segmentation model to respectively address the bottom-up and top-down processing of hearing, for singing melody extraction. Simulation results show the proposed model outperforms all previously proposed methods, emphasizing either bottom-up or top-down processing, in almost all objective evaluation metrics.},
keywords={Melody extraction;multi-resolution;fully convolutional network},
doi={10.1109/ICASSP40776.2020.9053059},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054141,
author={X. {Zhao} and G. {Huang} and J. {Chen} and J. {Benesty}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Improved Solution to the Frequency-Invariant Beamforming with Concentric Circular Microphone Arrays},
year={2020},
volume={},
number={},
pages={556-560},
abstract={Frequency-invariant beamforming with circular microphone arrays (CMAs) has drawn a significant amount of attention for its steering flexibility and high directivity. However, frequency-invariant beam-forming with CMAs often suffers from the so-called null problem, which is caused by the zeros of the Bessel functions; then, concentric CMAs (CCMAs) are used to deal with this problem. While frequency-invariant beamforming with CCMAs can mitigate the null problem, the beampattern is still suffering from distortion due to s-patial aliasing at high frequencies. In this paper, we find that the spatial aliasing problem is caused by higher-order circular harmonics. To deal with this problem, we take the aliasing harmonics into account and approximate the beampattern with a higher truncation order of the Jacobi-Anger expansion than required. Then, the beam-forming filter is determined by minimizing the errors between the desired directivity pattern and the approximated one. Simulation results show that the developed method can mitigate the distortion of the beampattern caused by spatial aliasing.},
keywords={Microphone arrays;concentric circular arrays;frequency-invariant beamforming;spatial aliasing},
doi={10.1109/ICASSP40776.2020.9054141},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053225,
author={R. M. {Corey} and A. C. {Singer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Binauralaudio Source Remixing with Microphone Array Listening Devices},
year={2020},
volume={},
number={},
pages={561-565},
abstract={Augmented listening devices, such as hearing aids and augmented reality headsets, enhance human perception by changing the sounds that we hear. Microphone arrays can improve the performance of listening systems in noisy environments, but most array-based listening systems are designed to isolate a single sound source from a mixture. This work considers a source-remixing filter that alters the relative level of each source independently. Remixing rather than separating sounds can help to improve perceptual transparency: it causes less distortion to the signal spectrum and especially to the interaural cues that humans use to localize sounds in space.},
keywords={Microphone array processing;hearing aids;augmented reality;beamforming;audio source separation},
doi={10.1109/ICASSP40776.2020.9053225},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054754,
author={R. {Varzandeh} and K. {Adiloğlu} and S. {Doclo} and V. {Hohmann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Exploiting Periodicity Features for Joint Detection and DOA Estimation of Speech Sources Using Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={566-570},
abstract={While many algorithms deal with direction of arrival (DOA) estimation and voice activity detection (VAD) as two separate tasks, only a small number of data-driven methods have addressed these two tasks jointly. In this paper, a multi-input single-output convolutional neural network (CNN) is proposed which exploits a novel feature combination for joint DOA estimation and VAD in the context of binaural hearing aids. In addition to the well-known generalized cross correlation with phase transform (GCC-PHAT) feature, the network uses an auditory-inspired feature called periodicity degree (PD), which provides a broadband representation of the periodic structure of the signal. The proposed CNN has been trained in a multi-conditional training scheme across different signal-to-noise ratios. Experimental results for a single-talker scenario in reverberant environments show that by exploiting the PD feature, the proposed CNN is able to distinguish speech from non-speech signal blocks, thereby outperforming the baseline CNN in terms of DOA estimation accuracy. In addition, the results show that the proposed method is able to adapt to different unseen acoustic conditions and background noises.},
keywords={convolutional neural networks;binaural DOA estimation;voice activity detection;periodicity},
doi={10.1109/ICASSP40776.2020.9054754},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053656,
author={Y. {Hu} and P. N. {Samarasinghe} and T. D. {Abhayapala} and S. {Gannot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unsupervised Multiple Source Localization Using Relative Harmonic Coefficients},
year={2020},
volume={},
number={},
pages={571-575},
abstract={This paper presents an unsupervised multi-source localization algorithm using a recently introduced feature called the relative harmonic coefficients. We derive a closed-form expression of the feature and briefly summarize its unique properties. We then exploit this feature to develop a single-source frame/bin detector which simplifies the challenging problem of multiple source localization into a single source localization problem. We show that the underlying method is suitable for localization using overlapped, disjoint as well as simultaneous multi-source recordings. Experimental results in both simulated and real-life reverberant environments confirm improved localization accuracy of the proposed method in comparison with the existing state-of-art approach.},
keywords={Unsupervised multiple source localization;relative harmonic coefficients;single-source frame/bin detector.},
doi={10.1109/ICASSP40776.2020.9053656},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054381,
author={D. {Mirabilii} and K. K. {Lakshminarayana} and W. {Mack} and E. A. P. {Habets}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Data-Driven Wind Speed Estimation Using Multiple Microphones},
year={2020},
volume={},
number={},
pages={576-580},
abstract={A deep neural network (DNN) based approach for estimating the speed of airflows using closely-spaced microphones is proposed. The spatial characteristics of wind noise measured with a smallaperture array are exploited, i.e., the low-frequency spatial coherence of wind noise signals is used as an input feature. The output is an estimate of the wind speed averaged over a specific time interval. The DNN is trained using synthetic wind noise, which overcomes the time-consuming data collection and allows to isolate wind noise from different acoustic sources. The dataset used for testing comprises wind noise measured outdoors with a circular linear array and a ground truth obtained using an ultrasonic anemometer. The obtained model is applied to generated and measured wind noise. The performance of the proposed method is assessed across a wide range of wind speeds and directions, using different time resolutions.},
keywords={Wind noise;wind speed;multi-channel;Corcos model},
doi={10.1109/ICASSP40776.2020.9054381},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054005,
author={C. {Schymura} and T. {Ochiai} and M. {Delcroix} and K. {Kinoshita} and T. {Nakatani} and S. {Araki} and D. {Kolossa}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Dynamic Stream Weight Backprop Kalman Filter for Audiovisual Speaker Tracking},
year={2020},
volume={},
number={},
pages={581-585},
abstract={Audiovisual speaker tracking is an application that has been tackled by a wide range of classical approaches based on Gaussian filters, most notably the well-known Kalman filter. Recently, a specific Kalman filter implementation was proposed for this task, which incorporated dynamic stream weights to explicitly control the influence of acoustic and visual observations during estimation. Inspired by recent progress in the context of integrating uncertainty estimates into modern deep learning frameworks, this paper proposes a deep neural-network-based implementation of the Kalman filter with dynamic stream weights, whose parameters can be learned via standard backpropagation. This allows for jointly optimizing the parameters of the model and the dynamic stream weight estimator in a unified framework. An experimental study on audiovisual speaker tracking shows that the proposed model shows comparable performance to state-of-the-art recurrent neural networks with the additional advantage of requiring a smaller number of parameters and providing explicit uncertainty information.},
keywords={audiovisual speaker tracking;backprop Kalman filter;dynamic stream weights},
doi={10.1109/ICASSP40776.2020.9054005},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053614,
author={E. {Hadad} and S. {Gannot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Maximum Likelihood Multi-Speaker Direction of Arrival Estimation Utilizing a Weighted Histogram},
year={2020},
volume={},
number={},
pages={586-590},
abstract={In this contribution, a novel maximum likelihood (ML) based direction of arrival (DOA) estimator for concurrent speakers in a noisy reverberant environment is presented. The DOA estimation task is formulated in the short-time Fourier transform (STFT) in two stages. In the first stage, a single local DOA per time-frequency (TF) bin is selected, using the W-disjoint orthogonality property of the speech signal in the STFT domain. The local DOA is obtained as the maximum of the narrow-band likelihood localization spectrum at each TF bin. In addition, for each local DOA, a confidence measure is calculated, determining the confidence in the local estimate. In the second stage, the wide-band localization spectrum is calculated using a weighted histogram of the local DOA estimates with the confidence measures as weights. Finally, the wide-band DOA estimation is obtained by selecting the peaks in the wide-band localization spectrum. The results of our experimental study demonstrate the benefit of the proposed algorithm in a reverberant environment as compared with the classical steered response power phase transform (SRP-PHAT) algorithm.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053614},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053790,
author={R. {Ikeshita} and T. {Nakatani} and S. {Araki}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Overdetermined Independent Vector Analysis},
year={2020},
volume={},
number={},
pages={591-595},
abstract={We address the convolutive blind source separation problem for the (over-)determined case where (i) the number of nonstationary target-sources K is less than that of microphones M, and (ii) there are up to M −K stationary Gaussian noises that need not to be extracted. Independent vector analysis (IVA) can solve the problem by separating into M sources and selecting the top K highly nonstationary signals among them, but this approach suffers from a waste of computation especially when $K \ll M$. Channel reductions in preprocessing of IVA by, e.g., principle component analysis have the risk of removing the target signals. We here extend IVA to resolve these issues. One such extension has been attained by assuming the orthogonality constraint (OC) that the sample correlation between the target and noise signals is to be zero. The proposed IVA, on the other hand, does not rely on OC and exploits only the independence between sources and the stationarity of the noises. This enables us to develop several efficient algorithms based on block coordinate descent methods with a problem specific acceleration. We clarify that one such algorithm exactly coincides with the conventional IVA with OC, and also explain that the other newly developed algorithms are faster than it. Experimental results show the improved computational load of the new algorithms compared to the conventional methods. In particular, a new algorithm specialized for K = 1 outperforms the others.},
keywords={Blind source separation;overdetermined;independent vector analysis;block coordinate descent method;generalized eigenvalue problem},
doi={10.1109/ICASSP40776.2020.9053790},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052905,
author={A. {Brendel} and T. {Haubner} and W. {Kellermann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Spatially Guided Independent Vector Analysis},
year={2020},
volume={},
number={},
pages={596-600},
abstract={We present a Maximum A Posteriori (MAP) derivation of the Independent Vector Analysis (IVA) algorithm for blind source separation incorporating an additional spatial prior over the demixing matrices. In this way, the outer permutation ambiguity of IVA is avoided and the algorithm can be guided towards a desired solution in adverse acoustic conditions. The resulting MAP optimization problem is solved by deriving majorize-minimize update rules to achieve convergence speed comparable to the well-known auxiliary function IVA algorithm, i.e., the convergence is not impaired by the additional constraint. The proposed algorithm exhibits superior performance at lower computational cost than a state-of-the-art spatially constrained IVA algorithm in a setup defined by real-world Room Impulse Responses (RIRs).},
keywords={Independent Vector Analysis;MM Algorithm;Directional Constraint},
doi={10.1109/ICASSP40776.2020.9052905},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053066,
author={R. {Scheibler} and N. {Ono}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast Independent Vector Extraction by Iterative SINR Maximization},
year={2020},
volume={},
number={},
pages={601-605},
abstract={We propose fast independent vector extraction (FIVE), a new algorithm that blindly extracts a single non-Gaussian source from a Gaussian background. The algorithm iteratively computes beam-forming weights maximizing the signal-to-interference-and-noise ratio for an approximate noise covariance matrix. We demonstrate that this procedure minimizes the negative log-likelihood of the input data according to a well-defined probabilistic model. The minimization is carried out via the auxiliary function technique whereas, unlike related methods, the auxiliary function is globally minimized at every iteration. Numerical experiments are carried out to assess the performance of FIVE. We find that it is vastly superior to competing methods in terms of convergence speed, and has high potential for real-time applications.},
keywords={Independent Vector Extraction;Auxiliary Function;Blind Source Separation;Maximum SINR Beamforming;Gaussian Background},
doi={10.1109/ICASSP40776.2020.9053066},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054603,
author={K. {Kamo} and Y. {Kubo} and N. {Takamune} and D. {Kitamura} and H. {Saruwatari} and Y. {Takahashi} and K. {Kondo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Regularized Fast Multichannel Nonnegative Matrix Factorization with ILRMA-Based Prior Distribution of Joint-Diagonalization Process},
year={2020},
volume={},
number={},
pages={606-610},
abstract={In this paper, we address a convolutive blind source separation (BSS) problem and propose a new extended framework of FastMNMF by introducing prior information for joint diagonalization of the spatial covariance matrix model. Recently, FastMNMF has been proposed as a fast version of multichannel nonnegative matrix factorization under the assumption that the spatial covariance matrices of multiple sources can be jointly diagonalized. However, its source-separation performance was not improved and the physical meaning of the joint-diagonalization process was unclear. To resolve these problems, we first reveal a close relationship between the joint-diagonalization process and the demixing system used in independent low-rank matrix analysis (ILRMA). Next, motivated by this fact, we propose a new regularized FastMNMF supported by ILRMA and derive convergence-guaranteed parameter update rules. From BSS experiments, we show that the proposed method outperforms the conventional FastMNMF in source-separation accuracy with almost the same computation time.},
keywords={blind source separation;spatial covariance model;joint diagonalization},
doi={10.1109/ICASSP40776.2020.9054603},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052950,
author={J. {Baumann} and T. {Lohrenz} and A. {Roy} and T. {Fingscheidt}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Beyond the Dcase 2017 Challenge on Rare Sound Event Detection: A Proposal for a More Realistic Training and Test Framework},
year={2020},
volume={},
number={},
pages={611-615},
abstract={There are many ways to evaluate rare sound event detection (SED) approaches, e.g., the DCASE 2017 challenge provides a widely employed framework. This paper proposes a rare SED training and test framework, which is reflecting an SED application in a more realistic way. Our setup gets rid of too much prior knowledge on the test data, and assumes additional unknown acoustic events both in training and test data, which in practice have to be identified as background. Taking this into account during training, the robustness in real-world scenarios can be significantly increased, with an average event-based error rate reduction of an absolute 34 percentage points. Further we show and compare the performance of multi-event (polyphonic) classifiers vs. single-event classifiers while outlining the benefits of multi-event training.},
keywords={Acoustic event detection;AED;sound event detection;SED;rare event detection;CRNN;DCASE 2017 challenge},
doi={10.1109/ICASSP40776.2020.9052950},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054712,
author={K. {Shimada} and Y. {Koyama} and A. {Inoue}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Metric Learning with Background Noise Class for Few-Shot Detection of Rare Sound Events},
year={2020},
volume={},
number={},
pages={616-620},
abstract={Few-shot learning systems for sound event recognition have gained interests since they require only a few examples to adapt to new target classes without fine-tuning. However, such systems have only been applied to chunks of sounds for classification or verification. In this paper, we aim to achieve few-shot detection of rare sound events, from query sequence that contain not only the target events but also the other events and background noise. Therefore, it is required to prevent false positive reactions to both the other events and background noise. We propose metric learning with background noise class for the few-shot detection. The contribution is to present the explicit inclusion of background noise as an independent class, a suitable loss function that emphasizes this additional class, and a corresponding sampling strategy that assists training. It provides a feature space where the event classes and the background noise class are sufficiently separated. Evaluations on few-shot detection tasks, using DCASE 2017 task2 and ESC-50, show that our proposed method outperforms metric learning without considering the background noise class. The few-shot detection performance is also comparable to that of the DCASE 2017 task2 baseline system, which requires huge amount of annotated audio data.},
keywords={Sound event detection;rare sound event;few-shot learning;metric learning;background noise class},
doi={10.1109/ICASSP40776.2020.9054712},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053912,
author={K. {Imoto} and N. {Tonami} and Y. {Koizumi} and M. {Yasuda} and R. {Yamanishi} and Y. {Yamashita}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sound Event Detection by Multitask Learning of Sound Events and Scenes with Soft Scene Labels},
year={2020},
volume={},
number={},
pages={621-625},
abstract={Sound event detection (SED) and acoustic scene classification (ASC) are major tasks in environmental sound analysis. Considering that sound events and scenes are closely related to each other, some works have addressed joint analyses of sound events and acoustic scenes based on multitask learning (MTL), in which the knowledge of sound events and scenes can help in estimating them mutually. The conventional MTL-based methods utilize one-hot scene labels to train the relationship between sound events and scenes; thus, the conventional methods cannot model the extent to which sound events and scenes are related. However, in the real environment, common sound events may occur in some acoustic scenes; on the other hand, some sound events occur only in a limited acoustic scene. In this paper, we thus propose a new method for SED based on MTL of SED and ASC using the soft labels of acoustic scenes, which enable us to model the extent to which sound events and scenes are related. Experiments conducted using TUT Sound Events 2016/2017 and TUT Acoustic Scenes 2016 datasets show that the proposed method improves the SED performance by 3.80% in F-score compared with conventional MTL-based SED.},
keywords={Sound event detection;multitask learning;teacher-student learning;acoustic scene classification},
doi={10.1109/ICASSP40776.2020.9053912},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053584,
author={L. {Lin} and X. {Wang} and H. {Liu} and Y. {Qian}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Guided Learning for Weakly-Labeled Semi-Supervised Sound Event Detection},
year={2020},
volume={},
number={},
pages={626-630},
abstract={We propose a simple but efficient method termed Guided Learning for weakly-labeled semi-supervised sound event detection (SED). There are two sub-targets implied in weakly-labeled SED: audio tagging and boundary detection. Instead of designing a single model by considering a trade-off between the two sub-targets, we design a teacher model aiming at audio tagging to guide a student model aiming at boundary detection to learn using the unlabeled data. The guidance is guaranteed by the audio tagging performance gap of the two models. In the meantime, the student model liberated from the trade-off is able to provide more excellent boundary detection results. We propose a principle to design such two models based on the relation between the temporal compression scale and the two sub-targets. We also propose an end-to-end semi-supervised learning process for these two models to enable their abilities to rise alternately. Experiments on the DCASE2018 Task4 dataset show that our approach achieves competitive performance.},
keywords={Sound event detection;weakly-labeled;semi-supervised learning;neural networks},
doi={10.1109/ICASSP40776.2020.9053584},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053776,
author={K. {He} and Y. {Shen} and W. {Zhang} and J. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Staged Training Strategy and Multi-Activation for Audio Tagging with Noisy and Sparse Multi-Label Data},
year={2020},
volume={},
number={},
pages={631-635},
abstract={Audio tagging aims to predict whether certain acoustic events occur in the audio clips. Due to the difficulty and huge cost of obtaining manually labeled data with high confidence, researchers begin to focus on audio tagging using a small set of manually-labeled data, and a larger set of noisy-labeled data. In addition, audio tagging is a sparse multi-label classification task, where only a small number of acoustic events may occur in an audio clip. In this paper, we propose a staged training strategy to deal with the noisy label, and adopt a sigmoid-sparsemax multi-activation structure to deal with the sparse multi-label classification. This paper is an improvement and extension of our previous work for participation in Task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge. We evaluate our methods on the identical task, and achieve state-of-the-art performance, with an lwlrap score of 0.7591 on official evaluation dataset.},
keywords={Audio tagging;noisy label;staged training strategy;multi-activation structure;DCASE2019 Challenge},
doi={10.1109/ICASSP40776.2020.9053776},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054444,
author={T. {Iqbal} and Y. {Cao} and Q. {Kong} and M. D. {Plumbley} and W. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning With Out-of-Distribution Data for Audio Classification},
year={2020},
volume={},
number={},
pages={636-640},
abstract={In supervised machine learning, the assumption that training data is labelled correctly is not always satisfied. In this paper, we investigate an instance of labelling error for classification tasks in which the dataset is corrupted with out-of-distribution (OOD) instances: data that does not belong to any of the target classes, but is labelled as such. We show that detecting and relabelling certain OOD instances, rather than discarding them, can have a positive effect on learning. The proposed method uses an auxiliary classifier, trained on data that is known to be in-distribution, for detection and relabelling. The amount of data required for this is shown to be small. Experiments are carried out on the FSDnoisy18k audio dataset, where OOD instances are very prevalent. The proposed method is shown to improve the performance of convolutional neural networks by a significant margin. Comparisons with other noise-robust techniques are similarly encouraging.},
keywords={Audio classification;out-of-distribution;convolutional neural network;pseudo-labelling},
doi={10.1109/ICASSP40776.2020.9054444},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053023,
author={Y. {Huang} and X. {Wang} and L. {Lin} and H. {Liu} and Y. {Qian}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Branch Learning for Weakly-Labeled Sound Event Detection},
year={2020},
volume={},
number={},
pages={641-645},
abstract={There are two sub-tasks implied in the weakly-supervised SED: audio tagging and event boundary detection. Current methods which combine multi-task learning with SED requires annotations both for these two sub-tasks. Since there are only annotations for audio tagging available in weakly-supervised SED, we design multiple branches with different learning purposes instead of pursuing multiple tasks. Similar to multiple tasks, multiple different learning purposes can also prevent the common feature which the multiple branches share from overfitting to any one of the learning purposes. We design these multiple different learning purposes based on combinations of different MIL strategies and different pooling methods. Experiments on the DCASE 2018 Task 4 dataset and the URBAN-SED dataset both show that our method achieves competitive performance.},
keywords={Sound event detection;weakly-labeled;multi-branch learning;multi-task learning},
doi={10.1109/ICASSP40776.2020.9053023},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053702,
author={T. {Komatsu} and K. {Imoto} and M. {Togami}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Scene-Dependent Acoustic Event Detection with Scene Conditioning and Fake-Scene-Conditioned Loss},
year={2020},
volume={},
number={},
pages={646-650},
abstract={In this paper, we propose scene-dependent acoustic event detection (AED) with scene conditioning and fake-scene-conditioned loss. The proposed method employs a multitask network, that has not only AED part but also acoustic scene classification (ASC). The scenes predicted by ASC are employed as an additional feature for scene conditioning of AED to learn the relationship between scenes and events. For efficient training, the proposed method incorporates a new AED loss function, which is the fake-scene-conditioned loss, in addition to the conventional AED loss. Upon training, the AED part is conditioned with fake scenes as well as predicted and true scenes. The fake-scene-conditioned loss is calculated between the fake-scene-conditioned AED results and labels of events that do not exist in the fake scenes are removed. Whereas training with combinations of true scenes/events, i.e., the conventional AED loss, only reveals that an event is present in a scene, with fake-scene-conditioned loss, the proposed method can learn that an event is absent in a scene. Experimental results show that the proposed method improves the AED performance compared with the baseline; an increase in the f1 score of 23% and a decrease in the false alarm rate of 56% for scenes where no event exists.},
keywords={Acoustic Event Detection;Acoustic Scene Classification;Joint Analysis;Scene-dependent Acoustic Event Detection},
doi={10.1109/ICASSP40776.2020.9053702},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054462,
author={M. {Yasuda} and Y. {Koizumi} and S. {Saito} and H. {Uematsu} and K. {Imoto}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sound Event Localization Based on Sound Intensity Vector Refined by Dnn-Based Denoising and Source Separation},
year={2020},
volume={},
number={},
pages={651-655},
abstract={We propose a direction-of-arrival (DOA) estimation method for Sound Event Localization and Detection (SELD). Direct estimation of DOA using a deep neural network (DNN), i.e. completely-datadriven approach, achieves high accuracy. However, there is a gap in the accuracy between DOA estimation for single and overlapping sources because they cannot incorporate physical knowledge. Meanwhile, although the accuracy of physics-based approaches is inferior to DNN-based approaches, it is robust for overlapping-source. In this study, we consider a combination of physics-based and DNN-based approaches; the sound intensity vectors (IVs) for physics-based DOA estimation is refined based on DNN-based denoising and source separation. This method enables the accurate DOA estimation for both single and overlapping sources using a spherical microphone array. Experimental results show that the proposed method achieves state-of-the-art DOA estimation accuracy on an open dataset of the SELD.},
keywords={sound event localization and detection;direction of arrival;deep neural network;and time-frequency mask},
doi={10.1109/ICASSP40776.2020.9054462},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053519,
author={X. {Bai} and J. {Du} and J. {Pan} and H. {Zhou} and Y. {Tu} and C. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={High-Resolution Attention Network with Acoustic Segment Model for Acoustic Scene Classification},
year={2020},
volume={},
number={},
pages={656-660},
abstract={The spectral information of acoustic scenes is diverse and complex, which poses challenges for acoustic scene tasks. To improve the classification performance, a variety of convolutional neural networks (CNNs) are proposed to extract richer semantic information of scene utterances. However, the different regions of the features extracted from CNN-based encoder have different importance. In this paper, we propose a novel strategy for acoustic scene classification, namely high-resolution attention network with acoustic segment model (HRAN-ASM). In this approach, we utilize fully CNN to obtain high-level semantic information and then adopt two-stage attention strategy to select the relevant acoustic scene segments. Besides, the acoustic segment model (ASM) proposed in our recent work provides embedding vectors for this attention mechanism. The performance is evaluated on DCASE 2018 Task 1a, showing 70.5% good classification accuracy under single system and no data expansion, which is superior to CNN-based self-attention mechanism and highly competitive.},
keywords={Acoustic scene classification;attention mechanism;acoustic segment model;fully convolutional neural network},
doi={10.1109/ICASSP40776.2020.9053519},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054628,
author={C. C. {Chatterjee} and M. {Mulimani} and S. G. {Koolagudi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Polyphonic Sound Event Detection Using Transposed Convolutional Recurrent Neural Network},
year={2020},
volume={},
number={},
pages={661-665},
abstract={In this paper we propose a Transposed Convolutional Recurrent Neural Network (TCRNN) architecture for polyphonic sound event recognition. Transposed convolution layer, which caries out a regular convolution operation but reverts the spatial transformation and it is combined with a bidirectional Recurrent Neural Network (RNN) to get TCRNN. Instead of the traditional mel spectrogram features, the proposed methodology incorporates mel-IFgram (Instantaneous Frequency spectrogram) features. The performance of the proposed approach is evaluated on sound events of publicly available TUT-SED 2016 and Joint sound scene and polyphonic sound event recognition datasets. Results show that the proposed approach outperforms state-of-the-art methods.},
keywords={Sound Event Detection (SED);Deep Neural Networks (DNN);Convolution Neural Networks (CNN);Recurrent Neural Networks (RNN);Transposed CNN (TCNN);Instantaneous Frequency spectrogram (IFgram)},
doi={10.1109/ICASSP40776.2020.9054628},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053613,
author={A. {Kumar} and V. K. {Ithapu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={SeCoST:: Sequential Co-Supervision for Large Scale Weakly Labeled Audio Event Detection},
year={2020},
volume={},
number={},
pages={666-670},
abstract={Weakly supervised learning algorithms are critical for scaling audio event detection to several hundreds of sound categories. Such learning models should not only disambiguate sound events efficiently with minimal class-specific annotation but also be robust to label noise, which is more apparent with weak labels instead of strong annotations. In this work, we propose a new framework for designing learning models with weak supervision by bridging ideas from sequential learning and knowledge distillation. We refer to the proposed methodology as SeCoST (pronounced Sequest) — Sequential Co-supervision for training generations of Students. SeCoST incrementally builds a cascade of student-teacher pairs via a novel knowledge transfer method. Our evaluations on Audioset (the largest weakly labeled dataset available) show that SeCoST achieves a mean average precision of 0.383 while outperforming prior state of the art by a considerable margin.},
keywords={Audio Event Detection;Teacher Student Models;Weakly Labeled;Sequential Learning},
doi={10.1109/ICASSP40776.2020.9053613},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053629,
author={Y. {Nakagome} and M. {Togami} and T. {Ogawa} and T. {Kobayashi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Speech Extraction with Time-Varying Spatial Filtering Guided By Desired Direction Attractor},
year={2020},
volume={},
number={},
pages={671-675},
abstract={In this investigation, a deep neural network (DNN) based speech extraction method is proposed to enhance a speech signal propagating from the desired direction. The proposed method integrates knowledge based on a sound propagation model and the time-varying characteristics of a speech source, into a DNN-based separation framework. This approach outputs a separated speech source using time-varying spatial filtering, which achieves superior speech extraction performance compared with time-invariant spatial filtering. Given that the gradient of all modules can be calculated, back-propagation can be performed to maximize the speech quality of the output signal in an end-to-end manner. Guided information is also modeled based on the sound propagation model, which facilitates disentangled representations of the target speech source and noise signals. The experimental results demonstrate that the proposed method can extract the target speech source more accurately than conventional DNN-based speech source separation and conventional speech extraction using time-invariant spatial filtering.},
keywords={direction-of-arrival information;attractor;time-varying spatial filtering;end-to-end speech source separation},
doi={10.1109/ICASSP40776.2020.9053629},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054693,
author={J. {Janský} and J. {Málek} and J. {Čmejla} and T. {Kounovský} and Z. {Koldovský} and J. {Žd’ánský}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adaptive Blind Audio Source Extraction Supervised By Dominant Speaker Identification Using X-Vectors},
year={2020},
volume={},
number={},
pages={676-680},
abstract={We propose a novel algorithm for adaptive blind audio source extraction. The proposed method is based on independent vector analysis and utilizes the auxiliary function optimization to achieve high convergence speed. The algorithm is partially supervised by a pilot signal related to the source of interest (SOI), which ensures that the method correctly extracts the utterance of the desired speaker. The pilot is based on the identification of a dominant speaker in the mixture using x-vectors. The properties of the x-vectors computed in the presence of cross-talk are experimentally analyzed. The proposed approach is verified in a scenario with a moving SOI, static interfering speaker and environmental noise.},
keywords={Independent vector extraction;adaptive processing;auxiliary function;x-vector;speaker identification},
doi={10.1109/ICASSP40776.2020.9054693},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054150,
author={T. {Kondo} and K. {Fukushige} and N. {Takamune} and D. {Kitamura} and H. {Saruwatari} and R. {Ikeshita} and T. {Nakatani}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Convergence-Guaranteed Independent Positive Semidefinite Tensor Analysis Based on Student’s T Distribution},
year={2020},
volume={},
number={},
pages={681-685},
abstract={In this paper, we address a blind source separation (BSS) problem and propose a new extended framework of independent positive semidefinite tensor analysis (IPSDTA). IPSDTA is a state-of-the-art BSS method that enables us to take interfrequency correlations into account, but the generative model is limited within the multivariate Gaussian distribution and its parameter optimization algorithm does not guarantee stable convergence. To resolve these problems, first, we propose to extend the generative model to a parametric multivariate Student’s t distribution that can deal with various types of signal. Secondly, we derive a new parameter optimization algorithm that guarantees the monotonic nonincrease in the cost function, providing stable convergence. Experimental results reveal that the cost function in the conventional IPSDTA does not display monotonically nonincreasing properties. On the other hand, the proposed method guarantees the monotonic nonincrease in the cost function and outperforms the conventional ILRMA and IPSDTA in the source-separation performance.},
keywords={blind source separation;independent positive semidefinite tensor analysis;Student’s t distribution},
doi={10.1109/ICASSP40776.2020.9054150},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054623,
author={Y. {Takahashi} and D. {Kitahara} and K. {Matsuura} and A. {Hirabayashi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Determined Source Separation Using the Sparsity of Impulse Responses},
year={2020},
volume={},
number={},
pages={686-690},
abstract={In this paper, we propose an over-determined sound source separation method considering the sparsity of impulse responses. Conventional methods, including independent low-rank matrix analysis (ILRMA), have mainly focused on design of realistic sound generation models, but the separation performance is sometimes not improved due to the incorrectness of the generation models and convergence to some poor local minimum. In the proposed method, we utilize a prior information on the mixing process, i.e., the sparsity of impulse responses, to determine the demixing matrices. Numerical experiments using publicly available impulse responses demonstrate that the proposed method based on ILRMA with supervised bases can robustly obtain better results compared to the standard and the supervised ILRMAs.},
keywords={Over-determined source separation;independent low-rank matrix analysis;impulse response;supervised learning},
doi={10.1109/ICASSP40776.2020.9054623},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054683,
author={M. {Delcroix} and T. {Ochiai} and K. {Zmolikova} and K. {Kinoshita} and N. {Tawara} and T. {Nakatani} and S. {Araki}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Speaker Discrimination of Target Speech Extraction With Time-Domain Speakerbeam},
year={2020},
volume={},
number={},
pages={691-695},
abstract={Target speech extraction, which extracts a single target source in a mixture given clues about the target speaker, has attracted increasing attention. We have recently proposed SpeakerBeam, which exploits an adaptation utterance of the target speaker to extract his/her voice characteristics that are then used to guide a neural network towards extracting speech of that speaker. SpeakerBeam presents a practical alternative to speech separation as it enables tracking speech of a target speaker across utterances, and achieves promising speech extraction performance. However, it sometimes fails when speakers have similar voice characteristics, such as in same-gender mixtures, because it is difficult to discriminate the target speaker from the interfering speakers. In this paper, we investigate strategies for improving the speaker discrimination capability of SpeakerBeam. First, we propose a time-domain implementation of SpeakerBeam similar to that proposed for a time-domain audio separation network (TasNet), which has achieved state-of-the-art performance for speech separation. Besides, we investigate (1) the use of spatial features to better discriminate speakers when microphone array recordings are available, (2) adding an auxiliary speaker identification loss for helping to learn more discriminative voice characteristics. We show experimentally that these strategies greatly improve speech extraction performance, especially for same-gender mixtures, and outperform TasNet in terms of target speech extraction.},
keywords={Target speech extraction;time-domain network;spatial features;multi-task loss},
doi={10.1109/ICASSP40776.2020.9054683},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053327,
author={M. {Maciejewski} and G. {Wichern} and E. {McQuinn} and J. L. {Roux}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={WHAMR!: Noisy and Reverberant Single-Channel Speech Separation},
year={2020},
volume={},
number={},
pages={696-700},
abstract={While significant advances have been made with respect to the separation of overlapping speech signals, studies have been largely constrained to mixtures of clean, near anechoic speech, not representative of many real-world scenarios. Although the WHAM! dataset introduced noise to the ubiquitous wsj0-2mix dataset, it did not include reverberation, which is generally present in indoor recordings outside of recording studios. The spectral smearing caused by reverberation can result in significant performance degradation for standard deep learning-based speech separation systems, which rely on spectral structure and the sparsity of speech signals to tease apart sources. To address this, we introduce WHAMR!, an augmented version of WHAM! with synthetic reverberated sources, and provide a thorough baseline analysis of current techniques as well as novel cascaded architectures on the newly introduced conditions.},
keywords={speech separation;speech enhancement;cocktail party problem;reverberation},
doi={10.1109/ICASSP40776.2020.9053327},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054496,
author={A. {Ferreira} and J. {Silva} and F. {Brito} and D. {Sinha}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Impact of a Shift-Invariant Harmonic Phase Model in Fully Parametric Harmonic Voice Representation and Time/Frequency Synthesis},
year={2020},
volume={},
number={},
pages={701-705},
abstract={Harmonic representation models are widely used, notably in speech coding and synthesis. In this paper, we describe two fully parametric harmonic representation and signal reconstruction alternatives that rely on a shift-invariant harmonic phase model and that implement accurate frame-based synthesis in the frequency-domain, and accurate pitch pulse-based synthesis in the time-domain. We use natural spoken and sung voice signals in order to assess the objective and subjective quality of both alternatives when parameters are exact, and when they are replaced by compact and shift-invariant harmonic phase and magnitude approximation models. We highlight the flexibility of these models and present results indicating that not only does the compact shift-invariant phase model cause a smaller impact than that caused by harmonic magnitude modeling, but it also compares favorably to results presented in the literature.},
keywords={Shift-invariant harmonic phase;magnitude parametric models},
doi={10.1109/ICASSP40776.2020.9054496},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053611,
author={A. {Hüwel} and K. {Adiloğlu} and J. {Bach}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Hearing aid Research Data Set for Acoustic Environment Recognition},
year={2020},
volume={},
number={},
pages={706-710},
abstract={State-of-the-art hearing aids (HA) are limited in recognizing acoustic environments. Much effort is spent on research to improve listening experience for HA users in every acoustic situation. There is, however, no dedicated public database to train acoustic environment recognition algorithms with a specific focus on HA applications accounting for their requirements. Existing acoustic scene classification databases are inappropriate for HA signal processing. In this work we propose a novel, binaural HA acoustic environment recognition data set (HEAR-DS) suitable for the environment recognition needs of HAs. We present the details about each individual environment provided within the data set. To show separability of these acoustic environments we trained a group of deep neural network-based classifiers which vary in complexity. The obtained classification accuracies provide a reliable indicator about the validity and separability of the provided data set. Finally, as we do not aim at providing the best possible neural network architecture to perform such a classification, but propose solely a novel data set, further research is needed to streamline such networks and optimize them for robustness, real-time and limited computational capability to fit into modern HAs.},
keywords={machine learning;acoustic scene classification;hearing aid signal processing},
doi={10.1109/ICASSP40776.2020.9053611},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053117,
author={L. {Becker} and A. {Nelus} and J. {Gauer} and L. {Rudolph} and R. {Martin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Audio Feature Extraction for Vehicle Engine Noise Classification},
year={2020},
volume={},
number={},
pages={711-715},
abstract={In this paper we propose a new scheme for vehicle engine noise classification as a more privacy-preserving alternative to classifying vehicles based on video recordings. We establish two scenarios: diesel vs. petrol and heavy goods vehicle vs. personal car classification. Our approach includes a novel modulation-spectrum-based feature representation that is used in conjunction with a siamese neural network classifier. Additionally, a database containing recordings from diverse urban acoustic scenarios is provided. The obtained results show the advantage of the proposed approach compared to conventional feature representations and classifiers. This is achieved by de-correlating background noise from target noise and by quantifying the degree of variation of noise characteristics.},
keywords={Classification;modulation per-channel energy normalization;siamese neural network;vehicle engine noise classification;privacy},
doi={10.1109/ICASSP40776.2020.9053117},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053194,
author={Y. {Wu} and T. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Time-Frequency Feature Decomposition Based on Sound Duration for Acoustic Scene Classification},
year={2020},
volume={},
number={},
pages={716-720},
abstract={Acoustic scene classification is the task of identifying the type of acoustic environment in which a given audio signal is recorded. The signal is a mixture of sound events with various characteristics. In-depth and focused analysis is needed to find out the most representative sound patterns for recognizing and differentiating the scenes. In this paper, we propose a feature decomposition method based on temporal median filtering, and use convolutional neural network to model long-duration background sounds and transient sounds separately. Experiments on log-mel and wavelet based time-frequency features show that using the proposed method leads to better classification accuracy. Analysis of detailed experimental results reveals that (1) long-duration sounds are generally most informative for acoustic scene classification; and (2) the focus of sound duration may be different for classifying different types of acoustic scenes.},
keywords={acoustic scene classification;convolutional neural network;feature decomposition;sound duration;median filtering},
doi={10.1109/ICASSP40776.2020.9053194},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053174,
author={H. {Chen} and W. {Xie} and A. {Vedaldi} and A. {Zisserman}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Vggsound: A Large-Scale Audio-Visual Dataset},
year={2020},
volume={},
number={},
pages={721-725},
abstract={Our goal is to collect a large-scale audio-visual dataset with low label noise from videos ‘in the wild’ using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 200k videos for 300 audio classes. Third, we investigate various Convolutional Neural Network (CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/.},
keywords={audio recognition;audio-visual correspondence;large-scale;dataset;convolutional neural network},
doi={10.1109/ICASSP40776.2020.9053174},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053338,
author={E. B. {Çoban} and D. {Pir} and R. {So} and M. I. {Mandel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Transfer Learning from Youtube Soundtracks to Tag Arctic Ecoacoustic Recordings},
year={2020},
volume={},
number={},
pages={726-730},
abstract={Sound provides a valuable tool for long-term monitoring of sensitive animal habitats at a spatial scale larger than camera traps or field observations, while also providing more details than satellite imagery. Currently, the ability to collect such recordings outstrips the ability to analyze them manually, necessitating the development of automatic analysis methods. While several datasets and models of large corpora of video soundtracks have recently been released, it is not clear to what extent these models will generalize to environmental recordings and the scientific questions of interest in analyzing them. This paper investigates this generalization in several ways and finds that models themselves display limited performance, however, their intermediate representations can be used to train successful models on small sets of labeled data.},
keywords={Ecoacoustics;soundscape analysis;transfer learning},
doi={10.1109/ICASSP40776.2020.9053338},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053671,
author={G. {Nam} and S. {Bu} and N. {Park} and J. {Seo} and H. {Jo} and W. {Jeong}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Data Augmentation Using Empirical Mode Decomposition on Neural Networks to Classify Impact Noise in Vehicle},
year={2020},
volume={},
number={},
pages={731-735},
abstract={In a vehicle, impact noise may occur during steering action due to clearance between parts of steering systems. Via structural path the noise is perceived by the drivers' ears and it can be the cause of a repair campaign. It is importatnt to know where the collision occurs to modify the parts causing impact noise. In this paper, we performed data augmentation using Empirical Mode Decomposition (EMD) method that decomposes the original signal into a finite number of intrinsic mode functions (IMFs). The IMFs were decomposed by descending order from high frequency to low frequency, and we add the residue each time one IMF is separated. After the data augmentation, the data were trained using the neural network model CNN-LSTM. The proposed method showed better classification performance than other classification methods. It seems that proposed method takes advantage of the impact noise characteristics concentrated at low frequency range.},
keywords={Empirical Mode Decomposition;Data augmentation;Neural network;Impact noise;In-vehicle},
doi={10.1109/ICASSP40776.2020.9053671},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052990,
author={K. {Drossos} and S. {Lipping} and T. {Virtanen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Clotho: an Audio Captioning Dataset},
year={2020},
volume={},
number={},
pages={736-740},
abstract={Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online 1.},
keywords={audio captioning;dataset;Clotho},
doi={10.1109/ICASSP40776.2020.9052990},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053018,
author={A. E. {Jaramillo} and A. {Jakobsson} and J. K. {Nielsen} and M. {Græsbøll Christensen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Fundamental Frequency Estimation in Coloured Noise},
year={2020},
volume={},
number={},
pages={741-745},
abstract={Most parametric fundamental frequency estimators make the implicit assumption that any corrupting noise is additive, white Gaus-sian. Under this assumption, the maximum likelihood (ML) and the least squares estimators are the same, and statistically efficient. However, in the coloured noise case, the estimators differ, and the spectral shape of the corrupting noise should be taken into account. To allow for this, we here propose two schemes that refine the noise statistics and parameter estimates in an iterative manner, one of them based on an approximate ML solution and the other one based on removing the periodic signal obtained from a linearly constrained minimum variance (LCMV) filter. Evaluations on real speech data indicate that the iteration steps improve the estimation accuracy, therefore offering improvement over traditional non-parametric fundamental frequency methods in most of the evaluated scenarios.},
keywords={fundamental frequency;coloured noise;maximum likelihood;pre-whitening;least-squares;LCMV filter},
doi={10.1109/ICASSP40776.2020.9053018},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053533,
author={A. {Solomes} and D. {Stowell}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Efficient Bird Sound Detection on the Bela Embedded System},
year={2020},
volume={},
number={},
pages={746-750},
abstract={Monitoring wildlife is an important aspect of conservation initiatives. Deep learning detectors can help with this, although it is not yet clear whether they can run efficiently on an embedded system in the wild. This paper proposes an automatic detection algorithm for the Bela embedded Linux device for wildlife monitoring. The algorithm achieves good quality recognition, efficiently running on continuously streamed data on a commercially available platform. The program is capable of computing on-board detection using convolutional neural networks (CNNs) with an AUC score of 82.5% on the testing set of an international data challenge. This paper details how the model is exported to work on the Bela Mini in C++, with the spectrogram generation and the implementation of the feed-forward network, and evaluates its performance on the Bird Audio Detection challenge 2018 DCASE data.},
keywords={embedded;bioacoustics;acoustic;detection;deep learning},
doi={10.1109/ICASSP40776.2020.9053533},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054315,
author={O. {Berlage} and K. {Lux} and D. {Graus}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Automated Segmentation of Radio Shows with Audio Embeddings},
year={2020},
volume={},
number={},
pages={751-755},
abstract={Audio features have been proven useful for increasing the performance of automated topic segmentation systems. This study explores the novel task of using audio embeddings for automated, topically coherent segmentation of radio shows. We created three different audio embedding generators using multi-class classification tasks on three datasets from different domains. We evaluate topic segmentation performance of the audio embeddings and compare it against a text-only baseline. We find that a set-up including audio embeddings generated through a non-speech sound event classification task significantly outperforms our text-only baseline by 32.3% in F1-measure. In addition, we find that different classification tasks yield audio embeddings that vary in segmentation performance.},
keywords={segmentation;audio embeddings;sound classification},
doi={10.1109/ICASSP40776.2020.9054315},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053298,
author={M. {Brousmiche} and J. {Rouat} and S. {Dupont}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={SECL-UMons Database for Sound Event Classification and Localization},
year={2020},
volume={},
number={},
pages={756-760},
abstract={We introduce the SECL-UMons dataset for sound event classification and localization in the context of office environments. The multichannel dataset is composed of 11 event classes recorded at several realistic positions in two different rooms. The dataset comprises two types of sequences according to the number of events in the sequence. 2662 unilabel sequences and 2724 multilabel sequences are recorded corresponding to a total of 5.24 hours. The database is publicly available to provide support for algorithm development and common ground for comparison of different techniques. The DCASE 2019 challenge baseline (SELDnet) employing a convolutional recurrent neural network is used to generate benchmark scores for the new dataset. We also slightly modify the model to introduce a benchmark score for real-time classification and localization for the new dataset.},
keywords={Dataset;Sound Source Localization;Sound Event Classification},
doi={10.1109/ICASSP40776.2020.9053298},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054500,
author={S. {Kothinti} and B. {Skerritt-Davis} and A. {Nair} and M. {Elhilali}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Synthesizing Engaging Music Using Dynamic Models of Statistical Surprisal},
year={2020},
volume={},
number={},
pages={761-765},
abstract={Synthesis of music content generally leverages the underlying statistical structure of music to develop generative models, able to create new musical expressions within the same genre. In this work, we explore the statistical structure of a musical corpus and its effect on modulating the attention of listeners. The study specifically explores listeners' engagement to newly synthesized music and tests the hypothesis that maximizing statistical surprisal would result in increased auditory salience. The study employs a dynamical statistical model to estimate melodic line surprisal and develops an optimization procedure using parametrized codebooks to synthesize musical segments that maximize statistical surprisal. A behavioral experiment with a dichotic listening task is designed to probe salience of the synthesized melodies against original melodies by measuring listeners' engagement in a continuous-fashion. Results indicate that we can control the salience of sounds by manipulating the statistical surprisal, guided by the complexity of the temporal structure of the musical corpus. This work suggests that future work in automated music synthesis could leverage statistical models of music beyond musical aesthetics to also manipulate the degree of engagement.},
keywords={Statistical surprisal;auditory attention;music synthesis;auditory salience;regularity extraction},
doi={10.1109/ICASSP40776.2020.9054500},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054020,
author={Y. {Wang} and X. {Guan} and Y. {Du} and N. {Nan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Harmonics Based Representation in Clarinet Tone Quality Evaluation},
year={2020},
volume={},
number={},
pages={766-770},
abstract={Music tone quality evaluation is generally performed by experts. It could be subjective and short of consistency and fairness as well as time-consuming. In this paper we present a new method for identifying the clarinet reed quality by evaluating tone quality based on the harmonic structure and energy distribution. We first decouple the quality of reed and clarinet pipe based on the acoustic harmonics, and discover that the reed quality is strongly relevant to the even parts of the harmonics. Then we construct a features set consisting of the even harmonic envelope and the energy distribution of harmonics in spectrum. The annotated clarinet audio data are recorded from 3 levels of performers and the tone quality is classified by machine learning. The results show that our new method for identifying low and medium high tones significantly outperforms previous methods.},
keywords={tone quality evaluation;acoustic model;harmonic features;machine learning;clarinet tone quality},
doi={10.1109/ICASSP40776.2020.9054020},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054340,
author={E. {Manilow} and P. {Seetharaman} and B. {Pardo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Simultaneous Separation and Transcription of Mixtures with Multiple Polyphonic and Percussive Instruments},
year={2020},
volume={},
number={},
pages={771-775},
abstract={We present a single deep learning architecture that can both separate an audio recording of a musical mixture into constituent single-instrument recordings and transcribe these instruments into a human-readable format at the same time, learning a shared musical representation for both tasks. This novel architecture, which we call Cerberus, builds on the Chimera network for source separation by adding a third "head" for transcription. By training each head with different losses, we are able to jointly learn how to separate and transcribe up to five instruments with a single network. We show that separation and transcription are highly complementary with one another and when learned jointly, lead to Cerberus networks that are better at both separation and transcription and generalize better to unseen mixtures.},
keywords={source separation;music transcription;multitask learning;deep clustering;computer audition},
doi={10.1109/ICASSP40776.2020.9054340},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054329,
author={T. {Greer} and K. {Mundnich} and M. {Sachs} and S. {Narayanan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={The Role of Annotation Fusion Methods in the Study of Human-Reported Emotion Experience During Music Listening},
year={2020},
volume={},
number={},
pages={776-780},
abstract={Music is a universally-enjoyed art form, but listeners often respond to it in tremendously different ways. The same song can bring one person great joy and another deep sorrow. This paper focuses on modeling human music experience at the group level. In this scenario, human annotations serve an important role in computational modeling, especially where the target constructs under study are hidden, such as dimensions of emotion or enjoyment to music listening. In this work, we investigate several ways to represent aggregate human annotations of the complex, subjective emotional experience of listening to music. We show the utility of several methods for fusing self-reported emotion and enjoyment ratings by predicting these responses with auditory features. Using traditional methods such as time alignment with simple averaging and Dynamic Time Warping, as well as state-of-the-art methods based on Expectation Maximization and Triplet Embeddings, we show that it is possible to accurately represent hidden constructs in time under noisy sampling conditions, evidenced by better performance on behavioral response predictions. That subjective responses to complex musical stimuli can be accurately captured using these methods suggests more general applications to research in areas such as affective computing and music perception.},
keywords={Music perception;music emotion recognition;annotation fusion;Triplet Embeddings;inter-rater agreement},
doi={10.1109/ICASSP40776.2020.9054329},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053024,
author={P. {Chandna} and M. {Blaauw} and J. {Bonada} and E. {Gómez}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Content Based Singing Voice Extraction from a Musical Mixture},
year={2020},
volume={},
number={},
pages={781-785},
abstract={We present a deep learning based methodology for extracting the singing voice signal from a musical mixture based on the underlying linguistic content. Our model follows an encoder-decoder architecture and takes as input the magnitude component of the spectrogram of a musical mixture with vocals. The encoder part of the model is trained via knowledge distillation using a teacher network to learn a content embedding, which is decoded to generate the corresponding vocoder features. Using this methodology, we are able to extract the unprocessed raw vocal signal from the mixture even for a processed mixture dataset with singers not seen during training. While the nature of our system makes it incongruous with traditional objective evaluation metrics, we use subjective evaluation via listening tests to compare the methodology to state-of-the-art deep learning based source separation algorithms. We also provide sound examples and source code for reproducibility.},
keywords={Source separation;singing voice;content disentangling;knowledge distillation;AutoVC},
doi={10.1109/ICASSP40776.2020.9053024},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053128,
author={A. {Ramires} and P. {Chandna} and X. {Favory} and E. {Gómez} and X. {Serra}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Neural Percussive Synthesis Parameterised by High-Level Timbral Features},
year={2020},
volume={},
number={},
pages={786-790},
abstract={We present a deep neural network-based methodology for synthesising percussive sounds with control over high-level timbral characteristics of the sounds. This approach allows for intuitive control of a synthesizer, enabling the user to shape sounds without extensive knowledge of signal processing. We use a feedforward convolutional neural network-based architecture, which is able to map input parameters to the corresponding waveform. We propose two datasets to evaluate our approach on both a restrictive context, and in one covering a broader spectrum of sounds. The timbral features used as parameters are taken from recent literature in signal processing. We also use these features for evaluation and validation of the presented model, to ensure that changing the input parameters produces a congruent waveform with the desired characteristics. Finally, we evaluate the quality of the output sound using a subjective listening test. We provide sound examples and the system’s source code for reproducibility.},
keywords={Wave-U-Net;Percussive Sound Synthesis;Generative Models;Music Information Retrieval;Creative Interfaces},
doi={10.1109/ICASSP40776.2020.9053128},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053576,
author={R. {Nakatsu} and D. {Kitahara} and A. {Hirabayashi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Non-Griffin–Lim Type Signal Recovery from Magnitude Spectrogram},
year={2020},
volume={},
number={},
pages={791-795},
abstract={Speech and audio signal processing frequently requires to recover a time-domain signal from the magnitude of a spectrogram. Conventional methods inversely transform the magnitude spectrogram with a phase spectrogram recovered by the Griffin–Lim algorithm or its accelerated versions. The short-time Fourier transform (STFT) perfectly matches this framework, while other useful spectrogram transforms, such as the constant-Q transform (CQT), do not, because their inverses cannot be computed easily. To make the best of such useful spectrogram transforms, we propose an algorithm which recovers the time-domain signal without the inverse spectrogram transforms. We formulate the signal recovery as a nonconvex optimization problem, which is difficult to solve exactly. To approximately solve the problem, we exploit a stochastic convex optimization technique. A well-organized block selection enables us both to avoid local minimums and to achieve fast convergence. Numerical experiments show the effectiveness of the proposed method for both STFT and CQT cases.},
keywords={Spectrogram;signal recovery;Griffin–Lim algorithm;constant-Q transform;nonconvex stochastic optimization},
doi={10.1109/ICASSP40776.2020.9053576},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054181,
author={K. {Subramani} and P. {Rao} and A. {D’Hooge}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Vapar Synth - A Variational Parametric Model for Audio Synthesis},
year={2020},
volume={},
number={},
pages={796-800},
abstract={With the advent of data-driven statistical modeling and abundant computing power, researchers are turning increasingly to deep learning for audio synthesis. These methods try to model audio signals directly in the time or frequency domain. In the interest of more flexible control over the generated sound, it could be more useful to work with a parametric representation of the signal which corresponds more directly to the musical attributes such as pitch, dynamics and timbre. We present Va-Par Synth - a Variational Parametric Synthesizer which utilizes a conditional variational autoencoder (CVAE) trained on a suitable parametric representation. We demonstrate1 our proposed model’s capabilities via the reconstruction and generation of instrumental tones with flexible control over their pitch.},
keywords={Generative Models;Conditional VAE;Source-Filter Model;Spectral Modeling Synthesis},
doi={10.1109/ICASSP40776.2020.9054181},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054194,
author={M. {Lagrange} and F. {Gontier}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Bandwidth Extension of Musical Audio Signals With No Side Information Using Dilated Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={801-805},
abstract={Bandwidth extension has a long history in audio processing. While speech processing tools do not rely on side information, production-ready bandwidth extension tools of general audio signals rely on side information that has to be transmitted alongside the bitstream of the low frequency part, mostly because polyphonic music has a more complex and less predictable spectral structure than speech.This paper studies the benefit of considering a dilated fully convolutional neural network to perform the bandwidth extension of musical audio signals with no side information on the magnitude spectra. Experimental evaluation using two public datasets, medley-solos-db and gtzan, respectively of monophonic and polyphonic music demonstrate that the proposed architecture achieves state of the art performance.},
keywords={Artificial audio bandwidth extension;deep neural network;musical audio processing},
doi={10.1109/ICASSP40776.2020.9054194},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053542,
author={M. {Huber} and G. {Schindler} and C. {Schörkhuber} and W. {Roth} and F. {Pernkopf} and H. {Fröning}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Towards Real-Time Single-Channel Singing-Voice Separation with Pruned Multi-Scaled Densenets},
year={2020},
volume={},
number={},
pages={806-810},
abstract={Modern musical source separation systems based on deep neural networks reach unprecedented levels of separation quality. However, harnessing the power of these large-scale models in typical audio production environments, which frequently offer only limited computing resources while demanding real-time processing, remains challenging. We extend the multi-scaled DenseNet in several aspects to facilitate real-time source separation scenarios. Specifically, we reduce the computational requirements by inferring Mel-scaled masks and decrease the model size via effective use of bottleneck layers, while improving performance using a deep clustering objective. In addition, we are able to further increase the model efficiency by applying parameterized structured pruning of convolutional weights without any significant impact on the separation performance. We significantly reduce the model size and increase the computational efficiency by a factor of 1.6 and 4.3, respectively, while maintaining the separation performance.},
keywords={Musical Source Separation;Real-time;Parameterized Structured Pruning;Multi-scaled DenseNet},
doi={10.1109/ICASSP40776.2020.9053542},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054435,
author={V. S. {Viraraghavan} and A. {Pal} and H. {Murthy} and R. {Aravind}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={State-Based Transcription of Components of Carnatic Music},
year={2020},
volume={},
number={},
pages={811-815},
abstract={Automatic Carnatic Music (CM) transcription is an open problem in need of a standardized descriptive notation. The level of detail needed in a descriptive transcription makes it tedious to obtain ground truth by manual means. In this paper, we propose a novel state-based representation of the pitch curve motivated by CM components called constant-pitch notes and stationary points. We also propose a novel transcription technique that uses the Viterbi algorithm to estimate the states and quantized pitch-values. The proposed technique adheres best to rāga-notes compared to the existing critical-points technique and uniform quantization. In a listening test, clips synthesized from the proposed notation were rated significantly better (324 ratings, p < 0.001) than those from critical-points. Further, speed-halving based on state information best matches the actual, observed CM component-duration ratios without losing rāga-characteristics. Thus, the proposed transcription can be corrected manually to obtain ground truth and can enhance learning tools.},
keywords={Descriptive transcription;Carnatic music;CPN-STA model;Viterbi algorithm},
doi={10.1109/ICASSP40776.2020.9054435},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053513,
author={D. {Samuel} and A. {Ganeshan} and J. {Naradowsky}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Meta-Learning Extractors for Music Source Separation},
year={2020},
volume={},
number={},
pages={816-820},
abstract={We propose a hierarchical meta-learning-inspired model for music source separation (Meta-TasNet) in which a generator model is used to predict the weights of individual extractor models. This enables efficient parameter-sharing, while still allowing for instrument-specific parameterization. Meta-TasNet is shown to be more effective than the models trained independently or in a multi-task setting, and achieve performance comparable with state-of-the-art methods. In comparison to the latter, our extractors contain fewer parameters and have faster run-time performance. We discuss important architectural considerations, and explore the costs and benefits of this approach.},
keywords={music source separation;meta-learning},
doi={10.1109/ICASSP40776.2020.9053513},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053501,
author={Y. {Masuyama} and M. {Togami} and T. {Komatsu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Consistency-Aware Multi-Channel Speech Enhancement Using Deep Neural Networks},
year={2020},
volume={},
number={},
pages={821-825},
abstract={This paper proposes a deep neural network (DNN)–based multichannel speech enhancement system in which a DNN is trained to maximize the quality of the enhanced time-domain signal. DNN-based multi-channel speech enhancement is often conducted in the time-frequency (T-F) domain because spatial filtering can be efficiently implemented in the T-F domain. In such a case, ordinary objective functions are computed on the estimated T-F mask or spectrogram. However, the estimated spectrogram is often inconsistent, and its amplitude and phase may change when the spectrogram is converted back to the time-domain. That is, the objective function does not evaluate the enhanced time-domain signal properly. To address this problem, we propose to use an objective function defined on the reconstructed time-domain signal. Specifically, speech enhancement is conducted by multi-channel Wiener filtering in the T-F domain, and its result is converted back to the time-domain. We propose two objective functions computed on the reconstructed signal where the first one is defined in the time-domain, and the other one is defined in the T-F domain. Our experiment demonstrates the effectiveness of the proposed system comparing to T-F masking and mask-based beamforming.},
keywords={Multi-channel Wiener filtering;Spectrogram consistency;deep neural networks (DNNs)},
doi={10.1109/ICASSP40776.2020.9053501},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053234,
author={Y. {Masuyama} and K. {Yatabe} and Y. {Koizumi} and Y. {Oikawa} and N. {Harada}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Phase Reconstruction Based On Recurrent Phase Unwrapping With Deep Neural Networks},
year={2020},
volume={},
number={},
pages={826-830},
abstract={Phase reconstruction, which estimates phase from a given amplitude spectrogram, is an active research field in acoustical signal processing with many applications including audio synthesis. To take advantage of rich knowledge from data, several studies presented deep neural network (DNN)–based phase reconstruction methods. However, the training of a DNN for phase reconstruction is not an easy task because phase is sensitive to the shift of a waveform. To overcome this problem, we propose a DNN-based two-stage phase reconstruction method. In the proposed method, DNNs estimate phase derivatives instead of phase itself, which allows us to avoid the sensitivity problem. Then, phase is recursively estimated based on the estimated derivatives, which is named recurrent phase unwrapping (RPU). The experimental results confirm that the proposed method outperformed the direct phase estimation by a DNN.},
keywords={Spectrogram inversion;group delay;instantaneous frequency;time-frequency analysis;recurrent neural network},
doi={10.1109/ICASSP40776.2020.9053234},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053846,
author={S. {Sonning} and C. {Schüldt} and H. {Erdogan} and S. {Wisdom}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Performance Study of a Convolutional Time-Domain Audio Separation Network for Real-Time Speech Denoising},
year={2020},
volume={},
number={},
pages={831-835},
abstract={Time-domain audio separation networks based on dilated temporal convolutions have recently been shown to perform very well compared to methods that are based on a time-frequency representation in speech separation tasks, even outperforming an oracle binary time-frequency mask of the speakers. This paper investigates the performance of such a time-domain network (Conv-TasNet) for speech denoising in a real-time setting, comparing various parameter settings. Most importantly, different amounts of lookahead are evaluated and compared to the baseline of a fully causal model. We show that a large part of the increase in performance between a causal and non-causal model is achieved with a lookahead of only 20 milliseconds, demonstrating the usefulness of even small lookaheads for many real-time applications.},
keywords={Speech enhancement;noise reduction;deep learning;convolutional neural networks;time domain},
doi={10.1109/ICASSP40776.2020.9053846},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053989,
author={B. {Tolooshams} and R. {Giri} and A. H. {Song} and U. {Isik} and A. {Krishnaswamy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Channel-Attention Dense U-Net for Multichannel Speech Enhancement},
year={2020},
volume={},
number={},
pages={836-840},
abstract={Supervised deep learning has gained significant attention for speech enhancement recently. The state-of-the-art deep learning methods perform the task by learning a ratio/binary mask that is applied to the mixture in the time-frequency domain to produce the clean speech. Despite the great performance in the single-channel setting, these frameworks lag in performance in the multichannel setting as the majority of these methods a) fail to exploit the available spatial information fully, and b) still treat the deep architecture as a black box which may not be well-suited for multichannel audio processing. This paper addresses these drawbacks, a) by utilizing complex ratio masking instead of masking on the magnitude of the spectrogram, and more importantly, b) by introducing a channel-attention mechanism inside the deep architecture to mimic beamforming. We propose Channel-Attention Dense U-Net, in which we apply the channel-attention unit recursively on feature maps at every layer of the network, enabling the network to perform non-linear beamforming. We demonstrate the superior performance of the network against the state-of-the-art approaches on the CHiME-3 dataset.},
keywords={Channel-Attention;U-Net;Complex Ratio Masking;Multichannel Speech Enhancement.},
doi={10.1109/ICASSP40776.2020.9053989},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053821,
author={Y. {Yemini} and S. E. {Chazan} and J. {Goldberger} and S. {Gannot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Composite DNN Architecture for Speech Enhancement},
year={2020},
volume={},
number={},
pages={841-845},
abstract={In speech enhancement, the use of supervised algorithms in the form of deep neural networks (DNNs) has become tremendously popular in recent years. The target function of the DNN (and the associated estimators) is often either a masking function applied to the noisy spectrum, or the clean log-spectrum. In this work, we show that both separate cost functions are unsuitable for dealing with narrowband noise, and propose a new composite estimator in the log-spectrum domain. The new technique relies on a single DNN that outputs both a masking function and an estimated log-spectrum. Both outputs are used for the composite enhancement. The proposed estimator demonstrates superior performance for speech utterances contaminated by additive narrowband noise, while maintaining the enhancement quality of the baseline algorithms for wideband noise.},
keywords={Speech enhancement;deep neural network;narrowband noise;single microphone},
doi={10.1109/ICASSP40776.2020.9053821},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053649,
author={L. {Li} and K. {Koishida}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Geometrically Constrained Independent Vector Analysis for Directional Speech Enhancement},
year={2020},
volume={},
number={},
pages={846-850},
abstract={This paper addresses the multichannel directional speech enhancement problem with geometrically constrained independent vector analysis (GCIVA), where we aim to combine the high separation performance from blind source separation and the capability of directional focus from beamforming. The proposed method exploits geometric constraints composed from the spatial information of sources to guide the target speech to the desired output channel. A convergence-guaranteed parameter estimation algorithm is derived from the framework of auxiliary function-based IVA (AuxIVA) to take advantage of fast convergence, low computational cost, and no step-size tuning. We propose a dual-microphone speech enhancement system based on the proposed method and investigate its effectiveness with objective metrics. The experimental evaluations revealed that the proposed system outperformed the conventional beamforming and the standard AuxIVA in a large margin in terms of source-to-distortion and source-to-interference ratios.},
keywords={Speech enhancement;independent vector analysis;geometric constraints;multichannel;auxiliary function approach},
doi={10.1109/ICASSP40776.2020.9053649},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054597,
author={D. {Takeuchi} and K. {Yatabe} and Y. {Koizumi} and Y. {Oikawa} and N. {Harada}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Real-Time Speech Enhancement Using Equilibriated RNN},
year={2020},
volume={},
number={},
pages={851-855},
abstract={We propose a speech enhancement method using a causal deep neural network (DNN) for real-time applications. DNN has been widely used for estimating a time-frequency (T-F) mask which enhances a speech signal. One popular DNN structure for that is a recurrent neural network (RNN) owing to its capability of effectively modelling time-sequential data like speech. In particular, the long short-term memory (LSTM) is often used to alleviate the vanishing/exploding gradient problem which makes the training of an RNN difficult. However, the number of parameters of LSTM is increased as the price of mitigating the difficulty of training, which requires more computational resources. For real-time speech enhancement, it is preferable to use a smaller network without losing the performance. In this paper, we propose to use the equilibriated recurrent neural network (ERNN) for avoiding the vanishing/exploding gradient problem without increasing the number of parameters. The proposed structure is causal, which requires only the information from the past, in order to apply it in real-time. Compared to the uni- and bi-directional LSTM networks, the proposed method achieved the similar performance with much fewer parameters.},
keywords={Real-time speech enhancement;equiribriated recurrent neural network;vanishing/exploding gradient problem},
doi={10.1109/ICASSP40776.2020.9054597},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052934,
author={D. {Fischer} and S. {Doclo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Subspace-Based Speech Correlation Vector Estimation for Single-Microphone Multi-Frame MVDR Filtering},
year={2020},
volume={},
number={},
pages={856-860},
abstract={Aiming at exploiting the speech correlation across consecutive timeframes in the short-time Fourier transform domain, the multi-frame minimum variance distortionless response (MFMVDR) filter for single-microphone speech enhancement has been proposed. This filter is designed to avoid speech distortion while minimizing the total signal output power. To compute the MFMVDR filter, an estimate of the highly time-varying normalized speech correlation vector is required. In this paper, we propose a subspace-based estimator for the normalized speech correlation vector based on the Q largest eigenvalues and their corresponding eigenvectors of the prewhitened noisy speech correlation matrix. Experimental results for different speech signals, noise types and signal-to-noise ratios show that the proposed subspace-based estimator yields the best results in terms of speech quality and noise reduction compared to a state-of-the-art maximum-likelihood estimator.},
keywords={MVDR;subspace estimation;interframe speech correlation;speech enhancement},
doi={10.1109/ICASSP40776.2020.9052934},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053712,
author={H. {Wang} and D. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Time-Frequency Loss for CNN Based Speech Super-Resolution},
year={2020},
volume={},
number={},
pages={861-865},
abstract={Speech super-resolution (SR), also called speech bandwidth extension (BWE), aims to increase the sampling rate of a given lower resolution speech signal. Recent years have witnessed the successful application of deep neural networks in time or frequency domains, and deep learning has improved the performance considerably compared with conventional approaches. This paper proposes an autoencoder based fully convolutional neural network (CNN) that merges the information from both time and frequency domains. At the training time, we optimize the CNN using a new time-frequency loss (T-F loss), which combines a time domain loss and a frequency domain loss. The experimental results show that our model trained with the T-F loss achieves significantly better results than other state-of-the-art models, and yields balanced performance in terms of time and frequency metrics.},
keywords={Super-resolution;bandwidth extension;deep learning;convolutional neural network;T-F loss},
doi={10.1109/ICASSP40776.2020.9053712},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054551,
author={X. {Hao} and C. {Xu} and N. {Hou} and L. {Xie} and E. S. {Chng} and H. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Time-Domain Neural Network Approach for Speech Bandwidth Extension},
year={2020},
volume={},
number={},
pages={866-870},
abstract={In this paper, we study the time-domain neural network approach for speech bandwidth extension. We propose a network architecture, named multi-scale fusion neural network (MfNet), that gradually restores the low-frequency signal and predicts the high-frequency signal through the exchange of information across different scale representations. We propose a training scheme to optimize the network with a combination of perceptual loss and time-domain adversarial loss. Experiments show the proposed multi-scale fusion network consistently outperforms the competing methods in terms of perceptual evaluation of speech quality (PESQ), signal to distortion rate (SDR), signal to noise ratio (SNR), log-spectral distance (LSD) and word error rate (WER). More promisingly, the multi-scale fusion network requires only 10% of the parameters of the time-domain reference baseline.},
keywords={speech bandwidth extension;multi-scale fusion;neural networks;deep learning},
doi={10.1109/ICASSP40776.2020.9054551},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054254,
author={Y. {Xia} and S. {Braun} and C. K. A. {Reddy} and H. {Dubey} and R. {Cutler} and I. {Tashev}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Weighted Speech Distortion Losses for Neural-Network-Based Real-Time Speech Enhancement},
year={2020},
volume={},
number={},
pages={871-875},
abstract={This paper investigates several aspects of training a RNN (recurrent neural network) that impact the objective and subjective quality of enhanced speech for real-time single-channel speech enhancement. Specifically, we focus on a RNN that enhances short-time speech spectra on a single-frame-in, single-frame-out basis, a framework adopted by most classical signal processing methods. We propose two novel mean-squared-error-based learning objectives that enable separate control over the importance of speech distortion versus noise reduction. The proposed loss functions are evaluated by widely accepted objective quality and intelligibility measures and compared to other competitive online methods. In addition, we study the impact of feature normalization and varying batch sequence lengths on the objective quality of enhanced speech. Finally, we show subjective ratings for the proposed approach and a state-of-the-art real-time RNN-based method.},
keywords={Real-time speech enhancement;recurrent neural networks;loss function;speech distortion;mean opinion score},
doi={10.1109/ICASSP40776.2020.9054254},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053683,
author={H. E. {Romero} and N. {Ma} and G. J. {Brown}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Snorer Diarisation Based On Deep Neural Network Embeddings},
year={2020},
volume={},
number={},
pages={876-880},
abstract={Acoustic analysis of sleep breathing sounds using a smartphone at home provides a much less obtrusive means of screening for sleep-disordered breathing (SDB) than assessment in a sleep clinic. However, application in a home environment is confounded by the problem that a bed partner may also be present and snore. This paper proposes a novel acoustic analysis system for snorer diarisation, a concept extrapolated from speaker diarisation research, which allows screening for SDB of both the user and the bed partner using a single smartphone. The snorer diarisation system involves three steps. First, a deep neural network (DNN) is employed to estimate the number of concurrent snorers in short segments of monaural audio recordings. Second, the identified snore segments are clustered using snorer embeddings, a feature representation that allows different snorers to be discriminated. Finally, a snore transcription is automatically generated for each snorer by combining consecutive snore segments. The system is evaluated on both synthetic snore mixtures and real two-snorer recordings. The results show that it is possible to accurately screen a subject and their bed partner for SDB in the same session from recordings of a single smartphone.},
keywords={Snorer diarisation;sleep-disordered breathing;deep neural network embeddings;LSTM},
doi={10.1109/ICASSP40776.2020.9053683},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053474,
author={C. {Wang} and V. {Lostanlen} and E. {Benetos} and E. {Chew}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Playing Technique Recognition by Joint Time–Frequency Scattering},
year={2020},
volume={},
number={},
pages={881-885},
abstract={Playing techniques are important expressive elements in music signals. In this paper, we propose a recognition system based on the joint time–frequency scattering transform (jTFST) for pitch evolution-based playing techniques (PETs), a group of playing techniques with monotonic pitch changes over time. The jTFST represents spectro-temporal patterns in the time–frequency domain, capturing discriminative information of PETs. As a case study, we analyse three commonly used PETs of the Chinese bamboo flute: acciacatura, portamento, and glissando, and encode their characteristics using the jTFST. To verify the proposed approach, we create a new dataset, the CBF-petsDB, containing PETs played in isolation as well as in the context of whole pieces performed and annotated by professional players. Feeding the jTFST to a machine learning classifier, we obtain F-measures of 71% for acciacatura, 59% for portamento, and 83% for glissando detection, and provide explanatory visualisations of scattering coefficients for each technique.},
keywords={Music signal analysis;scattering transform;performance analysis;playing technique recognition},
doi={10.1109/ICASSP40776.2020.9053474},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053172,
author={F. {Gontier} and M. {Lagrange} and C. {Lavandier} and J. {Petiot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Privacy Aware Acoustic Scene Synthesis Using Deep Spectral Feature Inversion},
year={2020},
volume={},
number={},
pages={886-890},
abstract={Gathering information about the acoustic environment of urban areas is now possible and studied in many major cities in the world. Part of the research is to find ways to inform the citizen about its sound environment while ensuring her privacy.We study in this paper how this application can be cast into a feature inversion problem. We argue that considering deep learning techniques to solve this problem allows us to produce sound sketches that are representative and privacy aware. Experiments done considering the dcase2017 dataset shows that the proposed learning based approach achieves state of the art performance when compared to blind inversion approaches.},
keywords={spectral feature inversion;privacy aware audio synthesis;deep neural network;environmental audio processing},
doi={10.1109/ICASSP40776.2020.9053172},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053676,
author={M. {Madruga} and Y. {Campos-Roca} and C. J. {Pérez}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robustness Assessment of Automatic Reinke’s Edema Diagnosis Systems},
year={2020},
volume={},
number={},
pages={891-895},
abstract={In the past few years there has been a great interest in computer aided diagnosis research. In the field of voice quality assessment, signal processing gives us tools to analyze and extract numeric characteristics describing the analyzed signal. These features might be used to tell an impaired voice from a healthy one for many different voice conditions, being Reinke’s edema one of the most severe ones. Most studies have been carried out under strict laboratory conditions, making use of professional sound equipment and facilities, trying to minimize the influence of external conditions. However, real world situations are exposed to adverse acoustic environments. The goal of this paper is to build automatic detection systems for Reinke’s edema based on a novel in-house dataset and, alternatively, on the Massachusetts Eye and Ear Infirmary Voice Disorders Database, and assess noise robustness in both cases.},
keywords={Noise;voice analysis;robust features;automatic diagnosis;Reinke’s edema},
doi={10.1109/ICASSP40776.2020.9053676},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053268,
author={M. {Whitehill} and J. {Garrison} and S. {Patel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Whosecough: In-the-Wild Cougher Verification Using Multitask Learning},
year={2020},
volume={},
number={},
pages={896-900},
abstract={Current automatic cough counting systems can determine how many coughs are present in an audio recording. However, they cannot determine who produced the cough. This limits their usefulness as most systems are deployed in locations with multiple people (i.e., a smart home device in a four-person home). Previous models trained solely on speech performed reasonably well on forced coughs [1]. By incorporating coughs into the training data, the model performance should improve. However, since limited natural cough data exists, training on coughs can lead to model overfitting. In this work, we overcome this problem by using multitask learning, where the second task is speaker verification. Our model achieves 82.15% classification accuracy amongst four users on a natural, in-the-wild cough dataset, outperforming human evaluators on average by 9.82%.},
keywords={Cough;Health Sensing;Multitask Learning;Speaker Verification;Deep Neural Networks},
doi={10.1109/ICASSP40776.2020.9053268},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052908,
author={J. {Cramer} and V. {Lostanlen} and A. {Farnsworth} and J. {Salamon} and J. P. {Bello}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Chirping up the Right Tree: Incorporating Biological Taxonomies into Deep Bioacoustic Classifiers},
year={2020},
volume={},
number={},
pages={901-905},
abstract={Class imbalance in the training data hinders the generalization ability of machine listening systems. In the context of bioacoustics, this issue may be circumvented by aggregating species labels into super-groups of higher taxonomic rank: genus, family, order, and so forth. However, different applications of machine listening to wildlife monitoring may require different levels of granularity. This paper introduces TaxoNet, a deep neural network for structured classification of signals from living organisms. TaxoNet is trained as a multitask and multilabel model, following a new architectural principle in end-to-end learning named "hierarchical composition": shallow layers extract a shared representation to predict a root taxon, while deeper layers specialize recursively to lower-rank taxa. In this way, TaxoNet is capable of handling taxonomic uncertainty, out-of-vocabulary labels, and open-set deployment settings. An experimental benchmark on two new bioacoustic datasets (ANAFCC and BirdVox-14SD) leads to state-of-the-art results in bird species classification. Furthermore, on a task of coarse-grained classification, TaxoNet also outperforms a flat single-task model trained on aggregate labels.},
keywords={Acoustic signal detection;audio databases;classification algorithms;multilayer neural network;phylogeny},
doi={10.1109/ICASSP40776.2020.9052908},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054234,
author={B. {Fan} and W. {Goodman} and R. Y. {Cho} and S. A. {Sheth} and R. R. {Bouchard} and B. {Aazhang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Beamforming Design for High-Resolution Low-Intensity Focused Ultrasound Neuromodulation},
year={2020},
volume={},
number={},
pages={906-910},
abstract={Low-intensity focused ultrasound (LIFU) has been shown to modulate neural activity. Recent experiments suggest potential applications of LIFU stimulation for treating neuropsychiatric disorders like depression and Alzheimer’s. The modulation effect is usually positively correlated with the ultrasound intensity, and there exists a minimum intensity threshold for the neuromodulation to be effective. Therefore, precise configuring of the ultrasound transducer is required to sonicate the target brain region at the desired intensity with appropriate spatial resolution. In this study, we investigate the optimization of targeting through fine temporal and spatial power delivery control of a phased array of ultrasound elements. A novel metric of the ultrasound neuromodulation resolution is proposed, and an optimization problem is formulated and solved to minimize side effects in the form of off-target region sonications. Simulation results show that our method is able to significantly improve the focusing resolution compared to the benchmark and reduce the volume experiencing possible off-target neuromodulation.},
keywords={Ultrasound neuromodulation;beamforming},
doi={10.1109/ICASSP40776.2020.9054234},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053366,
author={X. {Dong} and D. S. {Williamson}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Attention Enhanced Multi-Task Model for Objective Speech Assessment in Real-World Environments},
year={2020},
volume={},
number={},
pages={911-915},
abstract={Computational objective metrics that use reference signals have been shown to be effective forms of speech assessment in simulated environments, since they are correlated with subjective listening studies. Recent efforts have been dedicated towards effective forms of reference-less assessment to make real-world assessment more practical, but these approaches predict a limited number of assessment measures and they have not been evaluated in real-world conditions. In this work, we present a novel reference-less based framework called the attention enhanced multi-task speech assessment (AMSA) model, which provides reliable estimates of multiple objective quality and intelligibility measures in simulated and real-world environments. The multi-task learning (MTL) architecture effectively generates discriminative features that assist in improving our model’s robustness. An attention mechanism is employed to identify key features within the feature space, and it noticeably reduces the estimation errors. A classification-aided module is also included to further suppress prediction outliers. Our model achieves the state-of-the-art performance in simulated and real-world data environments, where the results are strongly correlated with the corresponding reference-based objective scores.},
keywords={speech quality and intelligibility;objective metrics;multi-task learning;attention networks;neural networks},
doi={10.1109/ICASSP40776.2020.9053366},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053141,
author={I. {Kiskin} and A. D. {Cobb} and L. {Wang} and S. {Roberts}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Humbug Zooniverse: A Crowd-Sourced Acoustic Mosquito Dataset},
year={2020},
volume={},
number={},
pages={916-920},
abstract={Mosquitoes are the only known vector of malaria, which leads to hundreds of thousands of deaths each year. Understanding the number and location of potential mosquito vectors is of paramount importance to aid the reduction of malaria transmission cases. In recent years, deep learning has become widely used for bioacoustic classification tasks. In order to enable further research applications in this field, we release a new dataset of mosquito audio recordings. With over a thousand contributors, we obtained 195,434 labels of two second duration, of which approximately 10 percent signify mosquito events. We present an example use of the dataset, in which we train a convolutional neural network on log-Mel features, showcasing the information content of the labels. We hope this will become a vital resource for those researching all aspects of malaria, and add to the existing audio datasets for bioacoustic detection and signal processing.},
keywords={Citizen science;dataset;CNN;classification;bioacoustics},
doi={10.1109/ICASSP40776.2020.9053141},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053960,
author={S. {Kurihara} and M. {Fukui} and S. {Shimauchi} and N. {Harada}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Subjective Quality Estimation Using PESQ For Hands-Free Terminals},
year={2020},
volume={},
number={},
pages={921-925},
abstract={Previous reports have mentioned the possibility that subjective quality of the echo-suppressed speech signal can be estimated based on perceptual evaluation of speech quality (PESQ), but there are few experimental results. We propose third-party listening and conversational test procedures to assess whether PESQ can be used for predicting the subjective quality of an acoustic echo canceler. In the proposed third-party listening test procedure, near-end and far-end signals are presented separately in the left and right channels of stereo playback and differential category rating evaluation is applied to those stimuli for obtaining differential mean opinion scores. In the proposed conversational test procedure, impaired and non-impaired reference signals are recorded during a conversation to make PESQ processing possible. Experimental results indicate that there is a strong correlation between PESQ and subjective scores.},
keywords={Perceptual Evaluation of Speech Quality (PESQ);Subjective quality estimation of hands-free terminals;Third-party tests;Conversational tests},
doi={10.1109/ICASSP40776.2020.9053960},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052948,
author={X. {Zhao} and J. {Solé-Casals} and B. {Li} and Z. {Huang} and A. {Wang} and J. {Cao} and T. {Tanaka} and Q. {Zhao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Classification of Epileptic IEEG Signals by CNN and Data Augmentation},
year={2020},
volume={},
number={},
pages={926-930},
abstract={Epileptic focus localization in patients with epileptic seizures is essential when surgery is needed. Recent studies show that this can be done automatically using machine learning approaches. However, well-designed feature extraction methods are often computationally demanding, requiring a large amount of data labeled by physicians, which is time consuming and impractical. In this paper, we firstly introduce a one-dimensional convolutional neural network (1D-CNN) model for epileptic seizure focus detection which avoids the manual, time-consuming feature extraction Moreover, to reduce the necessary number of training samples, we introduce an approach for data augmentation. The experimental results demonstrate the efficiency of the proposed method, with a nearly 3% improvement in performance using the data enhancement method compared to the best result obtained using the traditional feature extraction method.},
keywords={Epilepsy;convolutional neural network;data augmentation;cosine transform;artificial iEEG data},
doi={10.1109/ICASSP40776.2020.9052948},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052939,
author={T. {Yaqoob} and S. {Aziz} and S. {Ahmed} and O. {Amin} and M. {Alouini}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fractional Fourier Transform Based QRS Complex Detection in ECG Signal},
year={2020},
volume={},
number={},
pages={931-935},
abstract={By exploiting fractional-Fourier-transform (FrFT), a novel technique for the QRS complex detection is proposed. The application of the FrFT rotates the Electrocardiograph (ECG) signal in the time-frequency plane. We claim this rotation can give simple and effective QRS complex detection even in the presence of versatile artifacts, such as left-bundle-branch-block, right-bundle-branch-block, and negative polarization. In this work, in the first step, the noise and baseline drifts are removed by applying a wavelet transform on the given ECG signal. While, in the next step, the clean ECG signal is passed through the proposed algorithm, which rotates the ECG signal in the time-frequency plane and detects the QRS complex very easily. The proposed algorithm validated over the 48 signals of the MIT-BIH arrhythmia database, and it yielded 26 false-positive and only five false-negatives compared to the 80 and 42, the best result reported so far.},
keywords={ECG;QRS Complex Detection;MIT-BIH Arrhythmia Database;Sensitivity (Se(%));Positive Predictivity (+Pr)},
doi={10.1109/ICASSP40776.2020.9052939},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054242,
author={X. {Tian} and Q. {Zhu} and Y. {Li} and M. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cross-Domain Joint Dictionary Learning for ECG Reconstruction from PPG},
year={2020},
volume={},
number={},
pages={936-940},
abstract={An emerging research direction considers the inverse problem of inferring electrocardiogram (ECG) from photoplethysmogram (PPG) to bring about the synergy between the easy measurability of PPG and the rich clinical knowledge of ECG to facilitate preventive healthcare. Previous reconstruction using a universal basis has limited accuracy due to the lack of rich representative power. This paper proposes a cross-domain joint dictionary learning (XDJDL) framework to maximize the expressive power for the two cross-domain signals. Building on K-SVD technique, XDJDL optimizes simultaneously the PPG and ECG signal representations and the transform between them, enabling the joint learning of a pair of signal dictionaries with a transform to characterize the relation between their sparse codes. The proposed model is evaluated with 34,000+ ECG/PPG cycle pairs containing a variety of ECG morphologies and cardiovascular diseases. Experimental results validate the accuracy and the generality of the proposed algorithm, suggesting an encouraging potential for disease screening using PPG measurement based on the proactive learned PPG-ECG relationship.},
keywords={ECG;PPG;inverse problem;dictionary learning;sparse coding},
doi={10.1109/ICASSP40776.2020.9054242},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054000,
author={M. J. {Monesi} and B. {Accou} and J. {Montoya-Martinez} and T. {Francart} and H. V. {Hamme}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An LSTM Based Architecture to Relate Speech Stimulus to Eeg},
year={2020},
volume={},
number={},
pages={941-945},
abstract={Modeling the relationship between natural speech and a recorded electroencephalogram (EEG) helps us understand how the brain processes speech and has various applications in neuroscience and brain-computer interfaces. In this context, so far mainly linear models have been used. However, the decoding performance of the linear model is limited due to the complex and highly non-linear nature of the auditory processing in the human brain. We present a novel Long Short-Term Memory (LSTM)-based architecture as a nonlinear model for the classification problem of whether a given pair of (EEG, speech envelope) correspond to each other or not. The model maps short segments of the EEG and the envelope to a common embedding space using a CNN in the EEG path and an LSTM in the speech path. The latter also compensates for the brain response delay. In addition, we use transfer learning to fine-tune the model for each subject. The mean classification accuracy of the proposed model reaches 85%, which is significantly higher than that of a state of the art Convolutional Neural Network (CNN)-based model (73%) and the linear model (69%).},
keywords={LSTM;CNN;speech decoding;auditory system;EEG},
doi={10.1109/ICASSP40776.2020.9054000},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053721,
author={Y. {Peng} and Q. {Li} and W. {Kong} and J. {Zhang} and B. {Lu} and A. {Cichocki}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Joint Semi-Supervised Feature Auto-Weighting and Classification Model for EEG-Based Cross-Subject Sleep Quality Evaluation},
year={2020},
volume={},
number={},
pages={946-950},
abstract={Measuring the sleep quality is important or even crucial for people who are engaged in dangerous jobs such as the high-speed train drivers. Since the scalp EEG data are generated by the neural activities of the brain cortex, it is collected from subjects with different hours of sleep time (4 hours, 6 hours and 8 hours) to conduct sleep quality evaluation. To suppress the cross-subject variances of EEG data, in this paper, we propose a joint feature auto-weighting and semi-supervised classification model, termed GRLSR, which is formulated by introducing an auto-weighting variable into the least square regression to adaptively and quantitatively measure the importance of each dimension of the feature. Once the model is solved, besides the measurement results, we can use the auto-weighting variable to 1) analyze the importance of each frequency band in sleep quality expression and 2) identify the capacity of different channels connecting to the sleep effect. Therefore, the proposed GRLSR is a pure data-driven computing model for EEG-based cross-subject sleep quality evaluation. Experimental results show its effectiveness.},
keywords={Sleep quality evaluation;EEG;Feature auto-weighting;Semi-supervised learning;Classification},
doi={10.1109/ICASSP40776.2020.9053721},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053307,
author={Z. {Cao} and J. {Shi} and J. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Reversal No Longer Matters: Attention-Based Arrhythmia Detection with Lead-Reversal ECG Data},
year={2020},
volume={},
number={},
pages={951-955},
abstract={In this paper, we propose an attention-based multi-scale neural network for arrhythmia detection with lead-reversal electrocardiogram data. Electrocardiogram with a set of 12 waveforms(known as 12-lead ECG) measures myocardial electro-physiological activity, which is important in clinical diagnosis of arrhythmia. However, lead reversals caused by electrode interchange may cause great interference to the interpretation, leading to significant accuracy decline of automated algorithms and possible faulty diagnosis by cardiologists. To address this problem, we design a multi-scale neural network using attention mechanism to reduce the influence of lead reversals. The proposed model is evaluated on a dataset which consists of ECG data from 3,658 patients. In experiments, the proposed method shows high performance on both lead-reversal data and normal data, which proves great robustness of the method.},
keywords={arrhythmia detection;lead reversal;attention mechanism},
doi={10.1109/ICASSP40776.2020.9053307},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053425,
author={D. d. {Marchi} and A. {Budhiraja}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Augmenting Molecular Images with Vector Representations as a Featurization Technique for Drug Classification},
year={2020},
volume={},
number={},
pages={956-960},
abstract={One of the key steps in building deep learning systems for drug classification and generation is the choice of featurization for the molecules. Previous featurization methods have included molecular images, binary strings, graphs, and SMILES strings. This paper proposes the creation of molecular images "captioned" with binary vectors that encode information not contained in or easily understood from a molecular image alone. Specifically, we use Morgan fingerprints, which encode higher level structural information, and MACCS keys, which encode yes/no questions about a molecule’s properties and structure. We tested our method on the HIV dataset published by the Pande lab, which consists of 41,127 molecules labeled by if they inhibit the HIV virus. Our final model achieved a state-of-the-art AUC-ROC on the HIV dataset, outperforming all other methods. Moreover, the model converged significantly faster than most other methods, requiring dramatically less computational power than unaugmented images.},
keywords={Molecular Featurization;Drug Discovery;HIV;Image Classification;Convolutional Networks},
doi={10.1109/ICASSP40776.2020.9053425},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053551,
author={Á. S. {Hervella} and L. {Ramos} and J. {Rouco} and J. {Novo} and M. {Ortega}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Modal Self-Supervised Pre-Training for Joint Optic Disc and Cup Segmentation in Eye Fundus Images},
year={2020},
volume={},
number={},
pages={961-965},
abstract={This paper presents a novel approach for the segmentation of the optic disc and cup in eye fundus images using deep learning. The accurate segmentation of these anatomical structures in the eye is important towards the early detection of glaucoma and, therefore, potentially avoiding severe vision loss. In order to improve the segmentation of the optic disc and cup, we propose a novel self-supervised pretraining consisting in the multi-modal reconstruction of eye fundus images. This novel approach aims at facilitating the segmentation task and avoiding the necessity of excessively large annotated datasets.To validate the proposal, we perform several experiments on different public datasets. The results show that the proposed multi-modal self-supervised pre-training leads to a significant improvement in the performance of the segmentation task. Consequently, the presented approach shows remarkable potential towards further improving the interpretable and early diagnosis of a relevant disease as is glaucoma.},
keywords={Deep learning;self-supervised learning;segmentation;eye fundus;glaucoma},
doi={10.1109/ICASSP40776.2020.9053551},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054576,
author={A. {Salomon} and C. A. {Valades-Cruz} and L. {Leconte} and C. {Kervrann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Dense Mapping of Intracellular Diffusion and Drift from Single-Particle Tracking Data Analysis},
year={2020},
volume={},
number={},
pages={966-970},
abstract={It is of primary interest for biologists to be able to visualize the dynamics of proteins within the cell. In this paper, we propose a new mapping method to robustly estimate dynamics in the entire cell from particle tracks. To obtain satisfying diffusion and drift maps, we use a spatiotemporal kernel estimator. Trajectory classification data is used as input and allows to automatically label particle movements into three classes: confined motion (or subdiffusion), Brownian motion, and directed motion (or superdiffusion). We then use this information to calculate diffusion coefficient and drift maps separately on each class of motion.},
keywords={Diffusion;drift;mapping;classification;single-particle tracking},
doi={10.1109/ICASSP40776.2020.9054576},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053617,
author={Q. {Liu} and B. {Zou} and Y. {Zhao} and Y. {Liang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Deep Gradient Boosting Network for Optic Disc and Cup Segmentation},
year={2020},
volume={},
number={},
pages={971-975},
abstract={Segmentation of optic disc (OD) and optic cup (OC) is critical in automated fundus image analysis system. Existing state-of-the-arts focus on designing deep neural networks with one or multiple dense prediction branches. Such kind of designs ignore connections among prediction branches and their learning capacity is limited. To build connections among prediction branches, this paper introduces gradient boosting framework to deep classification model and proposes a gradient boosting network called BoostNet. Specifically, deformable side-output unit and aggregation unit with deep supervisions are proposed to learn base functions and expansion coefficients in gradient boosting framework. By stacking aggregation units in a deep-to-shallow manner, models’ performances are gradually boosted along deep to shallow stages. BoostNet achieves superior results to existing deep OD and OC segmentation networks on the public dataset ORIGA.},
keywords={Fundus image;OD and OC segmentation;gradient boosting;deep supervision},
doi={10.1109/ICASSP40776.2020.9053617},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053232,
author={Z. {Meng} and Z. {Zhao} and F. {Su} and W. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adaptive Elastic Loss Based on Progressive Inter-Class Association for Cervical Histology Image Segmentation},
year={2020},
volume={},
number={},
pages={976-980},
abstract={Cervical cancer is one of the most commonly diagnosed cancer types worldwide, while is curable if detected early. However, few computer-aided algorithms have been explored on cervical histology image, which is vital for abnormality assessment. In this paper, an end-to-end deep segmentation network for complex cervical histology images is proposed, and a benchmark evaluation is contributed. Specifically, we observe that four-category cervical histology images possess a progressive inter-class association. To model the relationship, inspired by the elasticity, an adaptive elastic loss is proposed to reduce the deviation between difficult samples and their true categories. Moreover, five evaluation metrics are designed to measure the segmentation performance, and the Window Precision is particularly valuable for the evaluation of semi-supervised algorithms due to its robustness to the mislabeling. Finally, on a cervical histology dataset, benchmark experiments based on deep networks are conducted, and the results demonstrate the superiority of our new loss.},
keywords={Cervical Cancer;Histology Image;Progressive Inter-class Association;Adaptive Elastic Loss},
doi={10.1109/ICASSP40776.2020.9053232},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054367,
author={M. {Yan} and Q. {Liu} and Z. {Yin} and D. {Wang} and Y. {Liang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Bidirectional Context Propagation Network for Urine Sediment Particle Detection in Microscopic Images},
year={2020},
volume={},
number={},
pages={981-985},
abstract={The microscopic urine sediment examination is a crucial part in the evaluation of renal and urinary tract diseases. Recently, there are emerging CNNs-based detectors to detect the urine sediment particles in an end-to-end manner. However, it is not very compatible to transfer CNNs-based detector directly from natural images application to microscopic images, especially in which small objects are in majority. This paper proposes a bidirectional context propagation network called BCPNet for urine sediment particle detection. In BCPNet, spatial details encoded by shallow convolutional layers are propagated upward to improve the localisation ability of deep features. On the contrary, high semantic information encoded by deep convolutional layers is propagated downward to enhance the distinctiveness of shallow features. With the refinement by convolutional block attention modules, the enriched features are more powerful to both localisation and classification. Experimental results on urine sediment particle dataset USE demonstrate effectiveness of the proposed BCPNet.},
keywords={Bidirectional propagation;feature pyramid network;urine sediment examination},
doi={10.1109/ICASSP40776.2020.9054367},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053946,
author={R. H. {Vareto} and A. {Marcia Saldanha} and W. R. {Schwartz}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={The Swax Benchmark: Attacking Biometric Systems with Wax Figures},
year={2020},
volume={},
number={},
pages={986-990},
abstract={A face spoofing attack occurs when an intruder attempts to impersonate someone who carries a gainful authentication clearance. It is a trending topic due to the increasing demand for biometric authentication on mobile devices, high-security areas, among others. This work introduces a new database named Sense Wax Attack dataset (SWAX), comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. Experiments performed with baseline methods show that despite the progress in recent years, advanced spoofing methods are still vulnerable to high-quality violation attempts.},
keywords={spoofing;presentation attack;face;wax figures},
doi={10.1109/ICASSP40776.2020.9053946},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054351,
author={M. K. {Ma} and T. {Lee} and M. C. {Fong} and W. {Shiyuan Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Resting-State EEG-Based Biometrics with Signals Features Extracted by Multivariate Empirical Mode Decomposition},
year={2020},
volume={},
number={},
pages={991-995},
abstract={EEG-based biometrics has gained great attention in recent years due to its superiority over traditional biometrics in terms of its resistance to circumvention. While there are numerous choices of data acquisition protocol, the present study is carried out with the least demanding resting-state condition. Motivated by neurophysiological knowledge, a type of novel feature, namely the intrinsic mode correlation (IMCOR), is proposed. It is designed by combining the nonstationary multivariate empirical mode decomposition (NA-MEMD) and the concept of brain connectivity. With machine learning classifiers, our system yields promising performance in a 81-class classification (F1 score: 0.99) within a single session. For 32-class cross-session classification, an F1 score of 0.55 is attained. The results suggest that the proposed method might be vulnerable to temporal effects and between-session variability. This study highlights the uniqueness of the proposed non-stationary and connectivity-based feature and demonstrated its success as a biometrics. Further investigation is needed to make the method practically useful.},
keywords={Biometrics;resting-state EEG;feature extraction;multivariate empirical mode decomposition;connectivity},
doi={10.1109/ICASSP40776.2020.9054351},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053587,
author={Z. {Yu} and Y. {Qin} and X. {Xu} and C. {Zhao} and Z. {Wang} and Z. {Lei} and G. {Zhao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Auto-Fas: Searching Lightweight Networks for Face Anti-Spoofing},
year={2020},
volume={},
number={},
pages={996-1000},
abstract={With the development of mobile devices, it is hopeful and pressing to deploy face recognition and face anti-spoofing (FAS) model on cell phone or portable devices. Most of existing face anti-spoofing methods focus on building computational costly detector for better spoofing face detection performance. However, these detectors are unfriendly to be deployed on the mobile device for real-time FAS applications. In this paper, we propose a neural architecture search (NAS) based method called Auto-FAS, intending to discover well-suitable lightweight networks for mobile-level face anti-spoofing. In Auto-FAS, a special search space is designed to restrict the model’s size, and pixel-wise binary supervision is used to improve the model’s performance. We demonstrate both the effectiveness and efficiency of the proposed approach on three public benchmark datasets, which shows the potential real-time FAS application for mobile devices.},
keywords={Face anti-spoofing;mobile;neural architecture search},
doi={10.1109/ICASSP40776.2020.9053587},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053685,
author={A. {Mohammadi} and S. {Bhattacharjee} and S. {Marcel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Domain Adaptation for Generalization of Face Presentation Attack Detection in Mobile Settengs with Minimal Information},
year={2020},
volume={},
number={},
pages={1001-1005},
abstract={With face-recognition (FR) increasingly replacing fingerprint sensors for user-authentication on mobile devices, presentation attacks (PA) have emerged as the single most significant hurdle for manufacturers of FR systems. Current machine-learning based presentation attack detection (PAD) systems, trained in a data-driven fashion, show excellent performance when evaluated in intra-dataset scenarios. Their performance typically degrades significantly in cross-dataset evaluations. This lack of generalization in current PAD systems makes them unsuitable for deployment in real-world scenarios. Considering each dataset as representing a different domain, domain adaptation techniques have been proposed as a solution to this generalization problem. Here, we propose a novel one class domain adaptation method which uses domain guided pruning to adapt a pre-trained PAD network to the target dataset. The proposed method works without the need of collecting PAs in the target domain (i.e., with minimal information in the target domain). Experimental results on several datasets show promising performance improvements in cross-dataset evaluations.1},
keywords={presentation attack detection;domain adaptation;domain generalization;pruning;feature selection},
doi={10.1109/ICASSP40776.2020.9053685},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054353,
author={C. {Wang} and Y. {Wang} and B. {Xu} and Y. {He} and Z. {Dong} and Z. {Sun}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Lightweight Multi-Label Segmentation Network for Mobile Iris Biometrics},
year={2020},
volume={},
number={},
pages={1006-1010},
abstract={This paper proposes a novel, lightweight deep convolutional neural network specifically designed for iris segmentation of noisy images acquired by mobile devices. Unlike previous studies, which only focused on improving the accuracy of segmentation mask using the popular CNN technology, our method is a complete end-to-end iris segmentation solution, i.e., segmentation mask and parameterized pupillary and limbic boundaries of the iris are obtained simultaneously, which further enables CNN-based iris segmentation to be applied in any regular iris recognition systems. By introducing an intermediate pictorial boundary representation, predictions of iris boundaries and segmentation mask have collectively formed a multi-label semantic segmentation problem, which could be well solved by a carefully adapted stacked hourglass network. Experimental results show that our method achieves competitive or state-of-the-art performance in both iris segmentation and localization on two challenging mobile iris databases.},
keywords={iris segmentation;iris localization;mobile iris biometrics;multi-label learning;stacked hourglass network},
doi={10.1109/ICASSP40776.2020.9054353},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054493,
author={T. {Feng} and S. S. {Narayanan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Modeling Behavioral Consistency in Large-Scale Wearable Recordings of Human Bio-Behavioral Signals},
year={2020},
volume={},
number={},
pages={1011-1015},
abstract={Continuously-worn wearable sensors provide an unprecedented opportunity to unobtrusively measure rich bio-behavioral time-series recordings in natural settings such as the workplace. These time-series data can be helpful in inferring broad patterns of behavior such as common routines and daily stress. Many existing approaches either rely on rigid pre-defined notions of activities or use sensitive contextual measurements, such as GPS location or localization within the home, that present privacy concerns and measurement challenges. In this work, we introduce a novel data processing pipeline to model behavioral consistency in a large real-world wearable recording data-set collected in a hospital workplace setting from nurses and direct clinical providers for a period of ten weeks. We use a non-parametric clustering method to generate time series clusters and capture behavioral consistency via the activity curve model. We evaluate the behavioral consistency model under different work roles and conditions such as between different groups of nursing professions and day versus night shift individuals. We also demonstrate that the learned behavioral consistency feature can assist in predicting self-reported work behaviors and anxiety levels.},
keywords={Wearable;Data Clustering;Routine Analysis;Machine Learning.},
doi={10.1109/ICASSP40776.2020.9054493},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054307,
author={T. {Feng} and B. M. {Booth} and S. S. {Narayanan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Modeling Behavior as Mutual Dependency between Physiological Signals and Indoor Location in Large-Scale Wearable Sensor Study},
year={2020},
volume={},
number={},
pages={1016-1020},
abstract={Wearable sensors today can unobtrusively collect rich time-series of physiological states and human movement patterns over a prolonged period. Gaining a better understanding of how an individual’s physiological responses vary in different workplace environments can be valuable in understanding human behavior related to wellness and performance. In this work, we describe our exploration in discovering the correlation between one’s physiological responses and movement patterns within different indoor locations using data collected from nurses in a hospital workplace for a ten week period. In this work, we use simple heuristics to empirically validate the idea that such a relationship may exist and then quantify it using mutual information analysis. We propose and demonstrate a data analysis approach that can also detect variations in the level of mutual dependency between different locations and physiological responses. The mutual dependency measures derived from our method are empirically shown to provide valuable information for improving modeling of self-reported work behavior patterns compared to using features derived from a single data stream.},
keywords={Wearable;Data Clustering;Data Segmentation;Mutual Dependency Analysis},
doi={10.1109/ICASSP40776.2020.9054307},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054144,
author={A. {Haboub} and H. {Baali} and A. {Bouzerdoum}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multichannel Signal Classification Using Vector Autoregression},
year={2020},
volume={},
number={},
pages={1021-1025},
abstract={The analysis of multichannel signals (MCS) has received a great deal of attention in the past few years. Modeling MCS requires depicting not only the temporal correlations within each single-channel signal (SCS) but also the interdependencies between marginal signals. The vector autoregressive (VAR) model is well adapted to providing insights to these ubiquitous dependencies, which is why it has been widely adopted for forecasting and analyzing impulse responses. Despite that, only a few studies have employed the VAR model for classification. To further explore this area, we propose a simple yet effective approach based on modeling MCS with a VAR process. To demonstrate the performance of our approach, we test it on real EEG recordings to discriminate between control and alcoholic subjects. Experimental results show that the proposed VAR approach can be very effective in MCS classification; it achieves competitive results on the benchmark dataset compared to existing state-of-the-art techniques.},
keywords={multichannel signal;vector autoregressive model;classification},
doi={10.1109/ICASSP40776.2020.9054144},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053314,
author={M. {Saeed} and C. C. {Took} and S. R. {Alty}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Efficient Algorithm to Implement Sliding Singular Spectrum Analysis with Application to Biomedical Signal Denoising},
year={2020},
volume={},
number={},
pages={1026-1029},
abstract={Previous work [1] has shown that Singular Spectrum Analysis (SSA) can be particularly effective at noise removal or signal separation in the case of single channel mixtures. The work presented here shows how the sliding or updating algorithm which performs best at signal separation can be implemented in a computationally efficient manner. The main computational burden involved in SSA is the evaluation of a full rank matrix Singular Value Decomposition (SVD). This process is well understood to be of $\mathcal{O}\left( {{n^3}} \right)$ where n is the rank of the matrix. Therefore, operation of the SSA algorithm in a sliding manner (once per new data sample) involves a very heavy computational cost. In this paper, we show it is possible to evaluate the rank-1 SVD update efficiently in $\mathcal{O}\left( {{n^2}} \right)$, thus dramatically increasing the speed of the sliding version of the SSA algorithm. Further, we demonstrate that our proposed sliding SSA can be particularly effective at removing ECG from EMG signals in an under-determined setting.},
keywords={Singular Value Decomposition;Singular Spectrum Analysis;Sliding Window},
doi={10.1109/ICASSP40776.2020.9053314},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053515,
author={J. {Martinez} and A. {Akbari} and K. {Sel} and R. {Jafari}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Strategic Attention Learning for Modality Translation},
year={2020},
volume={},
number={},
pages={1030-1034},
abstract={Novel wearable sensor modalities, such as bio-impedance (Bio-Z), are being introduced and often provide various advantages over current state-of-the-art in terms of accuracy, sensing coverage, or convenience of wear. The principal challenge, however, lies in the ability to interpret the sensor reading by healthcare providers. In this work, we propose a two-stage deep learning framework that leverages a novel attention mechanism to translate Bio-Z signals to highly interpretable electrocardiogram (ECG) waveforms while also predicting translation uncertainty. Our experiments indicate a 66% improvement in accuracy for 1D-CNN based models to perform competitively with more sophisticated hybrid CNN-LSTM based models in a fraction of the training time while also providing a valid uncertainty measurement.},
keywords={modality translation;sequence-to-sequence;deep learning;uncertainty quantification},
doi={10.1109/ICASSP40776.2020.9053515},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054526,
author={A. {Jiang} and J. {Shang} and W. {Cheng} and X. {Liu} and H. K. {Kwan} and Y. {Zhu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sparse CSP Algorithm via Joint Spatio-Temporal Filtering},
year={2020},
volume={},
number={},
pages={1035-1039},
abstract={Common spatial pattern (CSP) is widely used in motor imagery classification tasks. Classical CSP depends only on spatial filters. To improve its performance, a novel and efficient spatio-temporal filtering strategy is proposed in this paper to extract discriminative features. Common temporal filters are shared among all the spatial channels, so as to reduce the overfitting risk in the case of a small sample size. An efficient alternating optimization algorithm is also developed to optimize coefficients of spatial and temporal filters. To alleviate adverse effects of noise and artifacts and improve implementation efficiency, an ℓ1-norm-based sparsity regularization term is further introduced. The resulting problem is tackled by the reweighting technique. The effectiveness of the proposed algorithm is validated by the experiments using open datasets of BCI Competition.},
keywords={Alternating optimization;common spatial pattern (CSP);electroencephalograph (EEG);reweighting technique;sparsity;spatio-temporal filter.},
doi={10.1109/ICASSP40776.2020.9054526},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053555,
author={M. {Ravanbakhsh} and V. {Tschernezki} and F. {Last} and T. {Klein} and K. {Batmanghelich} and V. {Tresp} and M. {Nabi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Human-Machine Collaboration for Medical Image Segmentation},
year={2020},
volume={},
number={},
pages={1040-1044},
abstract={Image segmentation is a ubiquitous step in almost any medical image study. Deep learning-based approaches achieve state-of-the-art in the majority of image segmentation benchmarks. However, end-to-end training of such models requires sufficient annotation. In this paper, we propose a method based on conditional Generative Adversarial Network (cGAN) to address segmentation in semi-supervised setup and in a human-in-the-loop fashion. More specifically, we use the generator in the GAN to synthesize segmentations on unlabeled data and use the discriminator to identify unreliable slices for which expert annotation is required. The quantitative results on a conventional standard benchmark show that our method is comparable with the state-of-the-art fully supervised methods in slice-level evaluation, despite of requiring far less annotated data.},
keywords={GANs;Human-Machine Collaboration},
doi={10.1109/ICASSP40776.2020.9053555},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054672,
author={P. {Mathur} and M. {Piplani} and R. {Sawhney} and A. {Jindal} and R. R. {Shah}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Mixup Multi-Attention Multi-Tasking Model for Early-Stage Leukemia Identification},
year={2020},
volume={},
number={},
pages={1045-1049},
abstract={Recently, several image processing and deep learning techniques have been applied to automate the detection of Acute Lymphoblastic Leukemia cells (ALL). However, most of them have consistently focused on classification mature stage cell images into binary categories of ALL or normal cells. The real impetus of biomedical imaging lies in detecting early-stage cases since early-stage ALL cells have unintuitive global contextual and local spatial features, making their detection non-trivial. To this effect, we propose a novel architecture termed as Mixup Multi-Attention Multi-Task Learning Model (MMA-MTL), which introduces Pointwise Attention Convolution Layers and Local Spatial Attention blocks to capture global and local features simultaneously. We also introduce Rademacher Paired Sampling Mixup to prevent memorization of training data in cases of limited categorical shift. Our proposed method shows competitive performance on the ISBI-2019 CNMC dataset and benchmarks appropriate design choices for future biomedical imaging tasks.},
keywords={Attention networks;multi-task learning;mixup data augmentation;biomedical image analysis},
doi={10.1109/ICASSP40776.2020.9054672},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054612,
author={X. {Zhao} and L. {Yu} and X. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cross-View Attention Network for Breast Cancer Screening from Multi-View Mammograms},
year={2020},
volume={},
number={},
pages={1050-1054},
abstract={In this paper, we address the problem of breast caner detection from multi-view mammograms. We present a novel cross-view attention module (CvAM) which implicitly learns to focus on the cancer-related local abnormal regions and highlighting salient features by exploring cross-view information among four views of a screening mammography exam, e.g. asymmetries between left and right breasts and lesion correspondence between two views of the same breast. More specifically, the proposed CvAM calculates spatial attention maps based on the same view of different breasts to enhance bilateral asymmetric regions, and channel attention maps based on two different views of the same breast to enhance the feature channels corresponding to the same lesion in a single breast. CvAMs can be easily integrated into standard convolutional neural networks (CNN) architectures such as ResNet to form a multi-view classification model. Experiments are conducted on DDSM dataset, and results show that CvAMs can not only provide better classification accuracy over non-attention and single-view attention models, but also demonstrate better abnormality localization power using CNN visualization tools.},
keywords={Mammogram Classification;Multi-view Learning;Attention Mechanism;Deep Learning},
doi={10.1109/ICASSP40776.2020.9054612},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053405,
author={H. {Huang} and L. {Lin} and R. {Tong} and H. {Hu} and Q. {Zhang} and Y. {Iwamoto} and X. {Han} and Y. {Chen} and J. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation},
year={2020},
volume={},
number={},
pages={1055-1059},
abstract={Recently, a growing interest has been seen in deep learning-based semantic segmentation. UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. In this paper, we propose a novel UNet 3+, which takes advantage of full-scale skip connections and deep supervisions. The full-scale skip connections incorporate low-level details with high-level semantics from feature maps in different scales; while the deep supervision learns hierarchical representations from the full-scale aggregated feature maps. The proposed method is especially benefiting for organs that appear at varying scales. In addition to accuracy improvements, the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency. We further propose a hybrid loss function and devise a classification-guided module to enhance the organ boundary and reduce the over-segmentation in a non-organ image, yielding more accurate segmentation results. The effectiveness of the proposed method is demonstrated on two datasets. The code is available at: github.com/ZJUGiveLab/UNet-Version},
keywords={Segmentation;Full-scale skip connection;Deep supervision;Hybrid loss function;Classification.},
doi={10.1109/ICASSP40776.2020.9053405},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053490,
author={R. {Xu} and Z. {Cong} and X. {Ye} and S. {Kido} and N. {Tomiyama}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unsupervised Content-Preserved Adaptation Network for Classification of Pulmonary Textures from Different CT Scanners},
year={2020},
volume={},
number={},
pages={1060-1064},
abstract={Deep network based methods have been proposed for accurate classification of pulmonary textures on CT images. However, such methods well-trained on CT data from one scanner cannot perform well when they are directly applied to the data from other scanners. This domain shift problem is caused by different physical components and scanning protocols of different CT scanners. In this paper, we propose an unsupervised content-preserved adaptation network to address this problem. Our method can make a previously well-trained deep network to be adapted for the data of a new CT scanner and does not require the laboring annotation to delineate pulmonary texture regions on the new CT data. Extensive evaluations have been carried on images collected from GE and Toshiba CT scanners and show that the proposed method can alleviate the performance degradation problem of classifying pulmonary textures from different CT scanners.},
keywords={Pulmonary Textures Classification;Unsupervised Domain Adaptation;Different CT Scanners},
doi={10.1109/ICASSP40776.2020.9053490},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054605,
author={Y. {Li} and D. {Gu} and Z. {Wen} and F. {Jiang} and S. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Classify and Explain: An Interpretable Convolutional Neural Network For Lung Cancer Diagnosis},
year={2020},
volume={},
number={},
pages={1065-1069},
abstract={The deep network-based computer-aided diagnosis systems have encountered many difficulties in practical applications because of its "black box" feature. The crux of the problem is that these models should be explainable - the model should provide doctors rationales that can explain the diagnosis. In this paper, we present a novel network structure for visually interpretable lung nodule diagnosis. Our proposed model works in an end-to-end manner, consisting of an importance estimation network and a classification network. The former produces a diagnostic visual interpretation for each case, and the latter diagnoses the case. Based on a computed tomography image dataset (LUNA16) on pulmonary nodule, extensive experiments have been conducted, demonstrating that the proposed model can produce state-of-the-art diagnostic visual interpretations compared with all baseline methods.},
keywords={Biomedical image analysis;deep learning;medical diagnostic imaging;lung cancer diagnosis},
doi={10.1109/ICASSP40776.2020.9054605},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054568,
author={Y. {Lv} and X. {Chen} and C. {Shu} and H. {Han}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Global Optimized Affine Registration Method for Microscopic Images of Biological Tissue},
year={2020},
volume={},
number={},
pages={1070-1074},
abstract={Affine registration can fit the non-rigid deformation of slices effectively, and it is widely used in volume reconstruction of biological tissue. But most of the existing affine registration methods are registered in a given sequence, which results in the accumulation of errors. In this paper, a global optimized affine registration method is proposed, which can be used in volume reconstruction. To eliminate the cumulative error, the affine transformation of all images is estimated simultaneously based on an energy function. A soft penalty on affine transformation is added to restrict the shearing of images. Experiments show that our method provides a more reliable registration result compared with sequential affine registration. It can solve the problems caused by the accumulation of errors. The registration result fits the deformation of slices well and preserves the rigidity of images.},
keywords={Volume reconstruction;registration;affine transformation},
doi={10.1109/ICASSP40776.2020.9054568},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054640,
author={S. {Prigent} and S. {Dutertre} and C. {Kervrann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Empirical Sure-Guided Microscopy Super-Resolution Image Reconstruction from Confocal Multi-Array Detectors},
year={2020},
volume={},
number={},
pages={1075-1079},
abstract={The new generation of confocal microscopes are equipped with an array detector that generates an array of images corresponding to a multiview of the same sample. Several computational methods have been proposed to reconstruct a single super-resolution image from a stack of images associated to detectors. Each method has its pros and cons depending on the targeted application. In this paper, we review the most commonly used reconstruction methods and propose a SURE approach to automatically estimate parameters and improve reconstruction. Methods described in this paper are available in an open source software.},
keywords={Super-resolution;image reconstruction;confocal imaging;SURE;estimation},
doi={10.1109/ICASSP40776.2020.9054640},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054375,
author={W. C. {de Meto} and E. {Granger} and M. B. {Lopez}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Encoding Temporal Information For Automatic Depression Recognition From Facial Analysis},
year={2020},
volume={},
number={},
pages={1080-1084},
abstract={Depression is a mental illness that may be harmful to an individual’s health. Using deep learning models to recognize the facial expressions of individuals captured in videos has shown promising results for automatic depression detection. Typically, depression levels are recognized using 2D-Convolutional Neural Networks (CNNs) that are trained to extract static features from video frames, which impairs the capture of dynamic spatio-temporal relations. As an alternative, 3D-CNNs may be employed to extract spatiotemporal features from short video clips, although the risk of overfitting increases due to the limited availability of labeled depression video data. To address these issues, we propose a novel temporal pooling method to capture and encode the spatio-temporal dynamic of video clips into an image map. This approach allows fine-tuning a pre-trained 2D CNN to model facial variations, and thereby improving the training process and model accuracy. Our proposed method is based on two-stream model that performs late fusion of appearance and dynamic information. Extensive experiments on two benchmark AVEC datasets indicate that the proposed method is efficient and outperforms the state-of-the-art schemes.},
keywords={Affective Computing;Depression Detection;Expression Recognition;Temporal Pooling;Two-stream Model},
doi={10.1109/ICASSP40776.2020.9054375},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052914,
author={R. {Xu} and X. {Ye} and G. {Jiang} and T. {Liu} and L. {Li} and S. {Tanaka}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Retinal Vessel Segmentation via a Semantics and Multi-Scale Aggregation Network},
year={2020},
volume={},
number={},
pages={1085-1089},
abstract={Precise segmentation of retinal vessels is crucial for a computer-aided diagnosis system of retinal fundus images. However, this task remains challenging due to large variations in scales and poor segmentation of capillary vessels. In this paper, we propose a semantics and multi-scale aggregation network to address these difficulties. It includes semantics aggregation blocks that are designed for aggregating stronger high-level semantic information. These carefully designed blocks produce more semantic feature representation that is helpful for capillary vessel identification and vessel connection. Besides, a multi-scale aggregation block is designed by employing parallel dilated convolutional filters with different dilation rates to fully exploit the multi-scale information. We evaluate the network by using two public databases of retinal vessel segmentation and compare its performance with several leading methods published in the past several years. Extensive evaluations show that the proposed network has achieved the state-of-the-art performance on the public CHASE DB1 and HRF datasets.},
keywords={Semantic Information;Multi-Scale;Retinal Vessel Segmentation},
doi={10.1109/ICASSP40776.2020.9052914},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053748,
author={A. M. {Rekavandi} and A. {Seghouane} and R. J. {Evans}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adaptive Matched Filter using Non-Target Free Training Data},
year={2020},
volume={},
number={},
pages={1090-1094},
abstract={The problem of detecting a subspace signal in colored Gaussian noise with unknown covariance matrix is investigated when the training data may contain samples with target signal. The target signal is assumed that it lies in a subspace spanned by columns of a known matrix. To develop the test, an ad hoc approach, similar to the classical adaptive matched filter (AMF) is used where instead of the maximum likelihood (ML) estimator of the covariance, the minimum α−divergence based estimator is substituted in the likelihood ratio. This test just depends on the single parameter α and as a special case can be turned to the AMF. For a range of α, the proposed test has the benefits of being robust to outliers and the existence of other targets in the training data. Numerical examples illustrating that the proposed detector can achieve better detection rates in such a scenario while providing almost the same performance in a target free scenario are presented.},
keywords={Adaptive Detection;matched filter;maximum likelihood and α−divergence},
doi={10.1109/ICASSP40776.2020.9053748},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054703,
author={J. {Dorazil} and R. {Repp} and T. {Kropfreiter} and R. {Prüller} and K. {Říha} and F. {Hlawatsch}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Feature Drift Resilient Tracking of The Carotid Artery Wall Using Unscented Kalman Filtering With Data Fusion},
year={2020},
volume={},
number={},
pages={1095-1099},
abstract={An analysis of the motion of the common carotid artery (CCA) provides effective indicators for cardiovascular diseases. Here, we propose a method for tracking CCA wall motion from a B-mode ultrasound video sequence. An unscented Kalman filter based on a suitably devised state-space model fuses measurements produced by an optical flow algorithm and a CCA wall localization algorithm. This approach compensates for feature drift, which is a detrimental effect in optical flow algorithms. The proposed method is demonstrated to outperform a state-of-the-art tracking method based on optical flow.},
keywords={Atherosclerosis;common carotid artery;B-mode ultrasound;unscented Kalman filter;data fusion},
doi={10.1109/ICASSP40776.2020.9054703},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053902,
author={M. {Roald} and S. {Bhinge} and C. {Jia} and V. {Calhoun} and T. {Adalı} and E. {Acar}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Tracing Network Evolution Using The Parafac2 Model},
year={2020},
volume={},
number={},
pages={1100-1104},
abstract={Characterizing time-evolving networks is a challenging task, but it is crucial for understanding the dynamic behavior of complex systems such as the brain. For instance, how spatial networks of functional connectivity in the brain evolve during a task is not well-understood. A traditional approach in neuroimaging data analysis is to make simplifications through the assumption of static spatial networks. In this paper, without assuming static networks in time and/or space, we arrange the temporal data as a higher-order tensor and use a tensor fac-torization model called PARAFAC2 to capture underlying patterns (spatial networks) in time-evolving data and their evolution. Numerical experiments on simulated data demonstrate that PARAFAC2 can successfully reveal the underlying networks and their dynamics. We also show the promising performance of the model in terms of tracing the evolution of task-related functional connectivity in the brain through the analysis of functional magnetic resonance imaging data.},
keywords={PARAFAC2;tensor factorizations;network evolution;dynamic networks;time-evolving data},
doi={10.1109/ICASSP40776.2020.9053902},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053131,
author={X. {Qiao} and J. {Du} and L. {Wang} and Z. {He} and Y. {Jia}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Model-Based Deep Network for MRI Reconstruction Using Approximate Message Passing Algorithm},
year={2020},
volume={},
number={},
pages={1105-1109},
abstract={We propose a novel model-based network to reconstruct the magnetic resonance (MR) image. In this network, the Approximate Message Passing (AMP) algorithm is unrolled to solve the optimization problem of compressed sensing MR imaging, and several CNN blocks is embedded as de-aliasing steps. We relax the restriction on the parameter selection of AMP algorithm, and enable the parameters trainable in our proposed method. Each CNN block and AMP block is followed by a data consistency (DC) operation, which can efficiently accelerate the convergence of the reconstruction network. The trainable parameters of our DC share the weights and can automatically adapt to the error pattern. Experimental results show that the proposed method obtains faster convergence speed and achieves a new state-of-the-art MR image reconstruction performance.},
keywords={Deep Learning;MRI Reconstruction;Approximate Message Passing},
doi={10.1109/ICASSP40776.2020.9053131},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053230,
author={Y. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Online Positron Emission Tomography By Online Portfolio Selection},
year={2020},
volume={},
number={},
pages={1110-1114},
abstract={The number of measurement outcomes in positron emission tomography (PET) is typically large, rendering signal reconstruction computationally expensive. We propose an online algorithm to address this computational issue. The per-iteration computational complexity of the proposed algorithm is independent of the number of measurement outcomes and linear√ in the signal dimension. The algorithm has a rigorous $O\left( {1/\sqrt k } \right)$ convergence rate guarantee, where k denotes the iteration counter. Numerical experiments on synthetic data-sets show that the algorithm can be significantly faster than expectation maximization and stochastic primal-dual hybrid gradient method. The proposed algorithm is based on an equivalent stochastic optimization formulation, the Soft-Bayes algorithm for online portfolio selection, and standard online-to-batch conversion.},
keywords={Positron emission tomography;stochastic optimization;online portfolio selection;Soft-Bayes;online-to-batch conversion},
doi={10.1109/ICASSP40776.2020.9053230},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054372,
author={S. {Sharma} and K. V. S. {Hari} and G. {Leus}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Space Filling Curves for MRI Sampling},
year={2020},
volume={},
number={},
pages={1115-1119},
abstract={A novel class of k-space trajectories for magnetic resonance imaging (MRI) sampling using space filling curves (SFCs) is presented here. More specifically, Peano, Hilbert and Sierpinski curves are used. We propose 1-shot and 4-shot variable density SFCs by utilizing the space coverage provided by SFCs in different iterations. The proposed trajectories are compared with state-of-the-art echo planar imaging (EPI) trajectories for 128 × 128 and 256 × 256 phantom and brain images. The simulation results show that the readout time is reduced by up to 45% for the 128 × 128 image with little compromise in reconstruction quality. Also, the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index are improved by 2.32 dB and 0.1009, respectively, with an 18% shorter readout time using the 4-shot Hilbert SFC trajectory for reconstructing a 256 × 256 brain MRI image.},
keywords={MRI;k-space trajectories;space filling curves},
doi={10.1109/ICASSP40776.2020.9054372},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054306,
author={S. {Sharma} and K. V. S. {Hari} and G. {Leus}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={K-Space Trajectory Design for Reduced MRI Scan Time},
year={2020},
volume={},
number={},
pages={1120-1124},
abstract={The development of compressed sensing (CS) techniques for magnetic resonance imaging (MRI) is enabling a speedup of MRI scanning. To increase the incoherence in the sampling, a random selection of points on the k-space is deployed and a continuous trajectory is obtained by solving a traveling salesman problem (TSP) through these points. A feasible trajectory satisfying the gradient constraints is then obtained by parameterizing it using state-of-the-art methods. In this paper, a constrained convex optimization based method to obtain feasible trajectories is proposed. The method is motivated by the fact that the readout time is proportional to the number of sample points and includes the lengths of the segments of the trajectory in the cost function to obtain variable length trajectories. The proposed method provides a reduction in readout time by more than 50% for random-like trajectories with an improvement of about 1.5 dB in peak signal-to-noise ratio (PSNR) and 0.0762 in structural similarity (SSIM) index on average for a realistic brain phantom MRI image adopting single-shot trajectories.},
keywords={MRI;k-space trajectories;compressed sensing},
doi={10.1109/ICASSP40776.2020.9054306},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053177,
author={S. R. {Maiya} and P. {Mathur}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Rethinking Retinal Landmark Localization as Pose Estimation: Naïve Single Stacked Network for Optic Disk and Fovea Detection},
year={2020},
volume={},
number={},
pages={1125-1129},
abstract={Automatic detection of optic disk and fovea, the two fundamental biological landmarks of the retinal system, is crucial to track the disease progression in a diabetic patient. Recent advances in this direction were mostly limited to applying CNN based networks to aggressively extract visual geometric features. In a departure from that practice, we put forward the notion of treating the landmark detection problem in human eye scans as a pose estimation problem owing to the anatomical geometrical relationship between optic disk and fovea. In this regard, we present Naive Single Stacked Hourglass (NSSH) network which learns the spatial orientation and pixel intensity contrast between optic disk and fovea to accurately pinpoint their locations. NSSH network significantly reduces the mean squared loss, thus outperforming all previously known techniques and establishing a state of the art in both optic disk and fovea localization tasks.},
keywords={Biomedical imaging;key-point detection;pose estimation;hourglass networks},
doi={10.1109/ICASSP40776.2020.9053177},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054635,
author={A. {Honoré} and D. {Liu} and D. {Forsberg} and K. {Coste} and E. {Herlenius} and S. {Chatterjee} and M. {Skoglund}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Hidden Markov Models for Sepsis Detection in Preterm Infants},
year={2020},
volume={},
number={},
pages={1130-1134},
abstract={We explore the use of traditional and contemporary hidden Markov models (HMMs) for sequential physiological data analysis and sepsis prediction in preterm infants. We investigate the use of classical Gaussian mixture model based HMM, and a recently proposed neural network based HMM. To improve the neural network based HMM, we propose a discriminative training approach. Experimental results show the potential of HMMs over logistic regression, support vector machine and extreme learning machine.},
keywords={Neonatal Sepsis;Hidden Markov Model},
doi={10.1109/ICASSP40776.2020.9054635},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053446,
author={O. {Schlesinger} and N. {Vigderhouse} and D. {Eytan} and Y. {Moshe}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Blood Pressure Estimation From PPG Signals Using Convolutional Neural Networks And Siamese Network},
year={2020},
volume={},
number={},
pages={1135-1139},
abstract={Blood pressure (BP) is a vital sign of the human body and an important parameter for early detection of cardiovascular diseases. It is usually measured using cuff-based devices or monitored invasively in critically-ill patients. This paper presents two techniques that enable continuous and noninvasive cuff-less BP estimation using photoplethysmography (PPG) signals with Convolutional Neural Networks (CNNs). The first technique is calibration-free. The second technique achieves a more accurate measurement by estimating BP changes with respect to a patient's PPG and ground truth BP values at calibration time. For this purpose, it uses Siamese network architecture. When trained and tested on the MIMIC-II database, it achieves mean absolute difference in the systolic and diastolic BP of 5.95 mmHg and 3.41 mmHg respectively. These results almost comply with the AAMI recommendation and are as accurate as the values estimated by many home BP measuring devices.},
keywords={Blood pressure;convolutional neural network (CNN);noninvasive;photoplethysmography (PPG);Siamese network},
doi={10.1109/ICASSP40776.2020.9053446},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053753,
author={V. S. {Nallanthighal} and A. {Härmä} and H. {Strik}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Speech Breathing Estimation Using Deep Learning Methods},
year={2020},
volume={},
number={},
pages={1140-1144},
abstract={Breathing is the primary mechanism for maintaining the subglottal pressure for speech production. Speech can be seen as a systematic outflow of air during exhalation characterized by linguistic content and prosodic factors. Thus, sensing respiratory dynamics from the speech is plausible. In this paper, we explore techniques for sensing breathing from speech using deep learning architectures including multi-task learning approaches. Estimating the breathing pattern from the speech would give us information about the respiration rate, breathing capacity and thus enable us to understand the pathological condition of a person using one’s speech. Training and evaluation of our model on our database of breathing signal and speech for 40 subjects yielded a sensitivity of 0.88 for breath event detection and 5.6 % error for breathing rate estimation.},
keywords={Speech breathing;signal processing;speech technology;deep neural networks;Multi task learning},
doi={10.1109/ICASSP40776.2020.9053753},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053469,
author={Z. {Mei} and Q. {Wu} and Z. {Hu} and J. {Tao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Fast Non-Contact Vital Signs Detection Method Based on Regional Hidden Markov Model in A 77ghz Lfmcw Radar System},
year={2020},
volume={},
number={},
pages={1145-1149},
abstract={The technologies of vital signs detection have been proven of great use while it is still limited by several challenges. One of the major challenges in vital signs detection is strong interferences, such as multiple targets in continuous wave radar system and random body movement (RBM), which significantly degrade the accuracy of the measurement. In this paper, a 77GHz linear frequency modulated continuous-wave (LFMCW) radar system is investigated to mitigate multiple-targets interferences. Furthermore, a novel regional hidden Markov model (RHMM) is proposed to acquire accurate estimates of the respiration rate (RR) and heart rate (HR) by exploiting the underlying slow-variant characteristics of these vital signs in the RBM environment. Experiments demonstrate the error rates of the proposed method are less than 9% for RR and less than 3% for HR in the multi-targets RBM environment.},
keywords={Non-contact vital signs detection;linear-frequency-modulated continuous-wave (LFMCW) radar;random body movement (RBM);hidden Markov model},
doi={10.1109/ICASSP40776.2020.9053469},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053881,
author={A. M. {Rekavandi} and A. {Seghouane} and R. J. {Evans}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Likelihood Ratio Test Using α−Divergence},
year={2020},
volume={},
number={},
pages={1150-1154},
abstract={The problem of detecting a subspace signal in the presence of subspace interference and contaminated Gaussian noise with unknown variance is investigated. The target signal is assumed to lie in a subspace spanned by the columns of a known matrix. To develop the test, the same steps used in the generalized likelihood ratio test (GLRT) are used where instead of the maximum likelihood (ML) estimator of the parameters, the minimum α−divergence based estimator is substituted in the test to increase the robustness of the test against contaminations in noise. This test depends on the single parameter α and as the special case corresponds to the well known GLRT. Numerical examples illustrating that the proposed test can achieve better detection rates in such scenarios are presented. Moreover, the test is applied to real fMRI dataset to detect the active area of the brain for some task-related inputs.},
keywords={Subspace Detection;maximum likelihood;fMRI data and α−divergence},
doi={10.1109/ICASSP40776.2020.9053881},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053770,
author={L. {Moro-Velazquez} and J. {Villalba} and N. {Dehak}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Using X-Vectors to Automatically Detect Parkinson’s Disease from Speech},
year={2020},
volume={},
number={},
pages={1155-1159},
abstract={The promise of new neuroprotective treatments to stop or slow the advance of Parkinson’s Disease (PD) urges for new biomarkers or detection schemes that can deliver a faster diagnosis. Given that speech is affected by PD, the combination of deep neural networks and speech processing can provide automatic detection schemes. Accordingly, in this study we analyze for the first time a new state-of-the-art speaker recognition technique, x-Vectors, in a different scenario: the automatic detection of PD from speech. The proposed approach is compared with another speaker recognition technique, i-Vectors, employed in previous works and used as baseline in this study. A corpus with 43 PD patients and 46 control speakers was used to evaluate the performance of these two techniques at two sampling frequencies: 8 and 16 kHz.The x-Vector approach provided the best results in terms of accuracy and AUC reaching values of 90% and 0.94, respectively. Consequently, results suggest that speaker embeddings obtained using deep neural networks are successful extracting acoustic information relative to patterns in articulation, prosody and/or phonation common in persons with PD.},
keywords={Parkinson’s Disease;speech;i-Vectors;x-Vectors},
doi={10.1109/ICASSP40776.2020.9053770},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054430,
author={P. {Manomaisaowapak} and J. {Songsiri}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning A Common Granger Causality Network Using A Non-Convex Regularization},
year={2020},
volume={},
number={},
pages={1160-1164},
abstract={This paper proposes an estimation for learning a common Granger network of panel data. When vector time series are collected from several subjects belonging to a homogeneous group, a relationship structure of variables can be assumed to share the same topology, while the model parameters of individual subjects could be varied. The formulation is a regularized least-squares estimation of multiple vector autoregressive (VAR) models with a non-convex ℓ2,1/2-norm penalty. The common sparsity pattern of VAR coefficients from all models can reveal a consistent network in a group level. Simulation results show that the proposed formulation achieves better accuracy of learning Granger networks than the convex group lasso formulation. Preliminary results of discovering brain connectivities from ABIDE fMRI data sets showed that sparse common networks among subjects from normal and autism groups are distinctive in some brain regions.},
keywords={brain connectivity;Granger causality;group sparse learning;non-convex penalty},
doi={10.1109/ICASSP40776.2020.9054430},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054570,
author={S. {Bhattacharya} and O. {Mazumder} and D. {Roy} and A. {Sinha} and A. {Ghose}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Synthetic Data Generation Through Statistical Explosion: Improving Classification Accuracy of Coronary Artery Disease Using PPG},
year={2020},
volume={},
number={},
pages={1165-1169},
abstract={Synthetic data generation has recently emerged as a substitution technique for handling the problem of bulk data needed in training machine learning algorithms. Healthcare, primarily cardiovascular domain is a major area where synthetic physiological data can be used improve accuracy of machine learning algorithm. This paper presents a novel approach of generating synthetic Photoplethysmogram (PPG) data using statistical explosion. Synthetic data is subsequently used to classify Coronary Artery Disease (CAD) using a two stage cascaded classifier. Proposed classifier along with synthetic data removes class bias and provides better accuracy compared to state of art. The proposed data generation and cascaded classifier is generic enough to be used to improve machine learning algorithm on any time series signal.},
keywords={Cascaded classifier;Coronary Artery Disease;Kernel Density Estimate;Photoplethysmogram;Synthetic data},
doi={10.1109/ICASSP40776.2020.9054570},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053391,
author={Y. {Tang} and X. {Li} and Y. {Chen} and Y. {Zhong} and A. {Jiang} and X. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={High-Accuracy Classification of Attention Deficit Hyperactivity Disorder with L2,1-Norm Linear Discriminant Analysis},
year={2020},
volume={},
number={},
pages={1170-1174},
abstract={Attention Deficit Hyperactivity Disorder (ADHD) is a high incidence of neurobehavioral disease in school-age children. Its neurobiological classification is meaningful for clinicians. The existing ADHD classification methods suffer from two problems, i.e., insufficient data and noise disturbance. Here, a high-accuracy classification method is proposed, which uses brain Functional Connectivity (FC) as material for ADHD feature analysis. In detail, we introduce a binary hypothesis testing framework as the classification outline to cope with insufficient data of ADHD database. Under binary hypotheses, the FCs of test data are allowed to use for training and thus affect the subspace learning of training data. To overcome noise disturbance, an l2,1-norm LDA model is adopted to robustly learn ADHD features in subspaces. The subspace energies of training data under binary hypotheses are then calculated, and an energy-based comparison is finally performed to identify ADHD individuals. On the platform of ADHD-200 database, the experiments show our method outperforms other state-of-the-art methods with the significant average accuracy of 97.6%.},
keywords={ADHD classification;binary hypothesis;feature learning;LDA;subspace learning},
doi={10.1109/ICASSP40776.2020.9053391},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053530,
author={J. {Wouters} and F. {Kloosterman} and A. {Bertrand}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Neural Network-Based Spike Sorting Feature Map That Resolves Spike Overlap in the Feature Space},
year={2020},
volume={},
number={},
pages={1175-1179},
abstract={When inserting an electrode array in the brain, its electrodes will record so-called ’spikes’ which are generated by the neurons in the neighbourhood of the array. Spike sorting is the process of detecting and assigning these recorded spikes to their putative neurons. Many spike sorting pipelines rely on a clustering algorithm that groups the spikes coming from the same neuron in a pre-defined feature space. However, classical spike sorting algorithms fail when spike overlap, i.e., the near-simultaneous occurrence of two or more spikes from different neurons, is present in the recording. In such cases, the overlapping spikes segment ends up in a seemingly random position in the feature space and is not assigned to the correct cluster. This problem has been addressed before by extending the sorting algorithm with a template matching post-processor. In this work, a novel approach is presented to resolve spike overlap directly in the feature space. To this end, a neural network feature map is presented, that generates spike embeddings (feature vectors) that behave as a linear superposition in the feature space in the case of spike overlap. Its performance is quantified on semi-synthetic data obtained through a data augmentation procedure applied to real neural recordings.},
keywords={spike sorting;spike overlap;neural network;feature extraction},
doi={10.1109/ICASSP40776.2020.9053530},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053270,
author={T. {Chen} and T. {Lin} and Y. -. P. {Hong}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Gait Phase Segmentation Using Weighted Dynamic Time Warping and K-Nearest Neighbors Graph Embedding},
year={2020},
volume={},
number={},
pages={1180-1184},
abstract={Gait phase segmentation is the process of identifying the start and end of different phases within a gait cycle. It is essential to many medical applications, such as disease diagnosis or rehabilitation. This work utilizes inertial measurement units (IMUs) mounted on the individual’s foot to gather gait information and develops a gait phase segmentation method based on the collected signals. The proposed method utilizes a weighted dynamic time warping (DTW) algorithm to measure the distance between two different gait signals, and a k-nearest neighbors (kNN) algorithm to obtain the gait phase estimates. To reduce the complexity of the DTW-based kNN search, we propose a neural network-based graph embedding scheme that is able to map the IMU signals associated with each gait cycle into a distance-preserving low-dimensional representation while also producing a prediction on the k nearest neighbors of the test signal. Experiments are conducted on self-collected IMU gait signals to demonstrate the effectiveness of the proposed scheme.},
keywords={Gait analysis;dynamic time warping;k-nearest neighbors;graph embedding},
doi={10.1109/ICASSP40776.2020.9053270},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053037,
author={S. {Subramani} and M. V. {Achuth Rao} and D. {Giridhar} and P. S. {Hegde} and P. {Kumar Ghosh}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Automatic Classification of Volumes of Water Using Swallow Sounds from Cervical Auscultation},
year={2020},
volume={},
number={},
pages={1185-1189},
abstract={The signatures of swallowing vary depending on the volume of bolus swallowed. Among existing instrumental methods, cervical auscultation (CA) captures the acoustic signatures of the swallow sound. Although many features present in the literature can characterize volumes of swallow using CA, they require manual annotations of the different components in the sound. In this work, a rich set of acoustic features, the ComParE 2016 acoustic feature set (OS) is used to investigate whether several temporal, spectral, vocal and source features and their functionals provide cues for volume classification. Experiments are performed with CA data from 56 subjects, with dry swallow and swallows of 2ml, 5ml, and 10ml of water. Three types of classification namely, dry-vs-2ml, dry-vs-5ml and dry-vs-10ml are performed separately to analyze characteristic features. Experiments reveal that OS, which does not require annotations, performs better than the baseline features that require annotation. Within OS, the features unrelated to voice source yield a better performance than the features related to voice source. In this subset of features, MFCC, RASTA filtered audio spectrum and RMS energy are found to be consistently the top performing features across all three types of classifications.},
keywords={swallow sound signal;cervical auscultation;acoustic analysis;feature selection},
doi={10.1109/ICASSP40776.2020.9053037},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054606,
author={Y. {Huang} and W. {Hsieh} and H. {Yang} and C. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Conditional Domain Adversarial Transfer for Robust Cross-Site ADHD Classification Using Functional MRI},
year={2020},
volume={},
number={},
pages={1190-1194},
abstract={There is a growing number of large scale cross-site database collection of resting-state functional magnetic resonance imaging (rs-fMRI) for studying neurobehavioral diseases, such as ADHD. Although a large amount of data benefits machine learning-based classification methods, the idiosyncratic variability of each site can deteriorate cross-site generalization ability. This challenge creates a bottleneck in requiring a large number of labeled samples of each site. Hence in this research, we utilize an approach of conditional adversarial domain adaptation network (CDAN) to learn a discriminative fMRI representation that is site-invariant for unsupervised transfer of ADHD classification. We evaluate our framework on a multi-site ADHD dataset and achieve improvement in transferring between sites. Further visualization reveals that there indeed exists a substantial site discrepancy and statistically analysis indicates that male’s rs-fMRI could be more vulnerable toward site-specific effects.},
keywords={ADHD;fMRI;adversarial domain adaptation;multi-site transfer},
doi={10.1109/ICASSP40776.2020.9054606},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052923,
author={S. {Sanei} and C. C. {Took} and D. {Jarchi} and A. {Prochazka}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Eeg Connectivity - Informed Cooperative Adaptive Line Enhancer for Recognition of Brain State},
year={2020},
volume={},
number={},
pages={1195-1199},
abstract={Bursts of sleep spindles and paroxysmal fast brain activity waveforms have frequency overlap whilst generally, paroxysmal waveforms have shorter duration than spindles. Both resemble bursts of normal alpha activity during short rests while awake with closed eyes. In this paper, it is shown that for a proposed cooperative adaptive line enhancer, which can both detect and separate such periodic bursts, the combination weights are consistently different from each other. The outcome suggests that for accurate modelling of the brain neuro-generators, the brain connectivity has to be precisely estimated and plugged into the adaptation process.},
keywords={Cooperative adaptive line enhancer;EEG;brain connectivity;alpha wave;paroxysmal;sleep spindles},
doi={10.1109/ICASSP40776.2020.9052923},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053148,
author={M. {Moscu} and R. {Borsoi} and C. {Richard}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Online Graph Topology Inference with Kernels For Brain Connectivity Estimation},
year={2020},
volume={},
number={},
pages={1200-1204},
abstract={In graph signal processing, there are often settings where the graph topology is not known beforehand and has to be estimated from data. Moreover, some graphs can be dynamic, such as brain activity supported by neurons or brain regions. This paper focuses on estimating in an online and adaptive manner a network structure capturing the non-linear dependencies among streaming graph signals in the form of a possibly directed, adjacency matrix. By projecting data into a higher- or infinite-dimension space, we focus on capturing nonlinear relationships between agents. In order to mitigate the increasing number of data points, we employ kernel dictionaries. Finally, we run a series of tests in order to experimentally illustrate the usefulness of our kernel-based approach on biomedical data, on which we obtain results comparable to state-of-the-art methods.},
keywords={Topology inference;reproducing kernel;graph signal processing;adaptive algorithm;brain connectivity estimation},
doi={10.1109/ICASSP40776.2020.9053148},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053706,
author={A. {Aminifar}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Minimal Adversarial Perturbations in Mobile Health Applications: The Epileptic Brain Activity Case Study},
year={2020},
volume={},
number={},
pages={1205-1209},
abstract={Today, the security of wearable and mobile-health technologies represents one of the main challenges in the Internet of Things (IoT) era. Adversarial manipulation of sensitive health-related information, e.g., if such information is used for prescribing medicine, may have irreversible consequences involving patients’ lives. In this article, we demonstrate the power of such adversarial attacks based on a real-world epileptic seizure detection problem. We identify the minimum perturbation required by the adversaries to declare a seizure (ictal) sample as non-seizure (inter-ictal) in emergency situations, i.e., minimal adversarial perturbation to fool the classification algorithm.},
keywords={Adversarial Perturbation;Mobile Health;Privacy and Security;Epilepsy;Seizure Detection},
doi={10.1109/ICASSP40776.2020.9053706},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054747,
author={S. {Majumder} and F. {Apicella} and F. {Muratori} and K. {Das}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Detecting Autism Spectrum Disorder Using Topological Data Analysis},
year={2020},
volume={},
number={},
pages={1210-1214},
abstract={Autism Spectrum Disorder (ASD) is a neurological developmental disorder affecting children at an early age. ASD diagnosis remains a challenge and is carried out using behavorial and developmental screening currently. Here we show the efficacy of using single trial EEG as a potential neural marker for ASD detection. We used EEG signals of a group of children participating in a visual cognitive task where images of emotional faces and trees were displayed. We used a feature extraction technique based on Topological Data Analysis (TDA) to classify autistic subjects from typically developing ones. The high accuracy ( 90%) of the pattern classifier validates the efficacy of using topological features in ASD detection. Our results also showed that ASD detection can be achieved as early as 120 ms post stimulus display and is independent of face/non-face images displayed.},
keywords={Topological Data Analysis;Persistent Homology;Persistent Entropy;Pattern Classifier;Autism Spectrum Disorder},
doi={10.1109/ICASSP40776.2020.9054747},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053022,
author={Y. {Akamatsu} and R. {Harakawa} and T. {Ogawa} and M. {Haseyama}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-View Bayesian Generative Model for Multi-Subject FMRI Data on Brain Decoding of Viewed Image Categories},
year={2020},
volume={},
number={},
pages={1215-1219},
abstract={Brain decoding studies have demonstrated that viewed image categories can be estimated from human functional magnetic resonance imaging (fMRI) activity. However, there are still limitations with the estimation performance because of the characteristics of fMRI data and the employment of only one modality extracted from viewed images. In this paper, we propose a multi-view Bayesian generative model for multi-subject fMRI data to estimate viewed image categories from fMRI activity. The proposed method derives effective representations of fMRI activity by utilizing multi-subject fMRI data. In addition, we associate fMRI activity with multiple modalities, i.e., visual features and semantic features extracted from viewed images. Experimental results show that the proposed method outperforms existing state-of-the-art methods of brain decoding.},
keywords={Brain decoding;fMRI;generative model;multi-view learning;multi-subject fMRI data},
doi={10.1109/ICASSP40776.2020.9053022},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053592,
author={D. F. {D’Croz-Baron} and M. C. {Baker} and T. {Karp}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Time-Frequency Analysis of Unimodal Sensory Processing In Autism Spectrum Disorder},
year={2020},
volume={},
number={},
pages={1220-1224},
abstract={This work summarizes the results of a time-frequency analysis of sensory processing in young adults with Autism Spectrum Disorder via continuous wavelet transform. The sensory tasks consisted of two blocks of unimodal sensory stimuli of the same type (i.e., auditory-auditory or visual-visual). A total of 12 autistic and 15 neurotypical subjects, all between 18-30 years, were analyzed. The 60 electrodes were grouped into 14 regions of interest to identify time-locked elicited brain activity. The power within three selected time-frequency windows for each block was compared between groups, showing significant differences in the first window of the second block of the visual-visual task, with the neurotypicals displaying higher power, suggesting an augmented brain activity in visual processing.},
keywords={Electroencephalogram;autism;sensory;wavelet;time-frequency.},
doi={10.1109/ICASSP40776.2020.9053592},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053143,
author={P. {Boonyakitanont} and A. {Lek-uthai} and J. {Songsiri}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Automatic Epileptic Seizure Onset-Offset Detection Based On CNN in Scalp EEG},
year={2020},
volume={},
number={},
pages={1225-1229},
abstract={We establish a deep learning-based method to automatically detect the epileptic seizure onsets and offsets in multi-channel electroencephalography (EEG) signals. A convolutional neural network (CNN) is designed to identify occurrences of seizures in EEG epochs from the EEG signals and an onset-offset detector is proposed to determine the seizure onsets and offsets. The EEG signals are considered as inputs and the outputs are the onset and offset. In the CNN, a filter is factorized to separately capture temporal and spatial patterns in EEG epochs. Moreover, we develop an onset-offset detection method based on clinical decision criteria. As a result, verified on the whole CHB-MIT Scalp EEG database, the CNN model correctly detected seizure activities over 90%. Furthermore, combined with the onset-offset detector, this method accomplished F1 of 64.40% and essentially determined the seizure onset and offset with absolute onset and offset latencies of 5.83 and 10.12 seconds, respectively.},
keywords={CNN;EEG;seizure detection;onset-offset detection},
doi={10.1109/ICASSP40776.2020.9053143},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054520,
author={D. {Wang} and Y. {Fang} and Y. {Li} and C. {Chai}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Enhance feature representation of electroencephalogram for Seizure detection},
year={2020},
volume={},
number={},
pages={1230-1234},
abstract={In the treatment of epilepsy with intracranial electroencephalogram(iEEG), the recognition accuracy is low, and it is difficult to find the correlation between channels because of the large amount of channel numbers and time series data. In order to solve these problems, we propose a novel EEG feature pre-presentation method for seizure detection based on the Log Mel-Filterbank energy feature. We propose to adapt the Mel-Filterbank energy to EEG features with logrithm transform in the frequency domain. Meanwhile, we also propose the sequential forward channel selection(SFCS) algorithm to incorporate channel correlation and balance the computing consumption. Experiments show that our proposed method have significant contributions in channel selection and feature representation to the problem of EEG signal analysis. The average results of experiments judged by the mean area under the ROC curve (AUC) of the probability reach 99.13%.},
keywords={Mel-Filterbank;Channel Selection;EEG;Seizure Detection},
doi={10.1109/ICASSP40776.2020.9054520},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053340,
author={G. {Krishna} and C. {Tran} and Y. {Han} and M. {Carnahan} and A. H. {Tewfik}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Speech Synthesis Using EEG},
year={2020},
volume={},
number={},
pages={1235-1238},
abstract={In this paper we demonstrate speech synthesis using different electroencephalography (EEG) feature sets recently introduced in [1]. We make use of a recurrent neural network (RNN) regression model to predict acoustic features directly from EEG features. We demonstrate our results using EEG features recorded in parallel with spoken speech as well as using EEG recorded in parallel with listening utterances. We provide EEG based speech synthesis results for four subjects in this paper and our results demonstrate the feasibility of synthesizing speech directly from EEG features.},
keywords={Speech synthesis;EEG;Deep Learning},
doi={10.1109/ICASSP40776.2020.9053340},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054457,
author={X. {Xu} and F. {Wei} and Z. {Zhu} and J. {Liu} and X. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Eeg Feature Selection Using Orthogonal Regression: Application to Emotion Recognition},
year={2020},
volume={},
number={},
pages={1239-1243},
abstract={A common drawback of the EEG applications is that the volume conduction of human head leads to lots of redundant information in EEG recordings. To reduce the redundancy and choose informative EEG features, in this paper, we propose an EEG feature selection technique, termed as Feature Selection with Orthogonal Regression (FSOR). Compared with classical feature selection methods, for nonlinear and nonstationary EEG signals, FSOR can employ orthogonal regression to preserve more discriminative information in the subspace. To verify the EEG feature selection performance, we collected a multichannel EEG dataset for emotion recognition and compared FSOR with two popular feature selection methods. The experimental results demonstrate the advantage of FSOR method over others for reducing the redundant information among the EEG relevant features. Additionally, we found that the absolute power ratio of beta wave to theta wave is the most discriminative feature, and beta band is the critical band for emotion recognition.},
keywords={EEG feature selection;discriminative feature;redundant information;orthogonal regression;embedded approaches},
doi={10.1109/ICASSP40776.2020.9054457},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054705,
author={T. {Sakai} and T. {Shoji} and N. {Yoshida} and K. {Fukumori} and Y. {Tanaka} and T. {Tanaka}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Scalpnet: Detection of Spatiotemporal Abnormal Intervals in Epileptic EEG Using Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={1244-1248},
abstract={We propose ScalpNet: A deep neural network to detect spatiotemporal abnormal intervals from EEGs of epilepsy patients. Since the number of trained clinicians is very limited, it is very crucial to establish automatic detection of abnormal signals caused by epilepsy from EEGs. We build a convolutional neural network detecting spatiotemporal intervals that will be abnormal based on the fact that peaky EEG signals can be observed not only in the electrode close to the focal region but those in the surrounding regions. In the experiments with a real dataset, our proposed ScalpNet presents higher classification accuracy than existing machine learning methods, including a convolutional neural network performed by channel-by-channel.},
keywords={Epilepsy;deep neural networks;electroencephalogram (EEG)},
doi={10.1109/ICASSP40776.2020.9054705},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054632,
author={R. {Banerjee} and A. {Ghose}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Semi-Supervised Approach For Identifying Abnormal Heart Sounds Using Variational Autoencoder},
year={2020},
volume={},
number={},
pages={1249-1253},
abstract={Abnormal heart sounds may have diverse frequency characteristics depending upon underlying pathological conditions. Designing a binary classifier for predicting normal and abnormal heart sounds using supervised learning requires a lot of training data, covering different types of cardiac abnormalities. In this paper, we propose a semi-supervised approach to solve the problem. A convolutional Variational Autoencoder (VAE) structure is defined for learning the probability distribution of the spectrogram properties of normal heart sounds. The Kullback-Leibler (KL) divergence between the known prior distribution of the VAE and the encoded distribution is taken as an anomaly score for detecting abnormal heart sounds. The proposed approach is evaluated on open access and in-house datasets of Phonocardiogram (PCG) signals, recorded from normal subjects and patients, having cardiovascular diseases, cardiac murmurs and extra heart sounds. Results show that an improved classification performance is achieved in comparison to the existing approaches.},
keywords={Heart sounds;Variational Autoencoder;Semi-supervised learning;Convolutional Neural Network},
doi={10.1109/ICASSP40776.2020.9054632},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053155,
author={R. {Prasad} and G. {Yilmaz} and O. {Chetelat} and M. {Magimai.-Doss}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Detection Of S1 And S2 Locations In Phonocardiogram Signals Using Zero Frequency Filter},
year={2020},
volume={},
number={},
pages={1254-1258},
abstract={Heart auscultation is a widely used technique for diagnosing cardiac abnormalities. In that context, capturing of phonocardiogram (PCG) signals and automatically monitoring of the heart by identifying S1 and S2 complexes is an emerging field. One of the first steps involved for identifying S1–S2 complexes is detection of the locations of these events in the PCG signals. Methods proposed in literature, to detect these events in the PCG signal, have largely focused on exploiting the dominant low frequency characteristics of the S1–S2 complexes through frequency–domain processing. In this paper, we propose a purely time–domain processing based method that employs a heavily decaying low pass filter (referred to as zero frequency filter) to suppress extraneous factors and detect S1–S2 locations. We demonstrate the potential of the proposed approach through investigations on two publicly available data sets, namely the PASCAL heart sounds challenge 2011 (PHSC–2011) and Phys- ioNet CinC. The method is also evaluated through an analysis with wearable sensors in the presence and absence of speech activity.},
keywords={Phonocardiogram;S1–S2 detection;zero frequency filter;modified ZFF},
doi={10.1109/ICASSP40776.2020.9053155},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053358,
author={L. {Yao} and J. L. {Baker} and J. {Ryou} and N. D. {Schiff} and K. P. {Purpura} and M. {Shoaran}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Mental Fatigue Prediction from Multi-Channel ECOG Signal},
year={2020},
volume={},
number={},
pages={1259-1263},
abstract={Early detection of mental fatigue and changes in vigilance could be used to initiate neurostimulation to treat patients suffering from brain injury and mental disorders. In this study, we analyzed electrocorticography (ECoG) signals chronically recorded from two non-human primates (NHPs) as they performed a cognitively demanding task over extended periods of time. We employed a set of biomarkers to identify mental fatigue and a gradient boosting classifier to predict the performance outcome, seconds prior to the actual behavior response. An average F1 score of 75.4% ± 8.4% and 86.4%±6.6% was obtained for the two studied NHPs. Our preliminary results demonstrate the feasibility of detecting mental fatigue in healthy primates that could be used for closed-loop control of neurostimulation therapy.},
keywords={Mental fatigue;ECoG;machine learning;feature extraction;vigilance task;deep-brain stimulation},
doi={10.1109/ICASSP40776.2020.9053358},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053800,
author={F. N. {Hatamian} and N. {Ravikumar} and S. {Vesal} and F. P. {Kemeth} and M. {Struck} and A. {Maier}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={The Effect of Data Augmentation on Classification of Atrial Fibrillation in Short Single-Lead ECG Signals Using Deep Neural Networks},
year={2020},
volume={},
number={},
pages={1264-1268},
abstract={Cardiovascular diseases are the most common cause of mortality worldwide. Detection of atrial fibrillation (AF) in the asymptomatic stage can help prevent strokes. It also improves clinical decision making through the delivery of suitable treatment such as, anticoagulant therapy, in a timely manner. The clinical significance of such early detection of AF in electrocardiogram (ECG) signals has inspired numerous studies in recent years, of which many aim to solve this task by leveraging machine learning algorithms. ECG datasets containing AF samples, however, usually suffer from severe class imbalance, which if unaccounted for, affects the performance of classification algorithms. Data augmentation is a popular solution to tackle this problem.In this study, we investigate the impact of various data augmentation algorithms, e.g., oversampling, Gaussian Mixture Models (GMMs) and Generative Adversarial Networks (GANs), on solving the class imbalance problem. These algorithms are quantitatively and qualitatively evaluated, compared and discussed in detail. The results show that deep learning-based AF signal classification methods benefit more from data augmentation using GANs and GMMs, than oversampling. Furthermore, the GAN results in circa 3% better AF classification accuracy in average while performing comparably to the GMM in terms of f1-score.},
keywords={atrial fibrillation;data augmentation;GMM;DCGAN},
doi={10.1109/ICASSP40776.2020.9053800},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054403,
author={Y. {Chen} and A. H. {Twing} and D. {Badawi} and J. {Danavi} and M. {McCauley} and A. E. {Cetin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Atrial Fibrillation Risk Prediction from Electrocardiogram and Related Health Data with Deep Neural Network},
year={2020},
volume={},
number={},
pages={1269-1273},
abstract={Electrocardiography (ECG) is a widely used tool for studying and diagnosing the heart diseases. Atrial fibrillation (AF) is an irregular and often rapid heart rate that can increase the risk of strokes, heart failure and other heart-related complications. In this study, we develop a novel and effective method to predict the potential AF risk of patients using our ECG signal dataset collected in the University of Illinois Hospital and Health Sciences System. We use a convolutional neural network (CNN) structure to process both the ECG signals and the related health data of patients. Our experimental results indicate that the model with patients’ health data can predict the AF with 79.9% accuracy), and which is better than a CNN trained without related health data 72.2% accuracy), which implies that patients’ health data play an important role in predicting AF risk. Very high sensitivity and specificity of the class of normal sinus rhythm (NSR) cases also verify that the model works well for distinguishing between NSR and ECG signals with potential AF risk.},
keywords={ECG analysis;Demographic information;Atrial Fibrillation;Convolutional Neural Network},
doi={10.1109/ICASSP40776.2020.9054403},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053244,
author={M. {Chen} and G. {Wang} and H. {Chen} and Z. {Ding}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adaptive Region Aggregation Network: Unsupervised Domain Adaptation with Adversarial Training for ECG Delineation},
year={2020},
volume={},
number={},
pages={1274-1278},
abstract={Electrocardiogram (ECG) delineation, which provides clinically useful information for the diagnosis of cardiovascular disease, is an essential task in automated ECG analysis. The discrepancies among ECG signals from different datasets, namely domain shifts, may bring severe challenges to the cross-dataset performance of ECG delineation algorithms. The domain shifts are generally caused by the differences of conditions, collecting devices, and individual characteristics, and are inherent and non-negligible in ECG. In this work, we propose an unsupervised domain adaptation method called Adaptive Region Aggregation Network (ARAN) based on adversarial training to tackle domain shift problem in ECG delineation. The proposed algorithm promotes the state- of-the-art deep neural network RAN[1] to learn domain- invariant features and achieve improving performance on both source and target domain. The experiments results on two public datasets, LUDB and QT database, prove that our approach can effectively improve the cross-dataset performance of the state-of-the-art deep learning model.},
keywords={ECG delineation;deep learning;unsupervised domain adaptation},
doi={10.1109/ICASSP40776.2020.9053244},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054503,
author={T. T. K. {Munia} and S. {Aviyente}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Matching Pursuit Based Dynamic Phase-Amplitude Coupling Measure},
year={2020},
volume={},
number={},
pages={1279-1283},
abstract={Long-distance neuronal communication in the brain is enabled by the interactions across various oscillatory frequencies. One interaction that is gaining importance during cognitive brain functions is phase amplitude coupling (PAC), where the phase of a slow oscillation modulates the amplitude of a fast oscillation. Current techniques for calculating PAC provide a numerical index that represents an average value across a pre-determined time window. However, there is growing empirical evidence that PAC is dynamic, varying across time. Current approaches to quantify time-varying PAC relies on computing PAC over sliding short time windows. This approach suffers from the arbitrary selection of the window length and does not adapt to the signal dynamics. In this paper, we introduce a data-driven approach to quantify dynamic PAC. The proposed approach relies on decomposing the signal using matching pursuit (MP) to extract time and frequency localized atoms that best describe the given signal. These atoms are then used to compute PAC across time and frequency. As the atoms are time and frequency localized, we only compute PAC across time and frequency regions determined by the selected atoms rather than the whole time-frequency range. The proposed approach is evaluated on both simulated and real electroencephalogram (EEG) signals.},
keywords={Matching Pursuit;Dynamic;Phase Amplitude Coupling;Electroencephalogram.},
doi={10.1109/ICASSP40776.2020.9054503},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054388,
author={R. {Anderson} and M. {Sandsten}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multitaper Spectral Granger Causality with Application to Ssvep},
year={2020},
volume={},
number={},
pages={1284-1288},
abstract={The traditional parametric approach to Granger causality (GC), based on linear vector autoregressive modeling, suffers from difficulties related to the inaccurate modeling of the generative process. These limits can be solved by using nonparametric spectral estimates in the frequency-domain formulation of GC, also known as spectral GC. In a simulation study, we compare the mean square error of the estimated spectral GC using different multitaper spectral estimators, finding that the Peak Matched multitapers are preferable for estimating spectral GC characterized by peaks. As an illustrative example, we apply the non-parametric approach to the analysis of brain functional connectivity in steady-state visually evoked potentials.},
keywords={Non-parametric spectral Granger causality;multitaper spectral estimation;functional connectivity;steady-state visually evoked potentials},
doi={10.1109/ICASSP40776.2020.9054388},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053604,
author={E. {Lee} and A. {Ho} and Y. {Wang} and C. {Huang} and C. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cross-Domain Adaptation for Biometric Identification Using Photoplethysmogram},
year={2020},
volume={},
number={},
pages={1289-1293},
abstract={The adoption of biomedical signals such as photoplethysmogram (PPG) and electrocardiogram (ECG) for health parameter estimation on wearable devices is growing in tandem with the increase of attention in mobile healthcare. In our work, we use PPG signals extracted from PPG sensors which are used for biometric identification. A challenge for biometric identification using PPG signal is the variation in domain (placement of sensors, wavelengths, device variation, etc.). In this work, we propose the use of both unsupervised and semi-supervised adversarial learning techniques for cross-domain adaptation. As such algorithm will be deployed on wearable devices, we propose a compact model meeting tight memory footprint limitation. All experiments will be simulated using a public dataset (TROIKA) and our in-house dataset. By introducing a cross-domain adaptation approach across sensors, we observe an accuracy gain of 4.15% on our in-house dataset. The proposed semi-supervised learning technique gives an additional accuracy boost of 2.02%.},
keywords={Cross-domain adaptation;biometric identification;photoplethysmogram;deep learning},
doi={10.1109/ICASSP40776.2020.9053604},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054160,
author={E. H. {Nirjhar} and A. {Behzadan} and T. {Chaspari}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Exploring Bio-Behavioral Signal Trajectories of State Anxiety During Public Speaking},
year={2020},
volume={},
number={},
pages={1294-1298},
abstract={Public speaking anxiety (PSA) is among the top social phobias in the world. Quantifying PSA in a reliable and unobtrusive manner can lay the foundation toward personalized and inexpensive technology-based interventions. Existing work for quantifying PSA often relies on self-reported measures and statistical aggregates of bio-behavioral indices, such as physiology and speech. Such aggregated bio-behavioral indices are not able to capture time-based trajectories of PSA variation, that can be very useful for better understanding and reliably predicting moments of anxiety. We tackle this problem by introducing temporal parametric models to quantify bio-behavioral trajectories of PSA throughout a public speaking encounter. Using data from 55 participants in a real-life public speaking task, the parameters of the proposed models are found to be significantly correlated with individuals’ trait characteristics of general and communication-based anxiety, outperforming aggregate mean bio-behavioral measures.},
keywords={Public speaking anxiety;physiology;speech;transient oscillation;quadratic polynomial},
doi={10.1109/ICASSP40776.2020.9054160},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054227,
author={A. D. {Silva} and M. V. {Perera} and K. {Wickramasinghe} and A. M. {Naim} and T. {Dulantha Lalitharatne} and S. L. {Kappel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Real-Time Hand Gesture Recognition Using Temporal Muscle Activation Maps of Multi-Channel Semg Signals},
year={2020},
volume={},
number={},
pages={1299-1303},
abstract={Accurate and real-time hand gesture recognition is essential for controlling advanced hand prostheses. Surface Electromyography (sEMG) signals obtained from the forearm are widely used for this purpose. Here, we introduce a novel hand gesture representation called Temporal Muscle Activation (TMA) maps which captures information about the activation patterns of muscles in the forearm. Based on these maps, we propose an algorithm that can recognize hand gestures in real-time using a Convolution Neural Network. The algorithm was tested on 8 healthy subjects with sEMG signals acquired from 8 electrodes placed along the circumference of the forearm. The average classification accuracy of the proposed method was 94%, which is comparable to state-of-the-art methods. The average computation time of a prediction was 5.5ms, making the algorithm ideal for the real-time gesture recognition applications.},
keywords={Multi-Channel Surface Electromyography;Real-Time Hand Gesture Recognition;Temporal Muscle Activation Maps;Onset Detection;Convolutional Neural Networks},
doi={10.1109/ICASSP40776.2020.9054227},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054586,
author={E. {Rahimian} and S. {Zabihi} and S. F. {Atashzar} and A. {Asif} and A. {Mohammadi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={XceptionTime: Independent Time-Window Xceptiontime Architecture for Hand Gesture Classification},
year={2020},
volume={},
number={},
pages={1304-1308},
abstract={Capitalizing on the goal of addressing identified shortcomings of recent solutions developed for recognition tasks via sparse multichannel surface Electromyography (sEMG) signals, the paper proposes a novel deep learning model, referred to as the XceptionTime architecture. The proposed innovative XceptionTime architecture is designed by integration of depthwise separable convolutions, adaptive average pooling, and a novel no-linear normalization technique. At the hearth of the proposed architecture is several XceptionTime modules concatenated in series fashion designed to captures both temporal and spatial information-bearing contents of the sparse multichannel sEMG signals without the need for data augmentation and manual design of feature extraction. In addition to instruction of the new XceptionTime module, by integration of adaptive average pooling, instead of fully connected layers, and utilization of a novel non-linear normalization approach, the proposed architecture is less prone to overfitting, more robust to temporal translation of the input, and more importantly is independent from the input window size, i.e., there is no need to change/reconfigure the architecture by changing the size of the input sequence. Finally, by utilizing the depthwise separable convolutions, the XceptionTime network has far less parameters resulting in less complex network.},
keywords={Surface Electromyography (sEMG);Depthwise Separable Convolution;Adaptive Average Pooling},
doi={10.1109/ICASSP40776.2020.9054586},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053462,
author={G. {Feng} and J. G. {Quirk} and P. M. {Djurić}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Discovering Causalities from Cardiotocography Signals using Improved Convergent Cross Mapping with Gaussian Processes},
year={2020},
volume={},
number={},
pages={1309-1313},
abstract={Convergent cross mapping (CCM) is designed for causal discovery in coupled time series, where Granger causality may not be applicable because of a separability assumption. However, CCM is not robust to observation noise which limits its applicability on signals that are known to be noisy. Moreover, the parameters for state space reconstruction need to be selected using grid search methods. In this paper, we propose a novel improved version of CCM using Gaussian processes for discovery of causality from noisy time series. Specifically, we adopt the concept of CCM and carry out the key steps using Gaussian processes within a non-parametric Bayesian probabilistic framework in a principled manner. The proposed approach is first validated on simulated data, and then used for understanding the interaction between fetal heart rate and uterine activity in the last two hours before delivery and of interest in obstetrics. Our results indicate that uterine activity affects the fetal heart rate, which agrees with recent clinical studies.},
keywords={Convergent cross mapping;state space reconstruction;Gaussian processes;fetal heart rate;uterine activity},
doi={10.1109/ICASSP40776.2020.9053462},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053662,
author={D. M. {Montserrat} and C. {Bustamante} and A. {Ioannidis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Lai-Net: Local-Ancestry Inference with Neural Networks},
year={2020},
volume={},
number={},
pages={1314-1318},
abstract={Local-ancestry inference (LAI), also referred to as ancestry deconvolution, provides high-resolution ancestry estimation along the human genome. In both research and industry, LAI is emerging as a critical step in DNA sequence analysis with applications extending from polygenic risk scores (used to predict traits in embryos and disease risk in adults) to genome-wide association studies, and from pharmacogenomics to inference of human population history. While many LAI methods have been developed, advances in computing hardware (GPUs) combined with machine learning techniques, such as neural networks, are enabling the development of new methods that are fast, robust and easily shared and stored. In this paper we develop the first neural network based LAI method, named LAI-Net, providing competitive accuracy with state-of-the-art methods and robustness to missing or noisy data, while having a small number of layers.},
keywords={Local-Ancestry Inference;Genomics;Genetics;Neural Networks;Deep Learning},
doi={10.1109/ICASSP40776.2020.9053662},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054666,
author={V. {Raval} and K. P. {Nguyen} and A. {Gerald} and R. B. {Dewey} and A. {Montillo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Prediction of Individual Progression Rate in Parkinson’s Disease Using Clinical Measures and Biomechanical Measures of Gait and Postural Stability},
year={2020},
volume={},
number={},
pages={1319-1323},
abstract={Parkinson’s disease (PD) is a common neurological disorder characterized by gait impairment. PD has no cure, and an impediment to developing a treatment is the lack of any accepted method to predict disease progression rate. The primary aim of this study was to develop a model using clinical measures and biomechanical measures of gait and postural stability to predict an individual’s PD progression over two years. Data from 160 PD subjects were utilized. Machine learning models, including XGBoost and Feed Forward Neural Networks, were developed using extensive model optimization and cross-validation. The highest performing model was a neural network that used a group of clinical measures, achieved a PPV of 71% in identifying fast progressors, and explained a large portion (37%) of the variance in an individual’s progression rate on held-out test data. This demonstrates the potential to predict individual PD progression rate and enrich trials by analyzing clinical and biomechanical measures with machine learning.},
keywords={Parkinson’s Disease;Prognosis;Machine Learning;Biomechanical Measures;Progression Rate},
doi={10.1109/ICASSP40776.2020.9054666},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053827,
author={A. {Mongia} and A. {Majumdar}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Matrix Completion on Graphs: Application in Drug Target Interaction Prediction},
year={2020},
volume={},
number={},
pages={1324-1328},
abstract={This work proposes matrix completion via deep matrix factorization on graphs. The work is motivated by the success of two very recent studies on (shallow) matrix completion on graphs and deep matrix factorization (without graphs). We show that the proposed deep matrix factorization on graphs improves over both - shallow techniques on graphs and deep matrix factorization. Experiments are carried out on the challenging real-life problem of modeling drug-target interactions.},
keywords={Graph Regularization;Deep Matrix Completion;Drug-Target Interaction Prediction},
doi={10.1109/ICASSP40776.2020.9053827},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052965,
author={C. {Wu} and H. {Zhang} and L. {Zhang} and H. {Zheng}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Identification of Essential Proteins Using A Novel Multi-Objective Optimization Method},
year={2020},
volume={},
number={},
pages={1329-1333},
abstract={Using graph theory to identify essential proteins is a hot topic at present. These methods are called network-based methods. However, the generalization ability of most network-based methods is not satisfactory. Hence, in this paper, we consider the identification of essential proteins as a multi-objective optimization problem and use a novel multi-objective optimization method to solve it. The optimization result is a set of Pareto solutions. Every solution in this set is a vector which has a certain number of essential protein candidates and is considered as an independent predictor or voter. We use a voting strategy to assemble the results of these predictors. To validate our method, we apply it on the protein-protein interactions (PPI) datasets of two species (Yeast and Escherichia coli). The experiment results show that our method outperforms state-of-the-art methods in terms of sensitive, specificity, F-measure, accuracy, and generalization ability.},
keywords={Essential proteins;graph theory;multiobjective optimization;protein-protein interactions},
doi={10.1109/ICASSP40776.2020.9052965},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054598,
author={R. {Konda} and H. {Wu} and M. D. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Graph Convolutional Neural Networks to Classify Whole Slide Images},
year={2020},
volume={},
number={},
pages={1334-1338},
abstract={Whole slide images (WSIs) are the digitization of histology slides and are increasingly used by pathologists to detect the cancerous regions and make diagnosis for patients. Recent machine learning and deep learning algorithms have shown great success in automated classification of WSIs. However, given the computational challenge associated with processing high resolution WSIs, conventional techniques rely on a patch-based approach, and subsequently aggregate extracted features using firs-order statistics. However, as cancerous regions are clustered together, such an approach ignores the spatial relationships within each tile. Here, we present a novel application of graph convolutional networks (GCNs) to analyze WSIs. GCNs are powerful deep neural networks for modelling node relationships in a graph. To capture the spatial information, we model each tile as a node in the graph, and aggregate features by applying GCNs to the graph. The classification results on real-world histopathology datasets shows improved performance over conventional methods, and highlights the potential of graph-based methods in biomedical data analytics.},
keywords={Whole slide images;graph convolution networks;biomedical imaging},
doi={10.1109/ICASSP40776.2020.9054598},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053694,
author={M. {Angjelichinoski} and M. {Soltani} and J. {Choi} and B. {Pesaran} and V. {Tarokh}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep James-Stein Neural Networks For Brain-Computer Interfaces},
year={2020},
volume={},
number={},
pages={1339-1343},
abstract={Nonparametric regression has proven to be successful in extracting features from limited data in neurological applications. However, due to data scarcity, most brain-computer interfaces still rely on linear classifiers. This work leverages the robustness of the James-Stein theorem in nonparametric regression to harness the potentials of deep learning and foster its successful application in neural engineering with small data sets. We propose a novel method that combines James-Stein regression for feature extraction, and deep neural network for decoding; we refer to the architecture as deep James-Stein neural network (DJSNN). We apply the DJSNN to decode eye movement goals in a memory-guided visual saccades to one of eight target locations. The results demonstrate that the DJSNN outperforms existing methods by a substantial margin, especially at deep cortical sites.},
keywords={Brain-computer interfaces;nonparametric regression;James-Stein estimation;deep neural networks},
doi={10.1109/ICASSP40776.2020.9053694},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053168,
author={S. {Kumar} and T. K. {Reddy} and V. {Arora} and L. {Behera}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Formulating Divergence Framework for Multiclass Motor Imagery EEG Brain Computer Interface},
year={2020},
volume={},
number={},
pages={1344-1348},
abstract={The ubiquitous presence of non-stationarities in the EEG signals significantly perturb the feature distribution thus deteriorating the performance of Brain Computer Interface. In this work, a novel method is proposed based on Joint Approximate Diagonalization (JAD) to optimize stationarity for multiclass motor imagery Brain Computer Interface (BCI) in an information theoretic framework. Specifically, in the proposed method, we estimate the subspace which optimizes the discriminability between the classes and simultaneously preserve stationarity within the motor imagery classes. We determine the subspace for the proposed approach through optimization using gradient descent on an orthogonal manifold. The performance of the proposed stationarity enforcing algorithm is compared to that of baseline One-Versus-Rest (OVR)-CSP and JAD on publicly available BCI competition IV dataset IIa. Results show that an improvement in average classification accuracies across the subjects over the baseline algorithms and thus essence of alleviating within session non-stationarities.},
keywords={Brain Computer Interface (BCI);Electroencephalogram (EEG);Motor Imagery (MI);Divergence;Common Spatial Patterns (CSP);Joint Approximate Diagonalization (JAD);Information theoretic (IT)},
doi={10.1109/ICASSP40776.2020.9053168},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054070,
author={S. {Kanoga} and T. {Hoshino} and H. {Asoh}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Subject Transfer Framework Based on Source Selection and Semi-Supervised Style Transfer Mapping for Semg Pattern Recognition},
year={2020},
volume={},
number={},
pages={1349-1353},
abstract={To construct subject-specific feature extractors and classifiers for a new subject using pooled datasets, overcoming intersubject variabilities is required. In this study, we investigate the efficiency of the proposed subject transfer framework, which applies a discriminability-based source selection approach and semi-supervised style transfer mapping algorithm, by constructing support vector machine classifiers. We collect a surface electromyogram (sEMG) dataset acquired from 25 subjects using a wearable sEMG sensor. Classifiers are trained with gold-standard time-domain and autoregressive features extracted from eight-channel sEMG data. Compared with conventional subject transfer framework (85.08±1.38%), which applies the covariate shift adaptation algorithm to the linear discriminant analysis classifier and uses all source data, our proposed framework improves pattern recognition accuracy (90.63 ± 1.27%) by selection of discriminative source data and the mapping destination in the Euclidean space.},
keywords={Transfer learning;style transfer mapping;surface electromyogram (sEMG);covariate shift adaptation.},
doi={10.1109/ICASSP40776.2020.9054070},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052997,
author={D. {Lee} and J. {Jeong} and K. {Shim} and S. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Decoding Movement Imagination and Execution From Eeg Signals Using Bci-Transfer Learning Method Based on Relation Network},
year={2020},
volume={},
number={},
pages={1354-1358},
abstract={A brain-computer interface (BCI) is used to control external devices for healthy people as well as to rehabilitate motor functions for motor-disabled patients. Decoding movement intention is one of the most significant aspects for performing arm movement tasks using brain signals. Decoding movement execution (ME) from electroencephalogram (EEG) signals have shown high performance in previous works, however movement imagination (MI) paradigm-based intention decoding has so far failed to achieve sufficient accuracy. In this study, we focused on a robust MI decoding method with transfer learning for the ME and MI paradigm. We acquired EEG data related to arm reaching for 3D directions. We proposed a BCI-transfer learning method based on a Relation network (BTRN) architecture. Decoding performances showed the highest performance compared to conventional works. We confirmed the possibility of the BTRN architecture to contribute to continuous decoding of MI using ME datasets.},
keywords={Brain-computer interface (BCI);Electroencephalogram (EEG);Transfer learning;Movement imagination and execution},
doi={10.1109/ICASSP40776.2020.9052997},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054359,
author={B. {Lee} and J. {Jeong} and K. {Shim} and S. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Classification of High-Dimensional Motor Imagery Tasks Based on An End-To-End Role Assigned Convolutional Neural Network},
year={2020},
volume={},
number={},
pages={1359-1363},
abstract={A brain-computer interface (BCI) provides a direct communication pathway between user and external devices. EEG-based motor imagery paradigm is widely used in non-invasive BCI to obtain encoded signals contained user intention of movement execution. However, EEG has intricate and non-stationary properties resulting in insufficient decoding performance. By imagining numerous movements of a singlearm, decoding performance can be improved without artificial command matching. In this study, we collected intuitive EEG data contained the nine different types of movements of a single-arm from 9 subjects. We propose an end-to-end role assigned convolutional neural network (ERA-CNN) which considers discriminative features of each upper limb region by adopting the principle of a hierarchical CNN architecture. The proposed model outperforms previous methods on 3-class, 5-class and two different types of 7-class classification tasks. Hence, we demonstrate the possibility of decoding user intention by using only EEG signals with robust performance using the ERA-CNN.},
keywords={Brain-computer interface (BCI);Electroencephalogram (EEG);Motor imagery;Convolutional Neural Network (CNN)},
doi={10.1109/ICASSP40776.2020.9054359},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053101,
author={K. {Sadatnejad} and A. {Roc} and L. {Pillette} and A. {appriou} and T. {Monseigne} and F. {Lotte}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Channel Selection over Riemannian Manifold with Non-Stationarity Consideration for Brain-Computer Interface Applications},
year={2020},
volume={},
number={},
pages={1364-1368},
abstract={In this paper, we propose and compare multiple criteria for selecting ElectroEncephaloGraphic (EEG) channels over the Riemannian manifold, for EEG classification in Brain- Computer Interfaces (BCI). These criteria aim to promote EEG covariance matrix classifiers to generalize well by considering EEG data non-stationarity. Our approach consists of both increasing the discriminative information between classes over the manifold and reducing the dispersion within classes. We also reduce the influence of outliers in both discriminative and dispersion measures. Using the proposed criteria, channel selection is done automatically in a backward elimination process. The criteria are evaluated on EEG signals recorded from a tetraplegic subject and dataset IVa from BCI competition III. Experimental evidences confirm that considering the dispersion within each class as a measure for quantifying the effects of non-stationarity and removing the most affected channels can improve the performance of BCI by 5% on the tetraplegic subject and by 12 % on dataset IVa.},
keywords={Channel selection;BCI;EEG;covariance matrix;Riemannian manifold},
doi={10.1109/ICASSP40776.2020.9053101},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054077,
author={Y. {Wang} and J. {Zhang} and C. {An} and M. {Cavichini} and M. {Jhingan} and M. J. {Amador-Patarroyo} and C. P. {Long} and D. G. {Bartsch} and W. R. {Freeman} and T. Q. {Nguyen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Segmentation Based Robust Deep Learning Framework for Multimodal Retinal Image Registration},
year={2020},
volume={},
number={},
pages={1369-1373},
abstract={Multimodal image registration plays an important role in diagnosing and treating ophthalmologic diseases. In this paper, a deep learning framework for multimodal retinal image registration is proposed. The framework consists of a segmentation network, feature detection and description network, and an outlier rejection network, which focuses only on the globally coarse alignment step using the perspective transformation. We apply the proposed framework to register color fundus images with infrared reflectance images and compare it with the state-of-the-art conventional and learning-based approaches. The proposed framework demonstrates a significant improvement in robustness and accuracy reflected by a higher success rate and Dice coefficient compared to other coarse alignment methods.},
keywords={Image registration;multimodal;retinal images;convolutional neural networks},
doi={10.1109/ICASSP40776.2020.9054077},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054290,
author={C. {Guo} and M. {Szemenyei} and Y. {Yi} and Y. {Xue} and W. {Zhou} and Y. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Dense Residual Network for Retinal Vessel Segmentation},
year={2020},
volume={},
number={},
pages={1374-1378},
abstract={Retinal vessel segmentation plays an imaportant role in the field of retinal image analysis because changes in retinal vascular structure can aid in the diagnosis of diseases such as hypertension and diabetes. In recent research, numerous successful segmentation methods for fundus images have been proposed. But for other retinal imaging modalities, more research is needed to explore vascular extraction. In this work, we propose an efficient method to segment blood vessels in Scanning Laser Ophthalmoscopy (SLO) retinal images. Inspired by U-Net, "feature map reuse" and residual learning, we propose a deep dense residual network structure called DRNet. In DRNet, feature maps of previous blocks are adaptively aggregated into subsequent layers as input, which not only facilitates spatial reconstruction, but also learns more efficiently due to more stable gradients. Furthermore, we introduce DropBlock to alleviate the overfitting problem of the network. We train and test this model on the recent SLO public dataset. The results show that our method achieves the state-of-the-art performance even without data augmentation[[fn1]]1* Corresponding author},
keywords={Retinal vessel segmentation;Scanning Laser Ophthalmoscopy (SLO);U-Net;DRNet;DropBlock},
doi={10.1109/ICASSP40776.2020.9054290},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053454,
author={T. {Lei} and W. {Zhou} and Y. {Zhang} and R. {Wang} and H. {Meng} and A. K. {Nandi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Lightweight V-Net for Liver Segmentation},
year={2020},
volume={},
number={},
pages={1379-1383},
abstract={The V-Net based 3D fully convolutional neural networks have been widely used in liver volumetric data segmentation. However, due to the large number of parameters of these networks, 3D FCNs suffer from high computational cost and GPU memory usage. To address these issues, we design a lightweight V-Net (LV-Net) for liver segmentation in this paper. The proposed network makes two contributions. The first is that we design an inverted residual bottleneck block (IRB block) and a 3D average pooling block and apply them to the proposed LV-Net. Compared with vanilla convolution, depth-wise convolution and point-wise convolution employed by the IRB block can not only reduce the number of parameters significantly, but also extract features sufficiently well by decoupling cross-channel corrections and spatial correlations. The second is that the LV-Net employs 3D deep supervision to improve the final loss function in training phase, which makes the proposed LV-Net acquire a more powerful discrimination capability between liver areas and non-liver areas. The proposed LV-Net is evaluated on public LiTS dataset, and experiments demonstrate that the proposed LV-Net is superior to popular 2D and 3D networks in terms of segmentation performance, parameter quantity and computational cost.},
keywords={deep learning;image segmentation;3D fully convolutional neural network;network compression},
doi={10.1109/ICASSP40776.2020.9053454},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054616,
author={C. {hu} and G. {Kang} and B. {Hou} and Y. {Ma} and F. {Labeau} and Z. {Su}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Acu-Net: A 3D Attention Context U-Net for Multiple Sclerosis Lesion Segmentation},
year={2020},
volume={},
number={},
pages={1384-1388},
abstract={Multiple Sclerosis (MS) lesion segmentation from MR images is important for neuroimaging analysis. MS is diffuse, multifocal, and tend to involve peripheral brain structures such as the white matter, corpus callosum, and brainstem. Recently, U-Net has made great achievements in medical image segmentation area. However, the insufficiently use of context information and feature representation, makes it fail to achieve segmentation of MS lesions accurately. To solve the problem, 3D attention context U-Net (ACU-Net) is proposed for MS lesion segmentation in this paper. The proposed ACU-Net includes 3D spatial attention block, which is used to enrich spatial details and feature representation of lesion in the decoding stage. Furthermore, in the encoding and decoding stage of the network, 3D context guided module is designed for guiding local information and surrounding information. The proposed ACU-Net was evaluated on the ISBI 2015 longitudinal MS lesion segmentation challenge dataset, and it achieved superior performance compared to latest approaches.},
keywords={Multiple sclerosis lesion;segmentation;U-Net;spatial attention;context guided},
doi={10.1109/ICASSP40776.2020.9054616},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053633,
author={S. {Gehlot} and A. {Gupta} and R. {Gupta}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={EDNFC-Net: Convolutional Neural Network with Nested Feature Concatenation for Nuclei-Instance Segmentation},
year={2020},
volume={},
number={},
pages={1389-1393},
abstract={Accurate nuclei identification is an important step in diagnosis of several diseases. The problem is complex due to heterogeneity in structure, color, and texture among the different categories of cells. The problem is further complicated due to overlapped/clustered nuclei. To address these challenges, we propose an Encoder-Decoder based Convolutional Neural Network (CNN) with Nested-Feature Concatenation (EDNFC-Net) for automatic nuclei segmentation. The feature concatenation cell (FCC) of the EDNFC-Net is made up of two stacks of convolutional filters combined with non-linearity, followed by a concatenation of features. Apart from intra-FCC feature concatenation, a mechanism is also provided for inter-FCC feature concatenation. This arrangement leads to better feature flow and feature-reusability. Similarly, direct feature flow is provided between the encoder and decoder module that preserves the context information. A new loss function with better-penalizing capability is also proposed that helps in the better background and foreground separation. Qualitative and quantitative results are provided on two datasets to validate the proposed architecture and loss function.},
keywords={Nuclei segmentation;Deep Learning;CNN;Cell detection},
doi={10.1109/ICASSP40776.2020.9053633},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054023,
author={T. {Li} and M. {Comer} and J. {Zerubia}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Unsupervised Retinal Vessel Extraction and Segmentation Method Based On a Tube Marked Point Process Model},
year={2020},
volume={},
number={},
pages={1394-1398},
abstract={Retinal vessel extraction and segmentation is essential for supporting diagnosis of eye-related diseases. In recent years, deep learning has been applied to vessel segmentation and achieved excellent performance. However, these supervised methods require accurate hand-labeled training data, which may not be available. In this paper, we propose an unsupervised segmentation method based on our previous connected tube marked point process (MPP) model. The vessel network is extracted by the connected-tube MPP model first. Then a new tube-based segmentation method is applied to the extracted tubes. We test this method on STARE and DRIVE databases and the results show that not only do we extract the retina vessel network accurately, but we also achieve high G-means score for vessel segmentation, without using labeled training data.},
keywords={Retinal vessel segmentation;vessel network extraction;connected tube model;marked point process},
doi={10.1109/ICASSP40776.2020.9054023},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053768,
author={W. {Huang} and Z. {Xiong} and Q. {Wang} and X. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={KALM: Key Area Localization Mechanism for Abnormality Detection in Musculoskeletal Radiographs},
year={2020},
volume={},
number={},
pages={1399-1403},
abstract={Recently abnormality detection in musculoskeletal radio-graphs has attracted many attentions. For abnormality detection, it is crucial to locate the most important area in the musculoskeletal radiographs. To achieve this goal, we propose a key area localization mechanism (KALM) for abnormality detection for the first time in this paper. The proposed KALM explicitly defines the process of selecting the most important area from the whole image with using only image-level label. Based on KALM, we further present a joint global and local feature representation strategy for abnormality detection which takes as input both the entire image and the selected local area. The experimental results based on several classical convolutional neural network (CNN) architectures of MURA, the largest abnormality detection dataset of musculoskeletal radiographs, demonstrate the effectiveness of our KALM.},
keywords={Key area localization mechanism;abnormality detection;musculoskeletal radiographs;CNN},
doi={10.1109/ICASSP40776.2020.9053768},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052946,
author={H. {Xu} and S. {Geng} and Y. {Qiao} and K. {Xu} and Y. {Gu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Combining CGAN and Mil for Hotspot Segmentation in Bone Scintigraphy},
year={2020},
volume={},
number={},
pages={1404-1408},
abstract={Bone scintigraphy is widely used to diagnose bone tumor and metastasis. Accurate hotspot segmentation from bone scintigraphy is of great importance for tumor metastasis diagnosis. In this paper, we propose a new framework to detect and extract hotspots in thoracic region by integrating the techniques of both conditional generative adversarial networks (cGAN) and multiple instance learning (MIL). We first use cGAN to train a generator, which can be applied to separate input bone scan image into four anatomical regions and provide location information. A multi-dimensional feature is constructed to integrate contrast, texture and location information. We then use MIL to train a patch-level classifier with this constructed feature. In hotspot segmentation, a hotspots probability map can be estimated with the patch-level classifier. The hotspot segmentation is performed with level set method, in which the hotspot boundary is initialized based on the hotspot probability map. We evaluate the proposed framework quantitatively on the hotspot dataset, and compare it with other methods.},
keywords={hotspot segmentation;cGAN;MIL;probability map;level set},
doi={10.1109/ICASSP40776.2020.9052946},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054412,
author={Q. {Zhang} and J. {Zhou} and B. {Zhang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Noninvasive Method to Detect Diabetes Mellitus and Lung Cancer Using the Stacked Sparse autoencoder},
year={2020},
volume={},
number={},
pages={1409-1413},
abstract={Diabetes mellitus and lung cancer are two of the most common fatal diseases in the world, causing considerable deaths every year. However, it is not easy to detect diabetes mellitus and lung cancer efficiently--needing professional medical instruments such as a CT and a qualified individual to perform the Fasting Plasma Glucose test. Considering the risks and various inconveniences with conventional diagnosis methods, noninvasive approaches based on computerized analysis are desired. The aim of this paper is to distinguish patients with diabetes mellitus, lung cancer from healthy people simultaneously by analyzing facial images through the stacked sparse autoencoder. Experimental results on a dataset containing 450 healthy samples, 284 diabetes and 175 lung cancer patients produced the F1-score of 93.57%, 97.54%, 81.56% for detecting healthy, diabetes and lung cancer, respectively, validating the effectiveness of our proposed method.},
keywords={Diabetes mellitus;lung cancer;facial image;medical biometrics;stacked sparse autoencoder},
doi={10.1109/ICASSP40776.2020.9054412},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054030,
author={P. {Guo} and X. {Su} and H. {Zhang} and M. {Wang} and F. {Bao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Multi-Scaled Receptive Field Learning Approach for Medical Image Segmentation},
year={2020},
volume={},
number={},
pages={1414-1418},
abstract={Biomedical image segmentation has been widely studied, and lots of methods have been proposed. Among these methods, attention U-Net has achieved a promising performance. However, it has drawbacks of extracting the multi-scaled receptive field features at the high-level feature maps, resulting in the degeneration when dealing with the lesions with apparent scale variations. To solve this problem, this paper integrates an atrous spatial pyramid pooling (ASPP) module in the contracting path of attention U-Net. This module employs multiple dilation rates for the purpose of obtaining several multi-scale receptive fields, which significantly improves the networks’ ability to handle both large and small lesions. Evaluation experimental result shows that our approach significantly improves the performance of medical image segmentation and substantially outperforms the representative deep learning models on public datasets.},
keywords={biomedical image segmentation;atrous spatial pyramid pooling;attention U-Net;feature maps;receptive field},
doi={10.1109/ICASSP40776.2020.9054030},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053403,
author={T. {Qin} and Z. {Wang} and K. {He} and Y. {Shi} and Y. {Gao} and D. {Shen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Automatic Data Augmentation Via Deep Reinforcement Learning for Effective Kidney Tumor Segmentation},
year={2020},
volume={},
number={},
pages={1419-1423},
abstract={Conventional data augmentation realized by performing simple pre-processing operations (e.g., rotation, crop, etc.) has been validated for its advantage in enhancing the performance for medical image segmentation. However, the data generated by these conventional augmentation methods are random and sometimes harmful to the subsequent segmentation. In this paper, we developed a novel automatic learning-based data augmentation method for medical image segmentation which models the augmentation task as a trial-and-error procedure using deep reinforcement learning (DRL). In our method, we innovatively combine the data augmentation module and the subsequent segmentation module in an end-to-end training manner with a consistent loss. Specifically, the best sequential combination of different basic operations is automatically learned by directly maximizing the performance improvement (i.e., Dice ratio) on the available validation set. We extensively evaluated our method on CT kidney tumor segmentation which validated the promising results of our method.},
keywords={Medical image segmentation;Data augmentation;Deep reinforcement learning},
doi={10.1109/ICASSP40776.2020.9053403},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054505,
author={K. {Mei} and C. {Zhu} and L. {Jiang} and J. {Liu} and Y. {Qiao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cross-Stained Segmentation from Renal Biopsy Images Using Multi-Level Adversarial Learning},
year={2020},
volume={},
number={},
pages={1424-1428},
abstract={Segmentation from renal pathological images is a key step in automatic analyzing the renal histological characteristics. However, the performance of models varies significantly in different types of stained datasets due to the appearance variations. In this paper, we design a robust and flexible model for cross-stained segmentation. It is a novel multi-level deep adversarial network architecture that consists of three sub-networks: (i) a segmentation network; (ii) a pair of multi-level mirrored discriminators for guiding the segmentation network to extract domain-invariant features; (iii) a shape discriminator that is utilized to further identify the output of the segmentation network and the ground truth. Experimental results on glomeruli segmentation from renal biopsy images indicate that our network is able to improve segmentation performance on target type of stained images and use unlabeled data to achieve similar accuracy to labeled data. In addition, this method can be easily applied to other tasks.},
keywords={Segmentation;domain adaptation;multilevel adversarial network;domain-invariant feature},
doi={10.1109/ICASSP40776.2020.9054505},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053554,
author={L. {Yu} and D. {Liu} and H. {Mansour} and P. T. {Boufounos} and Y. {Ma}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Blind Multi-Spectral Image Pan-Sharpening},
year={2020},
volume={},
number={},
pages={1429-1433},
abstract={We address the problem of sharpening low spatial-resolution multi-spectral (MS) images with their associated misaligned high spatial-resolution panchromatic (PAN) image, based on priors on the spatial blur kernel and on the cross-channel relationship. In particular, we formulate the blind pan-sharpening problem within a multi-convex optimization framework using total generalized variation for the blur kernel and local Laplacian prior for the cross-channel relationship. The problem is solved by the alternating direction method of multipliers (ADMM), which alternately updates the blur kernel and sharpens intermediate MS images. Numerical experiments demonstrate that our approach is more robust to large misalignment errors and yields better super resolved MS images compared to state-of-the-art optimization-based and deep-learning-based algorithms.},
keywords={blind image fusion;pan-sharpening;local Laplacian prior;total generalized variation},
doi={10.1109/ICASSP40776.2020.9053554},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053284,
author={A. {Repetti} and Y. {Wiaux}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Forward-Backward Algorithm for Reweighted Procedures: Application to Radio-Astronomical Imaging},
year={2020},
volume={},
number={},
pages={1434-1438},
abstract={During the last decades, reweighted procedures have shown high efficiency in computational imaging. They aim to handle non-convex composite penalization functions by iteratively solving multiple approximated sub-problems. Although the asymptotic behaviour of these methods has recently been investigated in several works, they all necessitate the sub-problems to be solved accurately, which can be sub-optimal in practice. In this work we present a reweighted forward-backward algorithm designed to handle non-convex composite functions. Unlike existing convergence studies in the literature, the weighting procedure is directly included within the iterations, avoiding the need for solving any sub-problem. We show that the obtained reweighted forward-backward algorithm converges to a critical point of the initial objective function. We illustrate the good behaviour of the proposed approach on a Fourier imaging example borrowed to radio-astronomical imaging.},
keywords={Non-convex optimization;forward-backward algorithm;reweighted procedure;Fourier imaging;astronomical imaging},
doi={10.1109/ICASSP40776.2020.9053284},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052998,
author={Z. {Zhang} and K. {Xu} and F. {Ren}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cra: A Generic Compression Ratio Adapter for End-To-End Data-Driven Image Compressive Sensing Reconstruction Frameworks},
year={2020},
volume={},
number={},
pages={1439-1443},
abstract={End-to-end data-driven image compressive sensing reconstruction (EDCSR) frameworks achieve state-of-the-art reconstruction performance in terms of reconstruction speed and accuracy. However, due to their end-to-end nature, existing EDCSR frameworks can not adapt to a variable compression ratio (CR). For applications that desire a variable CR, existing EDCSR frameworks must be trained from scratch at each CR, which is computationally costly and time-consuming. This paper presents a generic compression ratio adapter (CRA) framework that addresses the variable compression ratio (CR) problem for existing EDCSR frameworks with no modification to given reconstruction models nor enormous rounds of training needed. CRA exploits an initial reconstruction network to generate an initial estimate of reconstruction results based on a small portion of the acquired measurements. Subsequently, CRA approximates full measurements for the main reconstruction network by complementing the sensed measurements with resensed initial estimate. Our experiments based on two public image datasets (CIFAR10 and Set5) show that CRA provides an average of 13.02 dB and 5.38 dB PSNR improvement across the CRs from 5 to 30 over a naive zero-padding approach and the AdaptiveNN approach(a prior work), respectively. CRA addresses the fixed-CR limitation of existing EDCSR frameworks and makes them suitable for resource-constrained compressive sensing applications.},
keywords={compressive sensing;neural network;signal processing;estimate resensing},
doi={10.1109/ICASSP40776.2020.9052998},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054460,
author={S. {Yan} and J. {Huang} and N. {Daly} and C. {Higgitt} and P. L. {Dragotti}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Revealing Hidden Drawings in Leonardo’s ‘the Virgin of the Rocks’ from Macro X-Ray Fluorescence Scanning Data through Element Line Localisation},
year={2020},
volume={},
number={},
pages={1444-1448},
abstract={Macro X-Ray Fluorescence (XRF) scanning is an increasingly widely used imaging technique for the non-invasive detection and mapping of chemical elements in Old Master paintings. Existing approaches for XRF signal analysis require varying degrees of expert user input. They are mainly based on peak fitting at fixed energies associated with each element and require the target elements to be selected manually. In this paper, we propose a new method that can process macro XRF scanning data from paintings fully automatically. The method consists of two parts: 1) detecting pulses in an XRF spectrum using Finite Rate of Innovation (FRI) theory; 2) producing the distribution maps for each element automatically identified in the painting. The results presented show the ability of our method to detect weak or partially overlapping signals and more excitingly to have visualisation of underdrawing in a masterpiece by Leonardo da Vinci.},
keywords={Painting;Finite element analysis;Shape;Photonics;X-ray imaging;Detection algorithms;Fluorescence;Signal processing;Macro X-ray Fluorescence scanning;Finite Rate of Innovation;Old Master Paintings},
doi={10.1109/ICASSP40776.2020.9054460},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053170,
author={M. {Zehni} and S. {Huang} and I. {Dokmanić} and Z. {Zhao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={3D Unknown View Tomography Via Rotation Invariants},
year={2020},
volume={},
number={},
pages={1449-1453},
abstract={In this paper, we study the problem of reconstructing a 3D point source model from a set of 2D projections at unknown view angles. Our method obviates the need to recover the projection angles by extracting a set of rotation-invariant features from the noisy projection data. From the features, we reconstruct the density map through a constrained nonconvex optimization. We show that the features have geometric interpretations in the form of radial and pairwise distances of the model. We further perform an ablation study to examine the effect of various parameters on the quality of the estimated features from the projection data. Our results showcase the potential of the proposed method in reconstructing point source models in various noise regimes.},
keywords={3D reconstruction;rotation-invariant features;point-source model;3D tomography;unassigned distance geometry},
doi={10.1109/ICASSP40776.2020.9053170},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053289,
author={O. {Karakuş} and E. E. {Kuruoğlu} and A. {Achim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Modelling Sea Clutter In Sar Images Using Laplace-Rician Distribution},
year={2020},
volume={},
number={},
pages={1454-1458},
abstract={This paper presents a novel statistical model for the characterisation of synthetic aperture radar (SAR) images of the sea surface. The analysis of ocean surface is widely performed using satellite imagery as it produces information for wide areas under various weather conditions. An accurate SAR amplitude distribution model enables better results in despeckling, ship detection/tracking and so forth. In this paper, we develop a new statistical model, namely the LaplaceRician distribution for modelling amplitude SAR images of the sea surface. The proposed statistical model is based on Rician distribution to model the amplitude of a complex SAR signal, the in-phase and quadrature components of which are assumed to be Laplace distributed. The Laplace-Rician model is investigated for SAR images of the sea surface from COSMO-SkyMed and Sentinel-1 in comparison to state-of-the-art statistical models such as $\mathcal{K}$, lognormal and Weibull distributions. In order to decide on the most suitable model, statistical significance analysis via Kullback-Leibler divergence and Kolmogorov-Smirnov statistics is performed. The results show a superior modelling performance of the proposed model for all of the utilised images.},
keywords={Sea clutter modelling;SAR Imaging;Laplace-Rician distribution},
doi={10.1109/ICASSP40776.2020.9053289},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053433,
author={H. {Verinaz-Jadan} and P. {Song} and C. L. {Howe} and A. J. {Foust} and P. L. {Dragotti}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Volume Reconstruction for Light Field Microscopy},
year={2020},
volume={},
number={},
pages={1459-1463},
abstract={Light Field Microscopy (LFM) is a 3D imaging technique that captures volumetric information in a single snapshot. It is appealing in microscopy because of its simple implementation and the peculiarity that it is much faster than methods involving scanning. However, volume reconstruction for LFM suffers from low lateral resolution, high computational cost, and reconstruction artifacts near the native object plane. In this work, we make two contributions. First, we propose a simplification of the forward model based on a novel discretization approach that allows us to accelerate the computation without drastically increasing memory consumption. Second, we experimentally show that by including regularization priors and an appropriate initialization strategy, it is possible to remove the artifacts near the native object plane. The algorithm we use for this is ADMM. Finally, the combination of the two techniques leads to a method that outperforms classic volume reconstruction approaches (variants of Richardson-Lucy) in terms of average computational time and image quality (PSNR).},
keywords={Light Field Microscopy;deconvolution;volume reconstruction;block Toeplitz;discretization},
doi={10.1109/ICASSP40776.2020.9053433},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053765,
author={S. {Chen} and Y. {Chuang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Exposure Fusion with Deghosting via Homography Estimation and Attention Learning},
year={2020},
volume={},
number={},
pages={1464-1468},
abstract={Modern cameras have limited dynamic ranges and often produce images with saturated or dark regions using a single exposure. Although the problem could be addressed by taking multiple images with different exposures, exposure fusion methods need to deal with ghosting artifacts and detail loss caused by camera motion or moving objects. This paper proposes a deep network for exposure fusion. For reducing the potential ghosting problem, our network only takes two images, an underexposed image and an overexposed one. Our network integrates together thew homography estimation for compensating camera motion, the attention mechanism for correcting remaining misalignment and moving pixels, and adversarial learning for alleviating other remaining artifacts. Experiments on real-world photos taken using handheld mobile phones show that the proposed method can generate high-quality images with faithful detail and vivid color rendition in both dark and bright areas.},
keywords={Exposure fusion;deghosting;homography estimation;attention learning;adversarial learning},
doi={10.1109/ICASSP40776.2020.9053765},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054565,
author={M. H. {Conde} and K. {Kagawa} and T. {Kokado} and S. {Kawahito} and O. {Loffeld}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Single-Shot Real-Time Multiple-Path Time-of-Flight Depth Imaging for Multi-Aperture and Macro-Pixel Sensors},
year={2020},
volume={},
number={},
pages={1469-1473},
abstract={Multiple-Path Interference (MPI) is a major drawback of Time-of-Flight (ToF) sensors. Current methods resolving more than a single light bounce per pixel rely on the sequential acquisition of large amounts of data and are computationally expensive. These factors have precluded the development of a real-time multiple-path ToF camera to date. In this work we consider two hardware alternatives able to acquire all necessary raw data in a single shot. We introduce a unified signal processing pipeline that takes the single-shot raw data from any of these sensors as input and delivers an independent depth image per interfering path. We use the measurements to estimate the lowest-frequency Fourier coefficients of the scene response function, which can be ideally modeled as a train of Dirac delta functions. A robust parametric spectral estimation method allows us recovering the unknown parameters (distance and reflectivity of the targets) for all pixels in real time.},
keywords={Time-of-Flight;single-shot;multipath;multi-aperture;macro-pixel},
doi={10.1109/ICASSP40776.2020.9054565},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054547,
author={S. {Gupta} and R. {Gribonval} and L. {Daudet} and I. {Dokmanić}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast Optical System Identification by Numerical Interferometry},
year={2020},
volume={},
number={},
pages={1474-1478},
abstract={We propose a numerical interferometry method for identification of optical multiply-scattering systems when only intensity can be measured. Our method simplifies the calibration of optical transmission matrices from a quadratic to a linear inverse problem by first recovering the phase of the measurements. We show that by carefully designing the probing signals, measurement phase retrieval amounts to a distance geometry problem—a multilateration—in the complex plane. Since multilateration can be formulated as a small linear system which is the same for entire rows of the transmission matrix, the phases can be retrieved very efficiently. To speed up the subsequent estimation of transmission matrices, we design calibration signals so as to take advantage of the fast Fourier transform, achieving a numerical complexity almost linear in the number of transmission matrix entries. We run experiments on real optical hardware and use the numerically computed transmission matrix to recover an unseen image behind a scattering medium. Where the previous state-of-the-art method reports hours to compute the transmission matrix on a GPU, our method takes only a few minutes on a CPU.},
keywords={Phase retrieval;random scattering media;transmission matrix calibration;distance geometry;imaging through scattering media},
doi={10.1109/ICASSP40776.2020.9054547},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053651,
author={F. {Arab} and M. S. {Asif}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fourier Phase Retrieval with Arbitrary Reference Signal},
year={2020},
volume={},
number={},
pages={1479-1483},
abstract={Fourier phase retrieval problem aims at recovering a signal from its Fourier amplitude measurements. A good initialization and prior information about the sparsity or support of the target signal is critical for robust recovery. Holographic phase retrieval is a related problem in which the presence of a reference signal makes the signal recovery problem linear. The existing methods, however, only work if the support of the reference and target signals are sufficiently separated. In this paper, we present a Fourier phase retrieval algorithm in the presence of a known (reference) signal at arbitrary location in the scene. We assume that a small part of the target signal is known without any other assumption about the support and separation of the known and unknown parts of the signal. Our recovery algorithm is based on alternating minimization and gradient descent. We demonstrate that our proposed method significantly improves the Fourier phase retrieval for natural images and synthetic images with multiple objects.},
keywords={Side information;holographic phase retrieval;alternating minimization},
doi={10.1109/ICASSP40776.2020.9053651},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053414,
author={Z. {Tong} and J. {Wang} and S. {Han}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Preconditioned Ghost Imaging Via Sparsity Constraint},
year={2020},
volume={},
number={},
pages={1484-1488},
abstract={Ghost imaging via sparsity constraint (GISC) can recover objects from the intensity fluctuation of light fields at a sampling rate far below the Nyquist rate. However, its imaging quality may degrade severely when the coherence of sampling matrices is large. To deal with this issue, we propose an eﬃcient recovery algorithm for GISC called the preconditioned multiple orthogonal least squares (PmOLS). Our algorithm consists of two major parts: i) the pseudo-inverse preconditioning (PIP) method refining the coherence of sampling matrices and ii) the multiple orthogonal least squares (mOLS) algorithm recovering the objects. Theoretical analysis shows that PmOLS recovers any n-dimensional K-sparse signal from m random linear samples of the signal with probability exceeding $1 - 3{n^2}{e^{ - cm/{K^2}}}$. Simulations and experiments demonstrate that PmOLS has competitive imaging quality compared to the state-of-the-art approaches.},
keywords={Ghost imaging;sparsity;compressive sensing;preconditioning;multiple orthogonal least squares},
doi={10.1109/ICASSP40776.2020.9053414},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054182,
author={K. {Zhou} and C. {Jung}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multispectral Fusion of RGB and NIR Images Using Weighted Least Squares and Alternating Guidance},
year={2020},
volume={},
number={},
pages={1489-1493},
abstract={In low light condition, color (RGB) images captured by camera contain much noise and loss of details and color. However, near infrared (NIR) images are robust to noise and have clear textures without color. In this paper, we propose multi-spectral fusion of RGB and NIR images using weighted least squares (WLS) and alternating guidance. Low light RGB images provide coarse image structure and color, while NIR images offer clear textures in a short distance. Since they are complementary, we adopt alternating guidance for fusion of RGB and NIR images based on WLS. First, we perform the first guidance for denoising on noisy RGB image to get base layer. Then, we conduct the second guidance for texture transfer on NIR image to get detail layer. Finally, we combine base and detail layers to produce a fusion result. We maximize the advantage of multispectral input (RGB and NIR) for fusion based on alternating guidance. Experimental results demonstrate that the proposed method achieves good performance in noise reduction, detail preservation and color reproduction as well as outperforms state-of-the-art ones in terms of quantitative measurement and computational efficiency.},
keywords={Image fusion;alternating guidance;detail transfer;multispectral;weighted least squares},
doi={10.1109/ICASSP40776.2020.9054182},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053559,
author={H. {Nguyen} and C. {Guillemot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Color and Angular Reconstruction of Light Fields from Incomplete-Color Coded Projections},
year={2020},
volume={},
number={},
pages={1494-1498},
abstract={We present a simple variational approach for reconstructing color light fields (LFs) in the compressed sensing (CS) framework with very low sampling ratio, using both coded masks and color filter arrays (CFAs). A coded mask is placed in front of the camera sensor to optically modulate incoming rays, while a CFA is assumed to be implemented at the sensor level to compress color information. Hence, the LF coded projections, operated by a combination of the coded mask and the CFA, measure incomplete color samples with a three-times-lower sampling ratio than reference methods that assume full color (channel-by-channel) acquisition. We then derive adaptive algorithms to directly reconstruct the light field from raw sensor measurements by minimizing a convex energy composed of two terms. The first one is the data fidelity term which takes into account the use of CFAs in the imaging model, and the second one is a regularization term which favors the sparse representation of light fields in a specific transform domain. Experimental results show that the proposed approach produces a better reconstruction both in terms of visual quality and quantitative performance when compared to reference reconstruction methods that implicitly assume prior color interpolation of coded projections.},
keywords={light field;compressed sensing;demosaicing;variational algorithm},
doi={10.1109/ICASSP40776.2020.9053559},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053824,
author={Z. {Li} and J. {Zheng} and S. {Xie} and H. {Shu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cross Image Cubic Interpolator for Spatially Varying Exposures},
year={2020},
volume={},
number={},
pages={1499-1503},
abstract={Spatially varying exposures via rolling shutter is an efficient way to capture differently exposed images for high dynamic range (HDR) scenes. Neither camera movement nor moving objects is an issue for such a captured method. However, a possible issue is that the resolution of captured images is reduced. In this paper, we introduce a novel cross image cubic interpolator for the spatially varying exposures via the rolling shutter. Both intra correlation among pixels with the same exposure and inter correlation among pixels with the different exposures are utilized by the proposed interpolator. Experimental results show that quality of upsampled images is significantly improved.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053824},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054291,
author={S. {Zhao} and B. {Zhang} and S. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Discriminant and Sparsity Based Least Squares Regression with l1 Regularization for Feature Representation},
year={2020},
volume={},
number={},
pages={1504-1508},
abstract={Least squares regression (LSR) has two main issues that greatly limits the improvement of performance: 1) The target matrix is too rigid leading to a large regression error; 2) the underlying geometric structure of the training data is often ignored to learn a more discriminative projection matrix. To solve these dilemmas, this paper presents a discriminant and sparsity based least squares regression with l1 regularization (DS_LSR). In DS_LSR, the sparse coefficient matrix of the training data with l1 regularization is jointly learned with the projection matrix to make the projection matrix discriminative. In addition, an orthogonal relaxed term is introduced to hold the structure of regression targets while relaxing the rigid label matrix. Extensive experimental results demonstrate the effectiveness of the proposed method in classification accuracy.},
keywords={Least squares regression;feature representation;image classification},
doi={10.1109/ICASSP40776.2020.9054291},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053330,
author={F. {Zhang} and Q. {Wang} and X. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Meta-Relation Network for Visual Few-Shot Learning},
year={2020},
volume={},
number={},
pages={1509-1513},
abstract={This paper proposes a novel metric-based deep learning method to solve the few-shot learning problem. It models the relation between images as high dimensional vector, and trains a network module to judge, when given two relational features, which one indicates a stronger connection between the image objects. By training such a network module, we introduce a comparative mechanism into the metric space, i.e., the similarity score of any two images is computed after seeing other images in the same task. Further more, we propose to incorporate a batch classification loss into episodic training to mitigate the hard training problem that occurs when embedding network is going deeper. Experiments demonstrate that the proposed network can achieve promising performance.},
keywords={Few-shot learning;deep learning;metric learning},
doi={10.1109/ICASSP40776.2020.9053330},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053931,
author={L. {NUS} and S. {MIRON} and B. {JAILLAIS} and S. {MOUSSAOUI} and D. {BRIE}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Semi-Supervised Rank Tracking Algorithm For On-Line Unmixing Of Hyperspectral Images},
year={2020},
volume={},
number={},
pages={1514-1518},
abstract={This paper addresses the problem of rank tracking in real time hyperspectral image unmixing. Based on the On-line Alternating Direction Method of Multipliers (ADMM), we propose a new hyperspectral unmixing approach that integrates prior information as well as joint sparsity regularization, allowing to select only the active components on each sample of the image. This results in a semi-supervised algorithm, well adapted for on-line rank tracking for pushbroom imager. Experimental results on synthetic and real data sets demonstrate the effectiveness of our method for parameter estimation and rank change detection.},
keywords={Hyperspectral imaging;Pushbroom imager;On-line semi-supervised unmixing;Alternating Direction Method of Multipliers;Rank tracking.},
doi={10.1109/ICASSP40776.2020.9053931},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053430,
author={M. A. {Lodhi} and Y. {Ma} and H. {Mansour} and P. T. {Boufounos} and D. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Inverse Multiple Scattering with Phaseless Measurements},
year={2020},
volume={},
number={},
pages={1519-1523},
abstract={We study the problem of reconstructing an object from phaseless measurements in the context of inverse multiple scattering. Our formulation explicitly decouples the variables that represent the unknown object image and the unknown phase, respectively, in the forward model. This enables us to simultaneously optimize over both unknowns with appropriate regularization for each. The resulting optimization problem is nonconvex due to the nonlinear propagation model for multiple scattering and the nonconvex regularization of the phase variables. Nevertheless, we demonstrate experimentally that we can solve the optimization problem using a variation of the fast iterative shrinkage-thresholding algorithm (FISTA)—a convex algorithm, popular for its speed and simplicity—that converges well in our experiments. Numerical results with both simulated and experimentally measured data show that the proposed method outperforms the state-of-the-art phaseless inverse scattering method.},
keywords={phaseless inverse scattering;nonconvex optimization;nonlinear forward model;phase retrieval;total variation regularization},
doi={10.1109/ICASSP40776.2020.9053430},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054207,
author={Y. {Cheng} and Z. {Zhao} and Y. {Wang} and Y. {Niu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Polarization Information Fusion for Object Contour Display in Passive Millimeter-Wave and Terahertz Security Imaging},
year={2020},
volume={},
number={},
pages={1524-1528},
abstract={Passive millimeter-wave/terahertz (PMMW/PTW) imaging has been widely developed for personal security screening in recent years. In PMMW/PTW images, object contours are an important feature for object detection and recognition. In this paper, a physical-based contour display method by fusing multi-polarization information is proposed. The polarization characteristics are investigated by analyzing the physical principle of contour edge generation. The feasibility of displaying contour edges is verified through simulation experiments for a typical security check scene. The presented method can enhance the contrast between concealed objects and human body, and then label contour edges.},
keywords={Multi-polarization imaging;millimeterwave;terahertz;information fusion;contour display},
doi={10.1109/ICASSP40776.2020.9054207},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053032,
author={D. {Picone} and A. {Dolet} and S. {Gousset} and D. {Voisin} and M. D. {Mura} and E. {Le Coarer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Characterisation of a Snapshot Fourier Transform Imaging Spectrometer Based on an Array of Fabry-Perot Interferometers},
year={2020},
volume={},
number={},
pages={1529-1533},
abstract={This study focuses on a novel snapshot Fourier Transform imaging spectrometer based on an array of Fabry-Perot interferometers. This device fully relies on signal processing in order to provide intelligible outputs and thus requires a precise characterisation. In this paper, we present a strategy for estimating the thickness of the Fabry-Perot cavities, as this information is typically not precise or even available. This is a fundamental step for obtaining calibrated acquisitions with this device and for allowing further data processing and analysis. The proposed technique relies on the device optical model and has proven effective with respect to alternative strategies when applied to real acquisitions.},
keywords={Inverse problem;Hyperspectral imaging;Sensor calibration;Fourier transform spectroscopy;Computational imaging},
doi={10.1109/ICASSP40776.2020.9053032},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053378,
author={J. {Wang} and Y. {Chuang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Shadow Removal of Text Document Images by Estimating Local and Global Background Colors},
year={2020},
volume={},
number={},
pages={1534-1538},
abstract={This paper proposes a simple yet effective method for removing shadows from text document images. Assuming that the document mainly contains texts, our method estimates the global and local background colors using statistical analysis of the whole image and local neighborhoods. By estimating the global and local background colors, we obtain the shadow map indicating the shadow ratio for each pixel. With the shadow map, a shadow-free image can be recovered by intrinsic decomposition. Experiments confirm that our method effectively removes the shadow of text document images regardless of the intensity, scope, and the number of shadows.},
keywords={Document image shadow removal;document image processing},
doi={10.1109/ICASSP40776.2020.9053378},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053653,
author={H. {Nejatollahi} and S. {Shahhosseini} and R. {Cammarota} and N. {Dutt}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Exploring Energy Efficient Quantum-resistant Signal Processing Using Array Processors},
year={2020},
volume={},
number={},
pages={1539-1543},
abstract={Quantum computers threaten to break public-key cryptography schemes such as DSA and ECDSA in polynomial time, which poses an imminent threat to secure signal processing. Ring learning with error (RLWE) lattice-based cryptography (LBC) is one of the most promising families of post-quantum cryptography (PQC) schemes in terms of efficiency and versatility. Two conventional methods to compute polynomial multiplication, the most compute-intensive routine in the RLWE schemes, are convolutions and Number Theoretic Transform (NTT).In this work, we explore the energy efficiency of polynomial multiplier using systolic architecture for the first time. As an early exploration, we design two high-throughput systolic array polynomial multipliers, including NTT-based and convolution-based, and compare them to our low-cost sequential (non-systolic) NTT-based multiplier. Our sequential NTT-based multiplier achieves 3x speedup over the state-of-the-art FGPA implementation of the polynomial multiplier in the NewHope-Simple key exchange mechanism on a low-cost Artix7 FPGA. When synthesized on a Zynq UltraScale+ FPGA, the NTT-based systolic and convolution-based systolic designs achieve on average 1.7x and 7.5x speedup over our sequential NTT-based multiplier respectively, which can lead to generating over 2x more signatures per second by CRYSTALS-Dilithium, a PQC digital signature scheme. These explorations help designers select the right PQC implementations for making future signal processing applications quantum-resistant.},
keywords={Public Key Cryptography;Lattice-based Cryptography;Acceleration;Number Theoretic Transform;Systolic Array},
doi={10.1109/ICASSP40776.2020.9053653},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054275,
author={S. {Dave} and A. {Shrivastava} and Y. {Kim} and S. {Avancha} and K. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={dMazeRunner: Optimizing Convolutions on Dataflow Accelerators},
year={2020},
volume={},
number={},
pages={1544-1548},
abstract={Convolution neural networks (CNNs) can be efficiently executed on dataflow accelerators. However, the vast space of executing convolutions on computational and memory resources of accelerators makes difficult for programmers to automatically and efficiently accelerate the convolutions and for architects to achieve efficient accelerator designs. We propose dMazeRunner framework, which allows users to optimize execution methods for accelerating convolution and matrix multiplication on a given architecture and to explore dataflow accelerator designs for efficiently executing CNN models. dMazeRunner determines efficient dataflows tailored for CNN layers and achieves efficient execution methods for CNN models within several seconds.},
keywords={Hardware accelerators;energy-efficiency;mapping;deep learning;design space exploration},
doi={10.1109/ICASSP40776.2020.9054275},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054524,
author={A. {Levisse} and M. {Rios} and M. {Peón-Quirós} and D. {Atienza}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Exploration Methodology for BTI-Induced Failures on RRAM-Based Edge AI Systems},
year={2020},
volume={},
number={},
pages={1549-1552},
abstract={Resistive switching memory technologies (RRAM) are seen by most of the scientific community as an enabler for Edge-level applications such as embedded deep Learning, AI or signal processing of audio and video signals. However, going beyond a "simple" replacement of eFlash in micro-controller and introducing RRAM inside the memory hierarchy is not a straightforward move. Indeed, integrating a RRAM technology inside the cache hierarchy requires higher endurance requirement than for eFlash replacement, and thus necessitates relaxed programming conditions. By doing so, the reliability bottleneck is moved from programming to the read operations (i.e., read margin is reduced and the risk of read failure is increased). Based on this observation, in this work, we propose to explore how Edge-level applications running on a RRAM-based Edge device could fail because of Bias Temperature Instability (BTI). BTI causes threshold voltage (Vt) degradation on the transistors along the memory WordLines (WL), leading to a reduction of the read margin along regularly used WLs. We thereby propose a 3-steps methodology consisting in (i) characterizing the RRAM bitcell and identifying beyond which Vt shift the read operation is going to fail. (ii) characterizing applications and extracting the memory traces. And (iii) running a long term BTI simulation to extract the actual Vt shift of the bitcells sharing the same array WordLine. Based on this, we show that for a 1T1R bitcell featuring a 250% High/Low Resistance State (HRS/LRS) ratio, read failures tend to happen after less than a month in the case of a constantly running convolution kernel. These simulations highlight the fact that transistor-level reliability can be critical for embedded RRAM and that specific workload aware simulation frameworks are required to assess their effects.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054524},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054285,
author={V. {Venkataramani} and B. {Bodin} and A. {Kulkarni} and T. {Mitra} and L. {Peh}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Time-Predictable Software-Defined Architecture with Sdf-Based Compiler Flow for 5g Baseband Processing},
year={2020},
volume={},
number={},
pages={1553-1557},
abstract={The advent of 5G networks motivates the need for high-performance, low-power, time-predictable hardware that can handle the aggressive real-time latency and throughput requirements of baseband processing. With newer generations like 5G, programmable hardware that can adapt readily to network specification updates becomes a critical requirement. We introduce a software-defined array-based many-core architecture, called SPECTRUM, that couples lightweight predictable hardware components with a compiler flow that orchestrates the on-chip hardware resources. This design, by construction, provides timing guarantees with a programmable architecture. Our architecture and compiler flow are designed to support basestation baseband processing computation represented using deterministic Synchronous Data Flow (SDF) model of computation. SDF is commonly used to represent signal processing applications and fits well with real-time systems requirements. We demonstrate substantial power savings with SPECTRUM compared to existing DSPs while meeting the performance requirements.},
keywords={Time-predictability;many-core architecture;LTE/5G baseband processing;synchronous dataflow},
doi={10.1109/ICASSP40776.2020.9054285},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054126,
author={A. {Soorishetty} and J. {Zhou} and S. {Pal} and D. {Blaauw} and H. {Kim} and T. {Mudge} and R. {Dreslinski} and C. {Chakrabarti}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Accelerating Linear Algebra Kernels on a Massively Parallel Reconfigurable Architecture},
year={2020},
volume={},
number={},
pages={1558-1562},
abstract={Much of the recent work on domain-specific architectures has focused on bridging the gap between performance/efficiency and programmability. We consider one such example architecture, Transformer, consisting of light-weight cores interconnected by caches and crossbars that supports run-time reconfiguration between shared and private cache mode operations. We present customized implementation of a select set of linear algebra kernels, namely, triangular matrix solver, LU decomposition, QR decomposition and matrix in-version, on Transformer. The performance of the kernel algorithms is evaluated with respect to execution time and energy efficiency. Our study shows that each kernel achieves high performance for a certain cache mode and that this cache mode can change when the matrix size changes, making a case for run-time reconfiguration.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054126},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054613,
author={S. {Das} and R. {Prasad} and K. J. M. {Martin} and P. {Coussy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Energy Efficient Acceleration Of Floating Point Applications Onto CGRA},
year={2020},
volume={},
number={},
pages={1563-1567},
abstract={In this paper, we propose a novel CGRA architecture and associated compilation flow supporting both integer and floating-point computations for energy efficient acceleration of DSP applications. Experimental results show that the proposed accelerator achieves a maximum of 4.61 × speedup compared to a DSP optimized, ultra low power RISC-V based CPU while executing seizure detection, a representative of wide range of EEG signal processing applications with an area overhead of 1.9×. The proposed CGRA achieves a maximum of 6.5× energy efficiency compared to the CPU.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054613},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054076,
author={C. {Wang} and C. {Chiu} and C. {Huang} and Y. {Ding} and L. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast and Accurate Embedded DCNN for Rgb-D Based Sign Language Recognition},
year={2020},
volume={},
number={},
pages={1568-1572},
abstract={In this paper, fast and accurate two paths CNN architecture was designed in hardware-oriented manner. Our proposed network is composed of RGB and depth path for gesture recognition by fusing RGB and depth features, following the pre-defined constraints on dedicated hardware. The RTL simulation results indicate it only takes 0.171 milliseconds to infer a single pair of RGB image and depth maps at the operational frequency of 250MHz. Compared with running the same model at Intel i7 and GTX 1080, the speedups are 593.92x and 7.68x respectively. Besides, to increase the recognition accuracy under the diversity of the circumstance, a new RGB-D dataset, captured from Kinect, with complex background was built. Moreover, the number of parameters in our model is only 0.17M and it achieves 99.79% accuracy on the ASL Finger Spelling dataset. Compared with the Gao’s CNN gesture recognition architecture, the number of parameter of our model is 2.9 times less and the accuracy is 6.49% higher. Demonstration video for sign language recognition is provided : https://youtu.be/DvO8mI7IZ5Q},
keywords={RGB-D multi-modality;Gesture recognition;RGB-D dataset;Hardware oriented DCNN;CNN accer-leration},
doi={10.1109/ICASSP40776.2020.9054076},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053567,
author={W. {Zheng} and V. {Tran} and C. {Huang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={D2NA: Day-To-Night Adaptation for Vision based Parking Management System},
year={2020},
volume={},
number={},
pages={1573-1577},
abstract={Recently, smart parking management systems built on deep learning frameworks have achieved promising performance. However, most of them are designed for the day-time. To help these systems work at night also, extra labor-intensive efforts and extra training time are needed. In this paper, we propose a novel framework based on day-night domain adaptation, feature disentanglement, and style transfer to transfer the knowledge from day to night. The key idea behind our framework is to embed images into two spaces. A domain-invariant space captured shared feature for classification, and a domain-specific space characterized the day or night style. By taking advantage of the exchange of two domains, our framework not only transfers knowledge and labels across domains but also synthesizes the style-transferred images. These features enable our parking lot system to detect the status of spaces at night time in a more efficient way. Experimental results show the effectiveness of our framework for day-to-night adaptation regarding status classification. It also shows visually pleasing results after image-to-image translation.},
keywords={Night-time Parking Lots;Domain Adaptation;Feature Disentanglement;Day-to-Night Image Translation.},
doi={10.1109/ICASSP40776.2020.9053567},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054342,
author={M. {Kaselimi} and A. {Voulodimos} and E. {Protopapadakis} and N. {Doulamis} and A. {Doulamis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={EnerGAN: A GENERATIVE ADVERSARIAL NETWORK FOR ENERGY DISAGGREGATION},
year={2020},
volume={},
number={},
pages={1578-1582},
abstract={An efficient, appliance-level approach for energy disaggregation, exploiting the benefits of Generative Adversarial Networks, is presented. The concept of adversarial training supports the creation of fine tuned dissagregators, which produce more detailed load estimations for a specific appliance, compared to state of the art deep learning models. The Generator and Discriminator of the model are appropriately adapted to fit the particularities of NILM problem, whereas a Seeder component is added to provide encoded compact input vectors to the Generator. The experimental evaluation against state of the art techniques indicates promising results.},
keywords={Generative Adversarial Networks;Generative models;Load sequences;NILM;Energy Disaggregation},
doi={10.1109/ICASSP40776.2020.9054342},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053625,
author={G. {Castel-Branco} and G. {Falcao} and F. {Perdigão}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Enhancing the Labelling of Audio Samples for Automatic Instrument Classification Based on Neural Networks},
year={2020},
volume={},
number={},
pages={1583-1587},
abstract={The polyphonic OpenMIC-2018 dataset is based on weak and incomplete labels. The automatic classification of sound events, based on the VGGish bottleneck layer as proposed before by the AudioSet, implies the classification of only one second at a time, making it hard to find the label of that exact moment. To answer this question, this paper proposes PureMic, a new strongly labelled dataset (SLD) that isolates 1000 single instrument clips manually labelled. Moreover, the proposed model classifies clips over time and also enhances the labelling robustness of a high number of unlabelled samples in OpenMIC-2018 due to its ability of classification over time. In the paper we disambiguate and report the automatic labelling of previously unlabelled samples. Our proposed new labels achieves a mean average precision (mAP) of 0.701 for OpenMIC test data, outperforming its baseline (0.66). We released our code online in order to follow the proposed implementation 1.},
keywords={OpenMIC-2018;AudioSet;PureMic;Instrument classification;Instrument labelling;Deep learning},
doi={10.1109/ICASSP40776.2020.9053625},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054161,
author={S. {Hu} and W. {Hu} and D. {Kapetanovic} and N. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep-Neural-Network Based Fall-Back Mechanism in Interference-Aware Receiver Design},
year={2020},
volume={},
number={},
pages={1588-1892},
abstract={In this paper, we consider designing a fall-back mechanism in an interference-aware receiver. Typically, there are two types of detectors dealing with interference, known as enhanced interference rejection combining (eIRC) and symbol-level interference cancellation (SLIC). Although a SLIC detector performs better, it yields a higher complexity than an eIRC detector. Further, it requires knowledge of interference modulation-format (MF). Due to potential detection errors, SLIC can run with a wrong interference MF and render unsatisfying results. Therefore, designing a mechanism that runs SLIC when the interference MF is reliable and otherwise switches to eIRC is of particular interest, which we call a "fall-back mechanism". Finding an optimal mechanism is difficult and we use deep-neural-network (DNN) for design, which is more effective than a traditional Bayes-risk mini-mization based approach.},
keywords={Deep-neural-network (DNN);symbol-level interference cancellation (SLIC);enhanced interference rejection combining (eIRC);modulation format (MF)},
doi={10.1109/ICASSP40776.2020.9054161},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053977,
author={Y. {Zhao} and C. {Li} and Y. {Wang} and P. {Xu} and Y. {Zhang} and Y. {Lin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={DNN-Chip Predictor: An Analytical Performance Predictor for DNN Accelerators with Various Dataflows and Hardware Architectures},
year={2020},
volume={},
number={},
pages={1593-1597},
abstract={The recent breakthroughs in deep neural networks (DNNs) have spurred a tremendously increased demand for DNN accelerators. However, designing DNN accelerators is non-trivial as it often takes months/years and requires cross-disciplinary knowledge. To enable fast and effective DNN accelerator development, we propose DNN-Chip Predictor, an analytical performance predictor which can accurately predict DNN accelerators’ energy, throughput, and latency prior to their actual implementation. Our Predictor features two highlights: (1) its analytical performance formulation of DNN ASIC/FPGA accelerators facilitates fast design space exploration and optimization; and (2) it supports DNN accelerators with different algorithm-to-hardware mapping methods (i.e., dataflows) and hardware architectures. Experiment results based on 2 DNN models and 3 different ASIC/FPGA implementations show that our DNN-Chip Predictor’s predicted performance differs from those of chip measurements of FPGA/ASIC implementation by no more than 17.66% when using different DNN models, hardware architectures, and dataflows. We will release code upon acceptance.},
keywords={DNN accelerator;ASIC;FPGA;design simulator;design automation},
doi={10.1109/ICASSP40776.2020.9053977},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054094,
author={H. {Dbouk} and H. {Geng} and C. M. {Vineyard} and N. R. {Shanbhag}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low-Complexity Fixed-Point Convolutional Neural Networks For Automatic Target Recognition},
year={2020},
volume={},
number={},
pages={1598-1602},
abstract={There has been growing interest in developing neural network based automatic target recognition systems for synthetic aperture radar applications. However, these networks are typically complex in terms of storage and computation which inhibits their deployment in the field, where such resources are heavily constrained. In order to bring the cost of implementing these networks down, we develop a set of compact network architectures and train them in fixed-point. Our proposed method achieves an overall 984 reduction in terms of storage requirements and 71 × reduction in terms of computational complexity compared to state-of-the-art con-volutional neural networks for automatic target recognition (ATR), while maintaining a classification accuracy of > 99% on the MSTAR dataset.},
keywords={deep learning;neural networks;automatic target recognition;synthetic aperture radar;quantization},
doi={10.1109/ICASSP40776.2020.9054094},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054164,
author={J. {Guo} and W. {Liu} and W. {Wang} and J. {Han} and R. {Li} and Y. {Lu} and S. {Hu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Accelerating Distributed Deep Learning By Adaptive Gradient Quantization},
year={2020},
volume={},
number={},
pages={1603-1607},
abstract={To accelerate distributed deep learning, gradient quantization technique is widely used to reduce the communication cost. However, the existing quantization schemes suffer from either model accuracy degradation or low compression ratio (arisen from a redundant setting of quantization level or high overhead in determining the level). In this work, we propose a novel adaptive quantization scheme (AdaQS) to explore the balance between model accuracy and quantization level. AdaQS determines the quantization level automatically according to gradient’s mean to standard deviation ratio (MSDR). Then, to reduce the quantization overhead, we employ a computationally-friendly way of moment estimation to calculate the MSDR. Finally, theoretical analysis of AdaQS’s convergence is conducted for non-convex objectives. Experiments demonstrate that AdaQS performs excellently on very deep model GoogleNet with 2.55% accuracy improvement relative to vanilla SGD and achieves 1.8x end-to-end speedup on AlexNet in a distributed cluster with 4*4 GPUs.},
keywords={Communication;acceleration;gradient;quantization},
doi={10.1109/ICASSP40776.2020.9054164},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054764,
author={A. T. {Kristensen} and R. {Giterman} and A. {Balatsoukas-Stimming} and A. {Burg}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Lupulus: A Flexible Hardware Accelerator for Neural Networks},
year={2020},
volume={},
number={},
pages={1608-1612},
abstract={Neural networks have become indispensable for a wide range of applications, but they suffer from high computationaland memory-requirements, requiring optimizations from the algorithmic description of the network to the hardware implementation. Moreover, the high rate of innovation in machine learning makes it important that hardware implementations provide a high level of programmability to support current and future requirements of neural networks. In this work, we present a flexible hardware accelerator for neural networks, called Lupulus, supporting various methods for scheduling and mapping of operations onto the accelerator. Lupulus was implemented in a 28nm FD-SOI technology and demonstrates a peak performance of 380GOPS/GHz with latencies of 21.4ms and 183.6ms for the convolutional layers of AlexNet and VGG-16, respectively.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054764},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054657,
author={W. {Lo} and C. {Chiu} and J. {Luo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Depth Estimation From Single Image Through Multi-Path-Multi-Rate Diverse Feature Extractor},
year={2020},
volume={},
number={},
pages={1613-1617},
abstract={Convolutional neural networks can effectively learn features and predict the depth by considering different scene types. However, previous studies have not accurately predicted the depth in cases wherein the objects or scenes were small and the background was complex. These studies have used the bilinear up-sampling method to enlarge the feature maps during training, or to disable the transfer of multiscale information to the end of the network. However, this has resulted in blurred regions in the depth maps and contour loss.This paper proposes a multi-path-multi-rate feature extractor, which can effectively extract multi-scale information to make accurate depth predictions. We used the U-NET [1] architecture to obtain depth maps with high resolution, and also used the proposed multi-path-multi-rate feature extractor to translate useful features from the encoder to the decoder. Dilated convolutions with different rates can provide different types of field-of-view information, which increases the precision of depth estimation and maintains the object contours. Finally, we conducted experiments using an indoor scene (NYUv2 [2]). The results show that the proposed framework achieved an improvement of 12.9% in RMSE, 9.9% in REL, and 9.3% in log10, and it requires approximately 0.048 seconds to predict a depth map from a single image.},
keywords={Depth estimation;Convolutional neural networks;Dilated convolution;Multi-path-multi-rate feature extractor},
doi={10.1109/ICASSP40776.2020.9054657},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054009,
author={J. {Lin} and C. -. T. {Chiu} and Y. {Cheng}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Object Detection with Color and Depth Images with Multi-Reduced Region Proposal Network and Multi-Pooling},
year={2020},
volume={},
number={},
pages={1618-1622},
abstract={Object detection technology has received increasing research attention with recent developments in automation technology. Most studies in this field, however, use RGB images as input to deep-learning classifiers, and they rarely use depth information.So, in this paper, we use images with both RGB and depth information as input to an object detection network. We base our network on the Faster R-CNN proposed by Shih et al., and we develop a fast and accurate object detection architecture. In addition to adding depth as input, we also adjust the type of anchor boxes to improve performance on some objects. We also discuss the impact of pooling training data with multiple region proposal networks (RPN) and regions of interest (ROI).Adding depth information improved the mAP by 8.15%, from 36.86% to 45.01%, when using the SUN RGB-D dataset with 10 classes. Optimizing the anchor boxes improved the mAP from 45.01% to 45.88%. After testing various architectures with different reduced RPNs, we find that the model of 1RRPN-2ROIP performs best. The running time is 0.123 s, which is 1.8 times faster than the 3D-SSD model.},
keywords={RGB-D Object Detection;Region Proposal Network (RPN);Region of Interest (ROI) Pooling;anchor box},
doi={10.1109/ICASSP40776.2020.9054009},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053784,
author={C. {Yang} and L. {Chang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deblurring And Super-Resolution Using Deep Gated Fusion Attention Networks For Face Images},
year={2020},
volume={},
number={},
pages={1623-1627},
abstract={Image deblurring and super-resolution are very important in image processing such as face verification. However, when in the outdoors, we often get blurry and low resolution images. To solve the problem, we propose a deep gated fusion attention network (DGFAN) to generate a high resolution image without blurring artifacts. We extract features from two task-independent structures for deburring and super-resolution to avoid the error propagation in the cascade structure of deblurring and super-resolution. We also add an attention module in our network by using channel-wise and spatial-wise features for better features and propose an edge loss function to make the model focus on facial features like eyes and nose. DGFAN performs favorably against the state-of-arts methods in terms of PSNR and SSIM. Also, using the clear images generated by DGFAN can improve the accuracy on face verification.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053784},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053106,
author={Y. {Fan} and F. {Zhang} and C. {Wu} and B. {Wang} and K. J. {Ray Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Indoor Heading Direction Estimation Using Rf Signals},
year={2020},
volume={},
number={},
pages={1628-1632},
abstract={Heading direction information is crucial to many ubiquitous computing applications. The main stream has been resorting to inertial sensors, such as accelerometer, gyroscope and magnetometer, which suffer from severe accumulative errors or large degradations indoors. In this paper, we utilize the radio frequency (RF) signals, received from the commercial off-the-shelf (COTS) WiFi devices, to accurately estimate the heading direction in indoor environments. Based on the time- reversal (TR) technique, we make use of the channel state information (CSI) and the geometry of the antenna array to design the proposed algorithm. A prototype is built using a single access point (AP), without knowing its location, and a two dimensional (2D) antenna array to validate the proposed method. Experiments, conducted in strong non-line-of-sight (NLOS) scenarios with rich multipaths indoors, have shown that the median error for heading direction estimation is 6.9°, which surpasses the inertial sensors. With the high accuracy and low cost, it illustrates the proposed system as a promising solution to large varieties of applications that require accurate heading direction information.},
keywords={Heading direction;time-reversal (TR);CSI},
doi={10.1109/ICASSP40776.2020.9053106},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054452,
author={S. {Wen} and W. {Gan} and D. {Shi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Improved Selective Active Noise Control Algorithm Based on Empirical Wavelet Transform},
year={2020},
volume={},
number={},
pages={1633-1637},
abstract={The gradual adaptation and possibility of divergence have been the two main obstacles in the efficient implementation of conventional adaptive active noise control (ANC) to a wider range of applications. Selective ANC (SANC) has been proposed to rapidly reduce noise by selecting a pre-trained control filter for different primary noise detected, and improve the robustness of the system. For stationary noise, considerable noise reduction performance and system stability are obtained by SANC. However, for non-stationary noise, in order to track the variability of the signal, frequency-band-match and selection have to be conducted constantly, which results in high computational burden. To confront this problem, empirical wavelet transform (EWT) is introduced to simplify the matching and selection of SANC in this paper. This EWT based SANC (SANC_EWT) algorithm extracts the first mode of random noises, and attenuates the noise immediately by picking the optimal pre-trained control filter labeled by the first boundary. Therefore, computational complexity is reduced drastically. Simulation results show that convergence could be reached rapidly. Better noise reduction performance is achieved by SANC_EWT compared to conventional FxLMS and SANC algorithms.},
keywords={Active noise control;nonstationary;selective control;computational complexity;empirical wavelet transform},
doi={10.1109/ICASSP40776.2020.9054452},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054391,
author={J. {Ke} and A. J. {Watras} and J. {Kim} and H. {Liu} and H. {Jiang} and Y. H. {Hu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Towards Real-Time, Multi-View Video Stereopsis},
year={2020},
volume={},
number={},
pages={1638-1642},
abstract={We present a real-time, multi-view video stereopsis (RTMVS) algorithm. This algorithm processes five synchronized video streams from cameras of a stationary camera array using a commodity laptop computer equipped with an Nvidia GPU. It provides 3D visualization of a dynamic scene from a chosen viewpoint at the video frame rate. In RTMVS, 3D surfaces are represented as a set of triangles anchored on a sparse set of 3D feature points. The computationally intensive Structure-from-Motion (SfM) algorithm is executed as an initial step. Feature points in each video stream are tracked using a KLT tracker. Triangles will be updated only when at least one vertex moves from its current position. The algorithm will redetect features every X frame. Epipolar geometry and Trifocal tensor are also exploited to accelerate sparse feature point matching and track filtering. Compared to a dense point cloud multi-view stereopsis baseline algorithm, RTMVS reduces the processing time per frame from 30s down to less than 44 ms.},
keywords={real-time implementation;multi-view visualization;3D reconstruction and rendering;Structure-from-Motion},
doi={10.1109/ICASSP40776.2020.9054391},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054521,
author={C. {Huang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Ernet Family: Hardware-Oriented Cnn Models For Computational Imaging Using Block-Based Inference},
year={2020},
volume={},
number={},
pages={1643-1647},
abstract={Convolutional neural networks (CNNs) demand huge DRAM bandwidth for computational imaging tasks, and block-based processing has recently been applied to greatly reduce the bandwidth. However, the induced additional computation for feature recomputing or the large SRAM for feature reusing will degrade the performance or even forbid the usage of state-of-the-art models. In this paper, we address these issues by considering the overheads and hardware constraints in advance when constructing CNNs. We investigate a novel model family—ERNet—which includes temporary layer expansion as another means for increasing model capacity. We analyze three ERNet variants in terms of hardware requirement and introduce a hardware-aware model optimization procedure. Evaluations on Full HD and 4K UHD applications will be given to show the effectiveness in terms of image quality, pixel throughput, and SRAM usage. The results also show that, for block-based inference, ERNet can outperform the state-of-the-art FFDNet and EDSR-baseline models for image denoising and super-resolution respectively.},
keywords={Convolutional neural network;computational imaging;block-based inference;ultra-high-definition},
doi={10.1109/ICASSP40776.2020.9054521},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053352,
author={G. {Georgis} and A. {Thanos} and M. {Filo} and K. {Nikitopoulos}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A DSP Acceleration Framework For Software-Defined Radios On X86 64},
year={2020},
volume={},
number={},
pages={1648-1652},
abstract={This paper presents a DSP acceleration and assessment framework targeting SDR platforms on x86_64 architectures. Driven by the potential of rapid prototyping and evaluation of breakthrough concepts that these platforms provide, our work builds upon the well-known OpenAirInterface codebase, extending it for advanced, previously unsupported modes towards large and massive MIMO such as non-codebook-based multi-user transmissions. We then develop an acceleration/profiling framework, through which we present finegrained execution results for DSP operations. Incorporating the latest SIMD instructions, our acceleration framework achieves a unitary speedup of up to 10×. Integrated into OpenAirInterface, it accelerates computationally expensive MIMO operations by up to 88% across tested modes. Besides resulting in a useful tool for the community, this work provides insight on runtime DSP complexity and the potential of modern x86_64 systems.},
keywords={SDRs;parallel processing;SIMD;MIMO},
doi={10.1109/ICASSP40776.2020.9053352},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053607,
author={C. {Hsu} and C. {Chiu} and C. {Kuan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast Single-View 3D Object Reconstruction with Fine Details Through Dilated Downsample and Multi-Path Upsample Deep Neural Network},
year={2020},
volume={},
number={},
pages={1653-1657},
abstract={Three-dimensional (3D) object reconstruction is among the most important research areas in the field of computer vision. Its purpose is to reconstruct the overall shape of an object from its twodimensional (2D) image. With the development of deep learning, many methods based on convolutional neural networks (CNNs) have been applied in related research.To achieve 3D shape reconstruction with low computation time, we focus on the commonly used method: single-image reconstruction. The main issue of using a single image as an input is that the reconstruction shape often lacks structural detail. To address this issue, we proposed two methods: the dilated downsample block and the multi-path upsample block. The dilated downsample block extracts more features and the multi-path upsample block uses the features in our architecture. Thereafter, we concatenate the encoder and decoder with corresponding layers to keep the image features in reconstruction process.Finally, we perform experiments on the dataset provided by Choy et al. Results show that our method achieves 67.7% intersection over-union (IoU) accuracy, 3.6% higher than state-of-the-art method, VTN. Compared to the PSVH method, our result achieves 71.4%, an increase of 3.4%. Our average reconstruction time is 13 ms, approximately 25 times faster than PSVH.},
keywords={3D object reconstruction;3D shape reconstruction;deep convolutional neural network;single view},
doi={10.1109/ICASSP40776.2020.9053607},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054326,
author={J. {Vieira} and N. {Roma} and G. {Falcao} and P. {Tomás}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Processing Convolutional Neural Networks on Cache},
year={2020},
volume={},
number={},
pages={1658-1662},
abstract={With the advent of Big Data application domains, several Machine Learning (ML) signal-processing algorithms, such as Convolutional Neural Networks (CNNs), are required to process progressively larger datasets at a great cost in terms of both compute power and memory bandwidth. Although dedicated accelerators have been developed targeting this issue, they usually require moving massive amounts of data across the memory hierarchy to the processing cores and low-level knowledge of how data is stored in the memory devices to enable in-/near-memory processing solutions. In this paper, we propose and assess a novel mechanism that operates at cache level, leveraging both data-proximity and parallel processing capabilities, enabled by dedicated fully-digital vector Functional Units (FUs). We also demonstrate the integration of this mechanism in a conventional Central Processing Unit (CPU). The obtained results show that our engine provides performance improvements on CNNs ranging from 3.92× to 16.6×.},
keywords={CNNs;SIMD;Near-cache processing},
doi={10.1109/ICASSP40776.2020.9054326},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054281,
author={I. {Farhat} and W. {Hamidouche} and A. {Grill} and D. {Menard} and O. {Déforges}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Lightweight Hardware Implementation of VVC Transform Block for ASIC Decoder},
year={2020},
volume={},
number={},
pages={1663-1667},
abstract={Versatile Video Coding (VVC) is the next generation video coding standard expected by the end of 2020. Compared to its predecessor, VVC introduces new coding tools to make compression more efficient at the expense of higher computational complexity. This rises a need to design an efficient and optimised implementation especially for embedded platforms with limited memory and logic resources. One of the newly introduced tools in VVC is the Multiple Transform Selection (MTS). This latter involves three Discrete Cosine Transform (DCT)/Discrete Sine Transform (DST) types with larger and rectangular transform blocks. In this paper, an efficient hardware implementation of all DCT/DST transform types and sizes is proposed. The proposed design uses 32 multipliers in a pipelined architecture which targets an ASIC platform. It consists in a multi-standard architecture that supports the transform block of recent MPEG standards including AVC, HEVC and VVC. The architecture is optimized and removes unnecessary complexities found in other proposed architectures by using regular multipliers instead of multiple constant multipliers. The synthesized results show that the proposed method which sustain a constant throughput of two pixels/cycle and constant latency for all block sizes can reach an operational frequency of 600 Mhz enabling to decode in real-time 4K videos at 48 fps.},
keywords={VVC;Multiple Transform Selection;Hardware implementation;ASIC;cross-standard implementation},
doi={10.1109/ICASSP40776.2020.9054281},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053502,
author={T. {Lin} and C. {Chiu} and C. {Tang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Rgb-D Based Multi-Modal Deep Learning for Face Identification},
year={2020},
volume={},
number={},
pages={1668-1672},
abstract={In recent years, the rapid development of depth cameras and wide application scenarios. The depth image information becomes more influential in face identification. In the proposed architecture, we implement the networks in dual CNN paths for color and depth images separately. Moreover, we design innovative loss functions to strengthen the discrimination and the complementary features between color and depth modalities. To preserve the strengthened color and depth features, we fuse both features by concatenation before classification. The experimental results show that our multi-modal learning method achieve 4.3381% EER, 0.27 FMR1000, and 0.33 ZeroFMR on IIIT-D Kinect RGB-D Face dataset for face verification and 99.7% classification accuracy, which exceeds the most state-of-the-art methods. Moreover, the global descriptors of model output are designed to be binarized. Our method requires less memory and computation time.},
keywords={face verification;deep convolutional neural networks;multi-modality;RGB-D recognition},
doi={10.1109/ICASSP40776.2020.9053502},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054525,
author={H. {Lee} and E. {Heo} and W. {Lee} and D. {Ahn} and J. {Cheon} and K. {Kim} and K. {Lee} and J. {Lee} and Y. {Choi} and S. {Chang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Real Time Implementation of a Bayer Domain Image Deblurring Core for Optical Blur Compensation},
year={2020},
volume={},
number={},
pages={1673-1677},
abstract={In this letter, we present an implementation of deblurring hardware to mitigate blur incurred by optical aberrations in a real-time manner to increase resolution for mobile camera modules. As optical aberrations tend to be variant according to spatial location, algorithm should support spatially variant deblurring. Also, a deblurring problem is known to be rank-deficient, thus requires a regularization and a sparse norm such as Lp norm where 0 ≤ p ≤ 1 has achieved satisfactory solutions. We focus on elaborating algorithm details and a corresponding architecture to efficiently implement with those two requirements. Also, we present a simulation result with the real sensor and this result shows that image resolution can be increased in terms of Modulated Transfer Function (MTF).},
keywords={Deblurring;spatially varying blur;optical aberration;deblurring implementation},
doi={10.1109/ICASSP40776.2020.9054525},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054274,
author={S. {Lin} and W. {Su} and P. {Chien} and M. {Tsai} and C. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Self-Attentive Sentimental Sentence Embedding for Sentiment Analysis},
year={2020},
volume={},
number={},
pages={1678-1682},
abstract={We propose the use of a word-level sentiment bidirectional LSTM in tandem with the self-attention mechanism for sentence-level sentiment prediction. In addition to the proposed model, we also present a finance report dataset for sentence-level financial risk detection. Experiments conducted on the proposed dataset together with two public review datasets attest the effectiveness of our model for sentence sentiment prediction.},
keywords={Financial NLP;Sentiment Analysis;Self-Attention},
doi={10.1109/ICASSP40776.2020.9054274},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054053,
author={Y. {Ma} and J. {Wu} and S. S. {Bhattacharyya} and J. {Boutellier}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Decidable Variable-Rate Dataflow for Heterogeneous Signal Processing Systems},
year={2020},
volume={},
number={},
pages={1683-1687},
abstract={Dynamic dataflow models of computation have become widely used through their adoption to popular programming frameworks such as TensorFlow and GNU Radio. Although dynamic dataflow models offer more programming freedom, they lack analyzability compared to their static counterparts (such as synchronous dataflow).In this paper we advocate the use of a boundedly dynamic dataflow model of computation, VR-PRUNE, that remains analyzable but still offers more programming freedom than a fully static dataflow model.The paper presents the VR-PRUNE model of computation and runtime, and illustrates its applicability to practical signal processing applications by two use cases: an adaptive convolutional neural network, and a predistortion filter for wireless communications. By runtime experiments on two heterogeneous computing platforms we show that VR-PRUNE is both flexible and efficient.},
keywords={Computational modeling;Runtime;Signal processing;Adaptation models;Dynamic programming;Programming;Analytical models;variable-rate dataflow;models of computation;signal processing;heterogeneous computing},
doi={10.1109/ICASSP40776.2020.9054053},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053364,
author={H. {Harb} and C. {Chavet}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Back-to-Back Butterfly Network, an Adaptive Permutation Network for New Communication Standards},
year={2020},
volume={},
number={},
pages={1688-1692},
abstract={In this paper, we introduce an adaptive Back-to-Back Butterfly Network (B2BN) dedicated to next communication standards. It can perform any kind of permutation, and its architecture is based on a concatenation of basic networks. However for a set of permutations, the selection of non-conflicting paths is still a complex task. Hence, in our paper we rely on the properties of the proposed architecture, to introduce a formal model that efficiently solve such conflicts. From the collection of all possible paths in the targeted B2BN, the proposed method select the conflict-free paths to transfer data from the input to their permuted output (w.r.t., a ad hoc constraints model). Once the appropriate paths are selected, the control signals for B2BN are generated. This model has been experimented with 5G communication, showing how to process several frames in parallel with different permutation constraints.},
keywords={Butterfly Network;Back-to-Back Butterfly Network;Permutation;QC-LDPC;5G},
doi={10.1109/ICASSP40776.2020.9053364},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053282,
author={O. {Ferraz} and V. {Silva} and G. {Falcao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={1.5GBIT/S 4.9W Hyperspectral Image Encoders on a Low-Power Parallel Heterogeneous Processing Platform},
year={2020},
volume={},
number={},
pages={1693-1697},
abstract={This work explores the utilization of low-power heterogeneous devices for parallelizing the compute-intensive hyper-spectral and multispectral image compression CCSDS-123 entropy encoders. Multithread processing allows for the near-optimal system’s bandwidth to be exploited increasing the system overall performance. The experimental platform consists of a low-power Jetson TX2 GPU equipped with an ARM Cortex-A57 and Denver 2 host processors, reporting more than 1552 Mb/s and, more importantly, 315 Mb/s/W, all running under a global 5 W power budget, which makes it a good candidate for onboard image compression.},
keywords={Low Power Graphics Processing Units;Parallel Programming;Multispectral Image Compression;Hyperspectral Image Compression;Lossless Compression},
doi={10.1109/ICASSP40776.2020.9053282},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053809,
author={J. {Lin} and P. {Tsai}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Design of A Convergence-Aware Based Expectation Propagation Algorithm for Uplink Mimo Scma Systems},
year={2020},
volume={},
number={},
pages={1698-1702},
abstract={Sparse code multiple access (SCMA) uses multidimensional sparse codewords to transmit user data. The expectation propagation algorithm (EPA) exploiting the sparse property shows linear complexity growth and thus is preferred for multi-user detection. To further reduce the complexity, a convergence-aware based EPA for uplink MIMO SCMA systems is proposed. Techniques including user termination, antenna termination, and codebook reduction are adopted. The user termination must be combined with the iteration constraint to avoid misjudgement. The antenna termination can stop the computations related with certain antennas having strong channel gains. Only possible codewords are considered in the reduced codebook to eliminate unnecessary calculation for posterior probability. From simulation results, we show that these three techniques can strike a balance between performance and complexity and more than 50% complexity can be saved.},
keywords={Expectation propagation algorithm (EPA);Sparse code multiple access (SCMA);Uplink MIMO},
doi={10.1109/ICASSP40776.2020.9053809},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053183,
author={Z. {Gong} and Y. {Shen} and H. {Ji} and W. {Song} and Z. {Zhang} and X. {You} and C. {Zhang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Bipartite Belief Propagation Polar Decoding With Bit-Flipping},
year={2020},
volume={},
number={},
pages={1703-1707},
abstract={For the scenarios with high throughput requirements, the belief propagation (BP) decoding is one of the most promising decoding strategies for polar codes. By pruning the redundant variable nodes (VNs) and check nodes (CNs) in the original factor graph, the graph is condensed to a sparse bipartite graph which is similar to the graph for low-density parity-check (LDPC) codes. In this paper, we introduce the bit-flipping scheme into the LDPC-like BP (L-BP) decoding and propose two methods to identify the error-prone VNs. By additional decoding attempts, the L-BP flip (L-BPF) decoding improves the error-rate performance with a similar average complexity for high Eb=N0 values. The simulation results show that the L-BPF decoding achieves 0:25 dB gain compared with the L-BP decoding.},
keywords={Polar codes;belief propagation;bit-flipping;factor graph},
doi={10.1109/ICASSP40776.2020.9053183},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053222,
author={C. {Chen} and C. {Teng} and A. A. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low-Complexity LSTM-Assisted Bit-Flipping Algorithm For Successive Cancellation List Polar Decoder},
year={2020},
volume={},
number={},
pages={1708-1712},
abstract={Polar codes have attracted much attention in the past decade due to their capacity-achieving performance. The higher decoding capacity is required for 5G and beyond 5G (B5G). Although the cyclic redundancy check (CRC)- assisted successive cancellation list bit-flipping (CA-SCLF) decoders have been developed to obtain a better performance, the solution to error bit correction (bitflipping) problem is still imperfect and hard to design. In this work, we leverage expert knowledge in communication systems and adopt deep learning (DL) techniques to obtain a better solution. A low-complexity long short-term memory network (LSTM)-assisted CASCLF decoder is proposed to further improve the performance of conventional CA-SCLF and avoid complexity and memory overhead. Our test results show that we can effectively improve the BLER performance by 0.11dB compared to prior work and reduce the complexity and memory overhead by over 30% of the network.},
keywords={Polar codes;successive cancellation list;bit flipping;long short-term memory network},
doi={10.1109/ICASSP40776.2020.9053222},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054321,
author={N. {Passalis} and A. {Tefas} and J. {Kanniainen} and M. {Gabbouj} and A. {Iosifidis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adaptive Normalization for Forecasting Limit Order Book Data Using Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={1713-1717},
abstract={Deep learning models are capable of achieving state-of-the-art performance on a wide range of time series analysis tasks. However, their performance crucially depends on the employed normalization scheme, while they are usually unable to efficiently handle non-stationary features without first appropriately pre-processing them. These limitations impact the performance of deep learning models, especially when used for forecasting financial time series, due to their non-stationary and multimodal nature. In this paper we propose a data-driven adaptive normalization layer which is capable of learning the most appropriate normalization scheme that should be applied on the data. To this end, the proposed method first identifies the distribution from which the data were generated and then it dynamically shifts and scales them in order to facilitate the task at hand. The proposed nor-malization scheme is fully differentiable and it is trained in an end-to-end fashion along with the rest of the parameters of the model. The proposed method leads to significant performance improvements over several competitive normalization approaches, as demonstrated using a large-scale limit order book dataset.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054321},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053007,
author={Y. {Zhao} and K. {Kang} and H. {Qian} and X. {Luo} and Y. {Jin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Greedy Hybrid Rate Adaptation in Dynamic Wireless Communication Environment},
year={2020},
volume={},
number={},
pages={1718-1722},
abstract={High data throughput is desired in the wireless communication system design. Rate adaptation is an efficient way to update the data rate in the dynamic wireless environment. Conventional rate adaptation algorithms rely on the feedback of acknowledgment/negative acknowledgment (ACK/NACK) messages or signal to noise ratio (SNR). Existing rate adaptation algorithms can not achieve satisfactory transmission rates in time-varying environments. In this paper, we model the rate selection problem as a multi-armed bandit (MAB) problem and propose an online learning rate adaptation algorithm that learns the channel status from both RSSI and ACK/NACK signals. Compared with existing rate adaptation algorithms, the proposed algorithm can adapt to the time-varying channel better and achieve near-optimal transmission rate performance.},
keywords={Rate adaptation;reinforcement learning;multi-armed bandit;time-varying channel},
doi={10.1109/ICASSP40776.2020.9053007},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054753,
author={Y. {Hu} and F. {Zhang} and C. {Wu} and B. {Wang} and K. J. {Ray Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A WiFi-Based Passive Fall Detection System},
year={2020},
volume={},
number={},
pages={1723-1727},
abstract={Fall detection systems based on WiFi signals are gaining popularity recently. However, most of the existing works relying on training are environment-dependent. In this paper, we propose DeFall, a novel WiFi-based environment-independent fall detection system by leveraging the features inherently associated with human falls — the patterns of speed and acceleration over time. The system consists of an offline template-generating stage and an online decision-making stage. In the offline stage, the speed of human falls is first estimated based on a statistical modeling about the Channel State Information (CSI). Dynamic Time Warping (DTW) based algorithms are applied to generate a representative template for typical human falls. Then fall event is detected in the online stage by evaluating the similarity between the patterns of realtime speed/acceleration estimates and the representative template. Extensive experiment results show that with a single pair of WiFi transceivers, the proposed system can achieve a detection rate of 96% and a false alarm rate smaller than 1.5% under both line-of-sight (LOS) and non-LOS (NLOS) scenarios.},
keywords={WiFi;Channel State Information;Fall Detection;Dynamic Time Warping},
doi={10.1109/ICASSP40776.2020.9054753},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053796,
author={Y. {Wu} and P. {Wang} and J. {McAllister}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Programmable Dataflow Accelerators: A 5G OFDM Modulation/Demodulation Case Study},
year={2020},
volume={},
number={},
pages={1728-1732},
abstract={Via OFDM technology, FFT and Inverse FFT (IFFT) operators enable the latest 5G radio standards. In these latests standards, the behaviour of FFT and IFFT needs to be flexible, supporting sub-carrier spacings from 15kHz to 480kHz and point sizes of up to 4096 point. An FFT or IFFT accelerator for 5G can take any configuration inside this spectrum both at design time, or potentially at run-time, under the control of a system control plane. This necessitates accelerators which combine high levels of flexibility and performance. This paper describes an FFT accelerator for such a context. Specifically, a novel data-driven programmable softcore processor is presented which enables run-time variable workloads and respond to data as provided by control processors in an MPSoC operating architecture. It is the first such accelerator to enable real-time IFFT/FFT for 5G, providing up to 3.89 times greater data rate than comparable accelerators.},
keywords={Field Programmable Gate Array (FPGA);Orthogonal Frequency-Division Multiplexing (OFDM);Fast Fourier Transform (FFT)},
doi={10.1109/ICASSP40776.2020.9053796},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052925,
author={F. {Ercan} and T. {Tonnellier} and N. {Doan} and W. J. {Gross}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Simplified Dynamic SC-Flip Polar Decoding},
year={2020},
volume={},
number={},
pages={1733-1737},
abstract={SC-Flip (SCF) decoding is a low-complexity polar code decoding algorithm alternative to SC-List (SCL) algorithm with small list sizes. To achieve the performance of the SCL algorithm with large list sizes, the Dynamic SC-Flip (DSCF) algorithm was proposed. However, DSCF involves logarithmic and exponential computations that are not suitable for practical hardware implementations. In this work, we propose a simple approximation that replaces the transcendental computations of DSCF decoding. Moreover, we show how to incorporate fast decoding techniques with the DSCF algorithm. With proposed approaches, the computational complexity of DSCF decoding is remarkably reduced while maintaining equivalent decoding performance.},
keywords={Polar codes;5G;SC-Flip decoding},
doi={10.1109/ICASSP40776.2020.9052925},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053747,
author={Y. {Xie} and C. {Shi} and Z. {Li} and J. {Liu} and Y. {Chen} and B. {Yuan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Real-Time, Universal, and Robust Adversarial Attacks Against Speaker Recognition Systems},
year={2020},
volume={},
number={},
pages={1738-1742},
abstract={As the popularity of voice user interface (VUI) exploded in recent years, speaker recognition system has emerged as an important medium of identifying a speaker in many security-required applications and services. In this paper, we propose the first real-time, universal, and robust adversarial attack against the state-of-the-art deep neural network (DNN) based speaker recognition system. Through adding an audio-agnostic universal perturbation on arbitrary enrolled speaker’s voice input, the DNN-based speaker recognition system would identify the speaker as any target (i.e., adversary-desired) speaker label. In addition, we improve the robustness of our attack by modeling the sound distortions caused by the physical over-the-air propagation through estimating room impulse response (RIR). Experiment using a public dataset of 109 English speakers demonstrates the effectiveness and robustness of our proposed attack with a high attack success rate of over 90%. The attack launching time also achieves a 100× speedup over contemporary non-universal attacks.},
keywords={speaker recognition systems;adversarial examples;universal adversarial attack},
doi={10.1109/ICASSP40776.2020.9053747},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054588,
author={A. A. {Lazar} and T. {Liu} and C. {Yeh}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Odorant Encoding Machine for Sampling, Reconstruction and Robust Representation of Odorant Identity},
year={2020},
volume={},
number={},
pages={1743-1747},
abstract={Despite recent advances in the understanding of olfactory signal processing [1], [2], [3], [4], [5], robust odorant sensing in complex environments with time-varying odorant identities and concentrations remains an open problem. Particularly, the operational principles of biological and biomimetic olfactory sensors define a new class of sampling problems in which the odorant identity and intensity are multiplicatively coupled into a volatile signal format. We solve the sampling problem by developing the Odorant Encoding Machine (OEM), a biomimetic system based on the latest insights in the architectural organization of the fruit fly early olfactory system. The OEM provides event-driven sensing, reconstruction and robust representation of odorant identity as a combinatorial code of multidimensional spike trains. Like its biological counterpart, OEM 1) decouples odorant identity and concentration encoding via a predictive coding circuit, 2) enables real-time responses to changing odorant input through an on-off circuit, and 3) provides robust representation of odorant identity with a real-time hashing circuit. Furthermore, the OEM is directly applicable for future in silico implementations.},
keywords={sampling;event-driven;biomimetic;early olfactory system;silicon nose},
doi={10.1109/ICASSP40776.2020.9054588},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054246,
author={S. {Huang} and W. {Chen} and C. {Huang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={FIR Filter Design and Implementation for Phase-Based Processing},
year={2020},
volume={},
number={},
pages={1748-1752},
abstract={Complex steerable pyramid (CSP) is widely used to decompose images into muti-scale and oriented subbands for phase-based processing, such as video magnification, frame interpolation, and view synthesis. The conventional implementation is based on frequency-domain bandpass filtering which relies on fast Fourier transform (FFT). However, FFT requires high-precision computation and complex memory access for hardware implementation. In this paper, we study computation- and memory-efficient finite impulse response filter implementation for CSP. We use Kaiser windowing for filter designs and adopt 9-tap radial and 11-tap angular filters which can achieve 38.6 dB of PSNR for frame interpolation. We then discuss about VLSI architecture designs and, in particular, propose a stripe-based computation flow for 2-D CSP to reduce the line buffer size down to 15%. For evaluation, we implement two VLSI circuits using TSMC 40nm technology. One is a 1-D CSP engine working at 4K UHD 30 fps, and it saves 67.8% of logic gates compared to a FFT-based design. The other is a 2-D CSP engine working at FHD 60 fps. It uses 32-KB SRAM and 3.5M-gate logic. We also implement a FPGA system for 2-D CSP engine, and it operates at 80 MHz and provides 1024×1024 resolution video at 16 fps.},
keywords={Complex steerable pyramid;finite impulse response filter;phase-based processing;VLSI architecture;FPGA},
doi={10.1109/ICASSP40776.2020.9054246},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054724,
author={Y. {Boo} and W. {Sung}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fixed-Point Optimization of Transformer Neural Network},
year={2020},
volume={},
number={},
pages={1753-1757},
abstract={The Transformer model adopts a self-attention structure and shows very good performance in various natural language processing tasks. However, it is difficult to implement the Transformer in embedded systems because of its very large model size. In this study, we quantize the parameters and hidden signals of the Transformer for complexity reduction. Not only matrices for weights and embedding but the input and the softmax outputs are also quantized to utilize low-precision matrix multiplication. The fixed-point optimization steps consist of quantization sensitivity analysis, hardware conscious word-length assignment, quantization and retraining, and post-training for improved generalization. We achieved 27.51 BLEU score on the WMT English-to-German translation task with 4-bit weights and 6-bit hidden signals.},
keywords={Deep neural network;Transformer model;network quantization},
doi={10.1109/ICASSP40776.2020.9054724},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053228,
author={V. {Panchbhaiyye} and T. {Ogunfunmi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Fifo Based Accelerator for Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={1758-1762},
abstract={In recent years, Deep Neural Networks (DNNs) have achieved state-of-the-art results in various fields like Computer Vision, Natural Language Processing and Speech Recognition. Of all the DNN architectures, Convolutional Neural Networks (CNNs) have been most effective in tasks like image classification and object detection. The high performance of the CNNs comes at the cost of computational complexity. Currently Graphics Processing Units (GPUs) are used to accelerate CNN training and inference on workstations and data servers. Though popular, GPUs are not suitable for embedded applications because they are not energy efficient. ASIC and FPGA accelerators have the potential to run CNNs that are optimized for energy and performance.In this paper we present an architecture which takes a novel approach to compute convolution results using row-wise inputs as opposed to traditional tile-based processing. We are able to exceed the results of state of the art architectures when implemented on an inexpensive PYNQ Z1 board running at 100Mhz. The total latency to run the convolution layers in the VGG16 benchmark is nearly 1.5x lower for our architecture than state of the art architectures.},
keywords={Convolution Neural Networks;FPGA;Dataflow Processing;Hardware Implementation;Machine Learning},
doi={10.1109/ICASSP40776.2020.9053228},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053097,
author={O. {Castañeda} and S. {Jacobsson} and G. {Durisi} and T. {Goldstein} and C. {Studer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Soft-Output Finite Alphabet Equalization for mmWave Massive MIMO},
year={2020},
volume={},
number={},
pages={1763-1767},
abstract={Nxt-generation wireless systems are expected to combine millimeter-wave (mmWave) and massive multi-user multiple-input multiple-output (MU-MIMO) technologies to deliver high data-rates. These technologies require the basestations (BSs) to process high-dimensional data at extreme rates, which results in high power dissipation and system costs. Finite-alphabet equalization has been proposed recently to reduce the power consumption and silicon area of uplink spatial equalization circuitry at the BS by coarsely quantizing the equalization matrix. In this work, we improve upon finite-alphabet equalization by performing unbiased estimation and soft-output computation for coded systems. By simulating a massive MU-MIMO system that uses orthogonal frequency-division multiplexing and per-user convolutional coding, we show that soft-output finite-alphabet equalization delivers competitive error-rate performance using only 1 to 3 bits per entry of the equalization matrix, even for challenging mmWave channels.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053097},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053677,
author={Y. {Zheng} and T. M. {Hospedales} and Y. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Diversity and Sparsity: A New Perspective on Index Tracking},
year={2020},
volume={},
number={},
pages={1768-1772},
abstract={We address the problem of partial index tracking, replicating a benchmark index using a small number of assets. Accurate tracking with a sparse portfolio is extensively studied as a classic finance problem. However in practice, a tracking portfolio must also be diverse in order to minimise risk-a requirement which has only been dealt with by ad-hoc methods before. We introduce the first index tracking method that explicitly optimises both diversity and sparsity in a single joint framework. Diversity is realised by a regulariser based on pairwise similarity of assets, and we demonstrate that learning similarity from data can outperform some existing heuristics. Finally, we show that the way we model diversity leads to an easy solution for sparsity, allowing both constraints to be optimised easily and efficiently. we run out-of-sample back-testing for a long interval of 15 years (2003-2018), and the results demonstrate the superiority of the proposed algorithm.},
keywords={Index tracking;portfolio optimisation},
doi={10.1109/ICASSP40776.2020.9053677},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052952,
author={S. H. {Mirfarshbafan} and C. {Studer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sparse Beamspace Equalization for Massive MU-MIMO MMWave Systems},
year={2020},
volume={},
number={},
pages={1773-1777},
abstract={We propose equalization-based data detection algorithms for all-digital millimeter-wave (mmWave) massive multiuser multiple-input multiple-out (MU-MIMO) systems that exploit sparsity in the beamspace domain to reduce complexity. We provide a condition on the number of users, basestation antennas, and channel sparsity for which beamspace equalization can be less complex than conventional antenna-domain processing. We evaluate the performance-complexity trade-offs of existing and new beamspace equalization algorithms using simulations with realistic mmWave channel models. Our results reveal that one of our proposed beamspace equalization algorithms achieves up to 8× complexity reduction under line-of-sight conditions, assuming a sufficiently large number of transmissions within the channel coherence interval.},
keywords={},
doi={10.1109/ICASSP40776.2020.9052952},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053612,
author={J. {Syu} and M. {Wu} and C. {Chen} and J. {Ho}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Threshold-Adjusted ORB Strategies with Genetic Algorithm and Protective Closing Strategy on Taiwan Futures Market},
year={2020},
volume={},
number={},
pages={1778-1782},
abstract={Opening range breakout (ORB) is a well-known intraday trading strategy that generates trading signals through technical analysis; however, ORB does not make full use of market characteristics and does not define closing strategy. These problems make the ORB strategy not stable or robust enough. In this paper, we adjust thresholds through historical data to enhance profitability, and design protective closing strategy to prevent unacceptable losses. However, there are numerous parameters combinations, and the solution space is approximately 214. Therefore, we implement genetic algorithm to improve the efficiency and rationality of parameter selection. We found that the performance of the GAORB_Sharpe with stop-loss mechanism is outstanding. The strategy we proposed can generate 9.303% annual return and 15.716% Sharpe ratio, which is 2.5% and 5% more than the original strategy, and cut the maximum drawdown by half. Further, it can save 90% of the computation by genetic algorithm. In summary, we recommend adjusting the threshold and implementing stop-loss mechanism to the ORB strategy, and selecting parameters through genetic algorithms to improve overall performance.},
keywords={ORB;opening range breakout;genetic algorithm;closing strategy;artificial intelligence},
doi={10.1109/ICASSP40776.2020.9053612},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054494,
author={H. {Boche} and V. {Pohl}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Can every analog system be simulated on a digital computer?},
year={2020},
volume={},
number={},
pages={1783-1787},
abstract={A Turing machine is a model describing the fundamental limits of any realizable computer, digital signal processor (DSP), or field programmable gate array (FPGA). This paper shows that there exist very simple linear time-invariant (LTI) systems which can not be simulated on a Turing machine. In particular, this paper considers the linear system described by the voltage-current relation of an ideal capacitor. For this system, it is shown that there exist continuously differentiable and computable input signals such that the output signal is a continuous function which is not computable. Moreover, for this particular system, we present sharp results characterizing computable input signals which guarantee that the output signal is computable. Additionally, it is shown that the computability of the step response of an LTI system does not necessarily imply that the impulse response is computable.},
keywords={Linear time-invariant systems;stable systems;computability;Turing machines},
doi={10.1109/ICASSP40776.2020.9054494},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053866,
author={Y. {Pua} and C. {Chou} and A. A. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low-Complexity Compressed Alignment-Aided Compressive Analysis for Real-Time Electrocardiography Telemonitoring},
year={2020},
volume={},
number={},
pages={1788-1792},
abstract={In order to implement a real-time electrocardiogram (ECG) telemonitoring, compressed sensing (CS) is a new technology that reduces the power consumption of biosensors and data transmission. Unfortunately, limited label data and computing resources hinder the real-time ECG telemonitoring. Prior experiments have shown that aligning ECG signals is a good way to solve the problem of limited label data. However, the reconstructed learning (RL) framework requires a lot of computing resources, and the compressed learning (CL) framework makes alignment difficult. In this paper, we propose a new compressed alignment-aided compressive analysis (CA-CA) framework that enables simple alignment and low-complexity requirements. From simulation results, we have demonstrated that our technology can maintain more than 95% accuracy while reducing training data (labeled data) by 70%. Therefore, compared to RL, the computation time and memory overhead of CA-CA are reduced by 6.6 times and 2.45 times, respectively. Compared with CL, the inference accuracy with a small amount of labeled data is improved by 13.5%.},
keywords={Atrial fibrillation detection;real-time ECG telemonitoring;compressive sensing;compressive analysis;compressed alignment},
doi={10.1109/ICASSP40776.2020.9053866},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054313,
author={X. {Hu} and C. {Deng} and B. {Yuan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Reduced-Complexity Singular Value Decomposition For Tucker Decomposition: Algorithm And Hardware},
year={2020},
volume={},
number={},
pages={1793-1797},
abstract={Tensors, as the multidimensional generalization of matrices, are naturally suited for representing and processing high-dimensional data. To date, tensors have been widely adopted in various data-intensive applications, such as machine learning and big data analysis. However, due to the inherent large-size characteristics of tensors, tensor algorithms, as the approaches that synthesize, transform or decompose tensors, are very computation and storage expensive, thereby hindering the potential further adoptions of tensors in many application scenarios, especially on the resource-constrained hardware platforms. In this paper, we propose a reduced-complexity SVD (Singular Vector Decomposition) scheme, which serves as the key operation in Tucker decomposition. By using iterative self-multiplication, the proposed scheme can significantly reduce the storage and computational costs of SVD, thereby reducing the complexity of the overall process. Then, corresponding hardware architecture is developed with 28nm CMOS technology. Our synthesized design can achieve 102GOPS with 1.09 mm2 area and 37.6 mW power consumption, and thereby providing a promising solution for accelerating Tucker decomposition.},
keywords={Tucker Decomposition;SVD;hardware architecture},
doi={10.1109/ICASSP40776.2020.9054313},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053566,
author={H. {Lee} and Y. {Pao} and C. {Chi} and H. {Lee} and Y. {Ueng}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Early Termination Scheme for Successive Cancellation List Decoding of Polar Codes},
year={2020},
volume={},
number={},
pages={1798-1802},
abstract={In order to minimize the decoding period and the response time for Polar Codes, an early termination (ET) scheme based on additional check points (ACPs) is proposed in this work. For conventional ET schemes based on distributed parity-check (PC) bits, ET can only be triggered when the decoding process reaches the PC bits. The ACPs are selected from the information bits, and extend the feature of the PC bits, where ET can also be triggered at the ACPs, meaning that a more rapid ET is available that does not impact the error-rate performance. With sophisticated method of selecting ACPs based on the channel-independent polarization weight (PW), the proposed ET scheme is able to reduce the response time by about 5},
keywords={Polar codes;successive cancellation list decoding;early termination},
doi={10.1109/ICASSP40776.2020.9053566},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053054,
author={J. {Park} and X. {Qian} and Y. {Jo} and W. {Sung}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low-Latency Lightweight Streaming Speech Recognition with 8-Bit Quantized Simple Gated Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={1803-1807},
abstract={Automatic speech recognition (ASR) is very important for mobile devices. However, deep neural network-based ASR demands a large number of computations, while the memory bandwidth and battery capacity of mobile devices are limited. Server-based implementations are mostly employed, but this increases latency or privacy concerns. Efficient on-device ASR is the solution for these issues. In this paper, we propose a low-latency on-device speech recognition system with a simple gated convolutional network (SGCN). The SGCN shows a competitive recognition accuracy even with 1M parameters. In addition, SGCN is advantageous for parallelization which enables efficient cache utilization. 8-bit quantization is applied to reduce the memory size and computation time. The proposed system features online recognition fulfilling the 0.4s latency limit and operates with the real-time factor of 0.2 using only a single 900MHz CPU core. The system occupying 1.2MB memory footprint shows 19.75% word error rate (WER) with greedy decoding.},
keywords={On-device speech recognition;Convolutional neural networks},
doi={10.1109/ICASSP40776.2020.9053054},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053365,
author={G. {Elhami} and A. J. {Scholefield} and M. {Vetterli}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Shape From Bandwidth: Central Projection Case},
year={2020},
volume={},
number={},
pages={1808-1812},
abstract={Consider an unknown surface painted with a band-limited texture. We show that only the knowledge of the bandwidth of the texture is enough to estimate the shape of the surface from a single image taken by a camera. We model the problem as a central projection operation in 2D and propose a two step approach for finding the surface from the observed image. Our proposed algorithm first estimates the arc-length of the surface using the texture bandwidth and then reconstructs the surface from the estimated arc-length. We evaluate our algorithm with a simulation as well as real experiments.},
keywords={shape from bandwidth;surface reconstruction;central projection},
doi={10.1109/ICASSP40776.2020.9053365},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052969,
author={X. {Xue} and Y. {Ding} and P. {Mu} and L. {Ma} and R. {Liu} and X. {Fan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sequential Deep Unrolling With Flow Priors For Robust Video Deraining},
year={2020},
volume={},
number={},
pages={1813-1817},
abstract={Video deraining has attracted wide attention since the urgent demand of high-quality video in recent years. The indistinct details and nonideal deraining effects are the most common defects in existing techniques, whose cause lies in the insufficient usage of single-frame image and temporal information. To effectively settle video deraining, we establish a new deraining model with flow priors to simultaneously introduce spatial and temporal information for accurately depicting the enhancement model of the current frame. A sequential deep unrolling framework is substantially presented by solving this model based on optimization techniques. The ablation study indicates our effectiveness as far as the design of architecture. Plenty of subjective and objective evaluations fully demonstrate our superiority in detail recovery and deraining effects against other state-of-the-are video deraining approaches.},
keywords={Video Deraining;Deep Unrolling;Optical Flow;Video Restoration},
doi={10.1109/ICASSP40776.2020.9052969},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053890,
author={H. {Liu} and Z. {Lu} and W. {Shi} and J. {Tu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Fast and Accurate Super-Resolution Network Using Progressive Residual Learning},
year={2020},
volume={},
number={},
pages={1818-1822},
abstract={Single-image super-resolution (SISR) task has witnessed great strides in the past few years with the development of deep learning. However, most existing studies concentrate on exploiting much deeper super-resolution networks, which are not friendly to the constrained computation resources. In this work, a lightweight network using progressive residual learning for SISR (PRLSR) is proposed to address this issue. Specifically, a progressive residual block (PRB) is designed to progressively downsample deep features for reducing the redundancy and obtaining refined features. Simultaneously, a high-frequency preserving module is proposed to lower the detail loss caused by resolution reduction in PRB. Furthermore, a residual learning-based architecture with learnable weights is utilized to extract multilevel features and adaptively adjust the contribution of residual mapping and identity mapping in residual structure to accelerate convergence. Experimental results on four benchmarks show that our PRLSR achieves superior performance over state-of-the-art methods with a significantly decreased computational cost.},
keywords={Single Image Super Resolution;Residual Learning;Lightweight Network},
doi={10.1109/ICASSP40776.2020.9053890},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053434,
author={S. {Li} and Z. {Zheng} and W. {Dai} and J. {Zou} and H. {Xiong}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={REV-AE: A Learned Frame Set for Image Reconstruction},
year={2020},
volume={},
number={},
pages={1823-1827},
abstract={Reversible residual network naturally extends the linear lifting scheme with no theoretic guarantee. In this paper, we propose a reversible autoencoder (Rev-AE) with this extended non-linear lifting scheme to improve image reconstruction. Nonlinear prediction and update operators are designed based on shallow convolutional neural networks to model multilayer non-linearities. Different from existing autoencoders, Rev-AE support efficient image reconstruction with parameters reusable for the symmetric encoder and decoder. Rev-AE forms a set of related frames to guarantee perfect reconstruction with the non-linear extension of classic lifting scheme. Lower and upper bounds are developed for the set of frames to relate with the singular values for each non-linear operator. Furthermore, we employ Rev-AE into lossy image compression to evaluate its effectiveness on image reconstruction. Experimental results show that Rev-AE achieves competitive performance in comparison to the state-of-the-art.},
keywords={Lifting scheme;frame theory;image reconstruction;image compression},
doi={10.1109/ICASSP40776.2020.9053434},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053123,
author={K. {Han} and X. {Xiang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Decomposed Cyclegan for Single Image Deraining With Unpaired Data},
year={2020},
volume={},
number={},
pages={1828-1832},
abstract={Most previous learning-based methods required paired rain image data. In practice, however, paired rain data cannot be collected. Inspired by adopting unpaired data in task of translation, in this paper we present a new method for rain removal using unpaired data. We noticed that direct use of unpaired training data may have problems, such as color shifts and background blurs. Thus, we formulate DCycleGAN, a new deep framework that decomposes the input rain image into the foreground and background parts, then produces a rain mask to guide the rain generation via re-formulated cycle-consistency constraints. Particular, the framework can simultaneously learn the foreground and background portions of the rain image, which can better remove the rain streak. Experimental results demonstrate the effectiveness of our method when trained on unpaired data.},
keywords={Image derainning;Unpaired data;Decomposed;Rain mask;Cycle-consistency},
doi={10.1109/ICASSP40776.2020.9053123},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054674,
author={Y. {Wu} and Z. {Sun} and Y. {Song} and Y. {Sun} and J. {Shi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Slicenet: Slice-Wise 3D Shapes Reconstruction from Single Image},
year={2020},
volume={},
number={},
pages={1833-1837},
abstract={3D object reconstruction from a single image is a highly ill-posed problem, requiring strong prior knowledge of 3D shapes. Deep learning methods are popular for this task. Especially, most works utilized 3D deconvolution to generate 3D shapes. However, the resolution of results is limited by the high resource consumption of 3D deconvolution. In this paper, we propose SliceNet, sequentially generating 2D slices of 3D shapes with shared 2D deconvolution parameters. To capture relations between slices, the RNN is also introduced. Our model has three main advantages: First, the introduction of RNN allows the CNN to focus more on local geometry details,improving the results’ fine-grained plausibility. Second, replacing 3D deconvolution with 2D deconvolution reducs much consumption of memory, enabling higher resolution of final results. Third, an slice-aware attention mechanism is designed to provide dynamic information for each slice’s generation, which helps modeling the difference between multiple slices, making the learning process easier. Experiments on both synthesized data and real data illustrate the effectiveness of our method.},
keywords={Image-based 3D Reconstruction;RNN;Slice-Aware Attention Model},
doi={10.1109/ICASSP40776.2020.9054674},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053115,
author={A. S. {Koepke} and O. {Wiles} and Y. {Moses} and A. {Zisserman}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sight to Sound: An End-to-End Approach for Visual Piano Transcription},
year={2020},
volume={},
number={},
pages={1838-1842},
abstract={Automatic music transcription has primarily focused on transcribing audio to a symbolic music representation (e.g. MIDI or sheet music). However, audio-only approaches often struggle with polyphonic instruments and background noise. In contrast, visual information (e.g. a video of an instrument being played) does not have such ambiguities. In this work, we address the problem of transcribing piano music from visual data alone. We propose an end-to-end deep learning framework that learns to automatically predict note onset events given a video of a person playing the piano. From this, we are able to transcribe the played music in the form of MIDI data. We find that our approach is surprisingly effective in a variety of complex situations, particularly those in which music transcription from audio alone is impossible. We also show that combining audio and video data can improve the transcription obtained from each modality alone.},
keywords={visual music transcription;automatic music transcription;music information retrieval;deep learning},
doi={10.1109/ICASSP40776.2020.9053115},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053957,
author={G. {Liu} and H. {Tang} and H. {Latapie} and Y. {Yan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Exocentric to Egocentric Image Generation Via Parallel Generative Adversarial Network},
year={2020},
volume={},
number={},
pages={1843-1847},
abstract={Cross-view image generation has been recently proposed to generate images of one view from another dramatically different view. In this paper, we investigate exocentric (third-person) view to egocentric (first-person) view image generation. This is a challenging task since egocentric view sometimes is remarkably different from exocentric view. Thus, transforming the appearances across the two views is a nontrivial task. To this end, we propose a novel Parallel Generative Adversarial Network (P-GAN) with a novel cross-cycle loss to learn the shared information for generating egocentric images from exocentric view. We also incorporate a novel contextual feature loss in the learning procedure to capture the contextual information in images. Extensive experiments on the Exo-Ego datasets [1] show that our model outperforms the state-of-the-art approaches.},
keywords={Egocentric;Exocentric;Cross-View Image Generation;Parallel GANs},
doi={10.1109/ICASSP40776.2020.9053957},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054768,
author={T. {Han} and J. {Gao} and Y. {Yuan} and Q. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Focus on Semantic Consistency for Cross-Domain Crowd Understanding},
year={2020},
volume={},
number={},
pages={1848-1852},
abstract={For pixel-level crowd understanding, it is time-consuming and laborious in data collection and annotation. Some domain adaptation algorithms try to liberate it by training models with synthetic data, and the results in some recent works have proved the feasibility. However, we found that a mass of estimation errors in the background areas impede the performance of the existing methods. In this paper, we propose a domain adaptation method to eliminate it. According to the semantic consistency, a similar distribution in deep layer’s features of the synthetic and real-world crowd area, we first introduce a semantic extractor to effectively distinguish crowd and background in high-level semantic information. Besides, to further enhance the adapted model, we adopt adversarial learning to align features in the semantic space. Experiments on three representative real datasets show that the proposed domain adaptation scheme achieves the state-of-the-art for cross-domain counting problems.},
keywords={Crowd counting;domain adaptation;semantic consistency;adversarial learning},
doi={10.1109/ICASSP40776.2020.9054768},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053709,
author={H. {Zhong} and X. {Yan} and Y. {Jiang} and S. {Xia}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improved Real-Time Visual Tracking via Adversarial Learning},
year={2020},
volume={},
number={},
pages={1853-1857},
abstract={The high accuracy and fast running speed are two goals of tracking algorithms. In recent years, many trackers based on deep learning have improved in both aspects respectively, but it is difficult to balance these two indicators. For example, a real-time tracking algorithm named RT-MDNet has greatly increased speed on the MDNet, but the accuracy is still limited to some extent. On the contrary, a state-of-the-art visual tracker based on adversarial learning named VITAL achieves a significant improvement in performance by alleviating the imbalance between positive and negative samples of tracking data. However, its running speed is seriously limited. In this paper, we attempt to combine the advantages from both methods and propose an improved real-time visual tracking algorithm via adversarial learning to get a more balanced result in accuracy and tracking speed. Specifically, we base on the framework of RT-MDNet and introduce a random feature map masking with adversarial learning to improve the quality of feature maps. Experiments on OTB2015 show our algorithm runs at 17 FPS with a precision of 87.6%.},
keywords={Visual tracking;Deep learning;Adversarial learning;Real-time tracking},
doi={10.1109/ICASSP40776.2020.9053709},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054080,
author={Z. {Chen} and W. {Li} and C. {Fei} and B. {Liu} and N. {Yu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Spatial-Temporal Feature Aggregation Network For Video Object Detection},
year={2020},
volume={},
number={},
pages={1858-1862},
abstract={Video object detection is a challenging problem in computer vision. In this paper, we propose a novel spatial-temporal feature aggregation network to deal with this issue. Specifically, we present a novel instance-level feature aggregation module as complementary to traditional pixel-level feature aggregation, in which we build a new movement estimation module to learn instance movements across frames. Then the Graph Convolutional Networks (GCNs) is applied to obtain temporal relation among instances over frames to implement instance-level feature aggregation. At last, we combine pixel-level and instance-level features by learnable soft weights to make use of their complementary information. Our framework is simple to implement and enables end-to-end training, which achieves state-of-art performance on the ImageNet VID dataset by extensive experiments.},
keywords={Video Object Detection;Feature Aggregation;Pixel-Level;Instance-Level},
doi={10.1109/ICASSP40776.2020.9054080},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053497,
author={F. {Yang} and F. {Li} and Y. {Wu} and S. {Sakti} and S. {Nakamura}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Using Panoramic Videos for Multi-Person Localization and Tracking In A 3D Panoramic Coordinate},
year={2020},
volume={},
number={},
pages={1863-1867},
abstract={3D panoramic multi-person localization and tracking are prominent in many applications, however, conventional methods using LiDAR equipment could be economically expensive and also computationally inefficient due to the processing of point cloud data. In this work, we propose an effective and efficient approach at a low cost. First, we obtain panoramic videos with four normal cameras. Then, we transform human locations from a 2D panoramic image coordinate to a 3D panoramic camera coordinate using camera geometry and human bio-metric property (i.e., height). Finally, we generate 3D tracklets by associating human appearance and 3D trajectory. We verify the effectiveness of our method on three datasets including a new one built by us, in terms of 3D single-view multi-person localization, 3D single-view multi-person tracking, and 3D panoramic multi-person localization and tracking. Our code and dataset are available at https://github.com/fandulu/MPLT.},
keywords={3D localization;multi-target tracking;panoramic videos},
doi={10.1109/ICASSP40776.2020.9053497},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054508,
author={H. {Liu} and M. {Song} and W. {Shi} and X. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Position Constraint Loss For Fashion Landmark Estimation},
year={2020},
volume={},
number={},
pages={1868-1872},
abstract={Fashion landmark estimation aims at locating functional key points of clothes, which has wide potential applications in electronic commerce. However, due to the occlusion and weak outline information, landmark estimation occurs outliers and duplicate detection problems. To alleviate these issues, we propose Position Constraint Loss (PCLoss) to constrain error landmark locations by utilizing the position relationship of landmarks. Specifically, PCLoss adds a regularization term for each landmark to regularize their relative positions, and it can be easily applied to both regression and heatmap based methods without extra computation during inference. Unlike existing approaches that propagate landmark information between feature layers by specific network structures, PCLoss introduces position relations of landmarks in the label space without modifying the network structure. In addition, we leverage the skeleton-like relation of clothing to further strengthen position constraints between landmarks. Extensive experimental results on DeepFashion, FLD and FashionAI demonstrate that our methods can effectively increase the performance of mainstream frameworks by a large margin.},
keywords={Fashion Landmark Estimation;Position Constraint Loss;Skeleton-like Relation},
doi={10.1109/ICASSP40776.2020.9054508},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053941,
author={F. {Wu} and A. J. {Ma} and Y. {Pan} and Y. {Gao} and X. {Yan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Receptive Field Pyramid Network for Object Detection},
year={2020},
volume={},
number={},
pages={1873-1877},
abstract={Current state-of-the-art methods usually utilize feature pyramid to provide various receptive fields for detecting objects at different scales. However, the feature maps from low- to high-level layers have large semantic gaps and are with different spatial resolutions, so that their representational capacity differs and noise is introduced when fusing them. To overcome this limitation and carry out better object detection, we design a novel network named Receptive Field Pyramid Network (RFPN). The proposed method is derived based on a receptive field pyramid through dilated convolutions, such that all of the extracted feature maps are with strong semantics and the same resolution. Moreover, we propose a pyramid attention mechanism by iteratively leveraging information from previous receptive fields to give higher responses for objects of interest. Experimental results on publicly available datasets show that the proposed method achieves better results than existing methods for comparison.},
keywords={Object detection;Receptive field;Dilated convolution;Attention mechanism},
doi={10.1109/ICASSP40776.2020.9053941},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053630,
author={Y. {Gu} and Y. {Qiao} and K. {Xu} and H. {Xu} and X. {Fang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Visual Tracking with Context-Based Active Occlusion Recognition},
year={2020},
volume={},
number={},
pages={1878-1882},
abstract={Occlusion is a great challenge for target model update in visual tracking. The target template may be corrupted by non-object information in the process of online learning due to occlusion. In this paper, we propose a context-based active occlusion recognition framework that can be integrated with various tracking approaches. The basic idea is to recognize the occlusion patches by actively tracking context patches and distinguish them from target based on a target model that integrates the information of both target and context. The framework consists of a context patch tracker, an occlusion patch recognizer and a local target template updater. The context patch tracker locates the context patches. The occlusion patch recognizer identifies the context patches occluding target. The local target template updater updates only non-occluded regions of the target template. In this way, the target template not only learns current target features but also avoids context corruption caused by occlusion. The experimental results show that our framework can improve the tracking performance in cases of heavy occlusion.},
keywords={Visual tracking;active occlusion recognition;occlusion detection},
doi={10.1109/ICASSP40776.2020.9053630},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053370,
author={S. {Xiao} and N. {Sang} and X. {Wang} and X. {Ma}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Leveraging Ordinal Regression With Soft Labels For 3d Head Pose Estimation From Point Sets},
year={2020},
volume={},
number={},
pages={1883-1887},
abstract={Head pose estimation from depth image is a challenging problem, considering its large pose variations, severer occlusions, and low quality of depth data. In contrast to existing approaches that take 2D depth image as input, we propose a novel deep regression architecture called Head PointNet, which consumes 3D point sets derived from a depth image describing the visible surface of a head. To cope with the non-stationary property of pose variation process, the network is facilitated with an ordinal regression module that incorporates metric penalties into ground truth label representation. The soft label representation encodes inter-class and intra-class information contained in the class labels simultaneously, and guides the network to learn discriminative features. Experiments on two challenging datasets, namely the Biwi Head Pose Dataset and Pandora Dataset, show that our proposed method outperforms state-of-the-art approaches.},
keywords={3D head pose estimation;ordinal regression;soft labels;point sets;deep learning},
doi={10.1109/ICASSP40776.2020.9053370},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053738,
author={H. {Zhang} and F. {Chen} and Z. {Shen} and Q. {Hao} and C. {Zhu} and M. {Savvides}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Solving Missing-Annotation Object Detection with Background Recalibration Loss},
year={2020},
volume={},
number={},
pages={1888-1892},
abstract={This paper focuses on a novel and challenging detection scenario: A majority of true objects/instances is unlabeled in the datasets, so these missing-labeled areas will be regarded as the background during training. Previous art [1] on this problem has proposed to use soft sampling to re-weight the gradients of RoIs based on the overlaps with positive instances, while their method is mainly based on the two-stage detector (i.e. Faster RCNN) which is more robust and friendly for the missing label scenario. In this paper, we introduce a superior solution called Background Recalibration Loss (BRL) that can automatically re-calibrate the loss signals according to the pre-defined IoU threshold and input image. Our design is built on the one-stage detector which is faster and lighter. Inspired by the Focal Loss [2] formulation, we make several significant modifications to fit on the missing-annotation circumstance. We conduct extensive experiments on the curated PASCAL VOC [3] and MS COCO [4] datasets. The results demonstrate that our proposed method outperforms the baseline and other state-of-the-arts by a large margin.},
keywords={Background Recalibration Loss;Object Detection;Missing-Annotation Scenarios},
doi={10.1109/ICASSP40776.2020.9053738},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053779,
author={C. {Fan} and C. {Liu} and K. {Wang} and J. {Jhan} and Y. F. {Wang} and J. {Chen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Face Feature Recovery via Temporal Fusion for Person Search},
year={2020},
volume={},
number={},
pages={1893-1897},
abstract={Searching actors from videos by a single portrait image is a challenging task, due to large variations of video scenes and intra-person appearance. To tackle this problem, most recent works apply deep neural networks for detecting and extracting robust facial features for matching. However, when the face of an actor is not detected due to occlusion, such image-matching based strategies would not be applicable. To address the issue, we propose a unique framework of "Face Feature Recovery via Temporal Fusion" to synthesize virtual facial features by observing both temporal and contextual information. Once such face features are extracted, a simple extension to the k-nearest neighbors for re-ranking, "Iterative k-nearest Multi-fusion", is presented to utilize both face and body features for improved person search. We conduct extensive experiments to evaluate the performance of our framework on the challenging extended version of the Cast Search in Movies (ECSM) dataset [1]. Without utilizing tracklet information during training, the proposed approach still performs favorably against recent works in searching actors of interest from movie videos. Besides, we also show the proposed approach can be fused with them to further improve the performance.},
keywords={person search;deep learning;image re-ranking},
doi={10.1109/ICASSP40776.2020.9053779},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054368,
author={A. S. {Shamsabadi} and C. {Oh} and A. {Cavallaro}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Edgefool: an Adversarial Image Enhancement Filter},
year={2020},
volume={},
number={},
pages={1898-1902},
abstract={Adversarial examples are intentionally perturbed images that mislead classifiers. These images can, however, be easily detected using denoising algorithms, when high-frequency spatial perturbations are used, or can be noticed by humans, when perturbations are large. In this paper, we propose EdgeFool, an adversarial image enhancement filter that learns structure-aware adversarial perturbations. Edge-Fool generates adversarial images with perturbations that enhance image details via training a fully convolutional neural network end-to-end with a multi-task loss function. This loss function accounts for both image detail enhancement and class misleading objectives. We evaluate EdgeFool on three classifiers (ResNet-50, ResNet-18 and AlexNet) using two datasets (ImageNet and Private-Places365) and compare it with six adversarial methods (DeepFool, SparseFool, Carlini-Wagner, SemanticAdv, Non-targeted and Private Fast Gradient Sign Methods). Code is available at https://github.com/smartcameras/EdgeFool.git.},
keywords={Adversarial images;detail enhancement},
doi={10.1109/ICASSP40776.2020.9054368},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054007,
author={H. {Wang} and H. {Zhang} and L. {Yu} and L. {Wang} and X. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Facial Feature Embedded Cyclegan For Vis-Nir Translation},
year={2020},
volume={},
number={},
pages={1903-1907},
abstract={Visible and near-infrared (VIS-NIR) face recognition remains a challenging task due to distinctions between spectral components of two modalities. Inspired by the CycleGAN, this paper presents a method aiming to translate between VIS and NIR face images. To achieve this, we propose a new facial feature embedded CycleGAN. Firstly, to learn the particular feature while preserving common facial representation between VIS and NIR domains, we employ a general facial feature extractor (FFE) to extract effective features. Herein the MobileFaceNet is pre-trained on a VIS face database and serves as the FFE. Secondly, the domain-invariant feature learning is enhanced by proposing a new pixel consistency loss. Lastly, we establish a new WHU VIS-NIR database including varies in face rotation and expressions to enrich the training data. Experimental results on the Oulu-CASIA and our WHU VIS-NIR databases show that the proposed FFE-based CycleGAN (FFE-CycleGAN) outperforms some state-of-the-art methods and achieves 96.5% accuracy.},
keywords={VIS-NIR Translation;CycleGAN},
doi={10.1109/ICASSP40776.2020.9054007},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053350,
author={W. {Su} and Y. {Yuan} and Q. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Image Deblurring Using Local Correlation Block},
year={2020},
volume={},
number={},
pages={1908-1912},
abstract={Dynamic scene deblurring is a challenging problem due to the various blurry source. Many deep learning based approaches try to train end-to-end deblurring networks, and achieve successful performance. However, the architectures and parameters of these methods are unchanged after training, so they need deeper network architectures and more parameters to adapt different blurry images, which increase the computational complexity. In this paper, we propose a local correlation block (LCBlock), which can adjust the weights of features adaptively according to the blurry inputs. And we use it to construct a dynamic scene deblurring network named LCNet. Experimental results show that the proposed LC-Net produces compariable performance with shorter running time and smaller network size, compared to state-of-the-art learning-based methods.},
keywords={Dynamic scene deblurring;Encoder-Decoder network;Local correlation block},
doi={10.1109/ICASSP40776.2020.9053350},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052902,
author={C. {Wang} and H. {Fu} and H. {Ma}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Global Structure Graph Guided Fine-Grained Vehicle Recognition},
year={2020},
volume={},
number={},
pages={1913-1917},
abstract={Fine-grained vehicle recognition is a challenging problem due to the subtle intra-category appearance variation, which requires the recognition model can capture discriminative features from distinguishing regions. The structure is an important characteristic of vehicles which can help to find substantial parts and learn distinguishing representations. In this paper, we propose an approach that introduces the structure graph into consideration to learn distinguishing representations for vehicle recognition. Our proposed method first constructs a global structure graph from the features generated by the convolutional network and then it applies the graph as the guidance to produce effective representations of vehicles. The results of extensive experiments demonstrate that our proposed method can produce more promising results than other state-of-the-art methods. The results of the visualization illustrate that our approach can construct a suitable structure graph and the global structure information facilitates learning discriminative representations at crucial parts of vehicles.},
keywords={Fine-grained vehicle recognition;representation learning;global structure graph},
doi={10.1109/ICASSP40776.2020.9052902},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053908,
author={W. {Jia} and L. {Li} and Z. {Li} and S. {Zhao} and S. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Triplet Loss Feature Aggregation for Scalable Hash},
year={2020},
volume={},
number={},
pages={1918-1922},
abstract={The increasing demands of high resolution and high quality videos aggravate the burden of limited cluster storage and restricted bandwidth resources. Hence, video de-duplication in storage and transmission is becoming an important feature for video cloud storage and Content Delivery Network (CDN) service providers. However, the current video de-duplication schemes mostly rely on the URL-based solution, which is unable to deal with non-cacheable content like video. The same video content with various resolutions and qualities may have completely different URL identification. In this paper, we propose a novel content-based video segmentation identification scheme that is invariant to the underlying codec and operational bit rates. It computes robust features from a triplet loss deep learning network that captures the invariance of the same content under different coding tools and strategy. In addition, a scalable hashing solution is developed based on Fisher Vector aggregation of the convolutional features from the Triplet loss network. Furthermore, we apply binary tree to obtain the triplets to improve the performance of the triplet-loss based VGG network. Our simulation results show significant improvements in terms of large scale video repository de-duplication compared with the state-of-the-art method.},
keywords={Binary Hash;Binary Tree;Fisher Vector;Triplet Loss;Video De-duplication},
doi={10.1109/ICASSP40776.2020.9053908},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054516,
author={J. {Gao} and W. {Zhang} and Z. {Chen} and F. {Zhong}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={HDMFH: Hypergraph Based Discrete Matrix Factorization Hashing for Multimodal Retrieval},
year={2020},
volume={},
number={},
pages={1923-1927},
abstract={In recent years, hashing based cross-modal retrieval methods have attracted considerable attention for the high retrieval efficiency and low storage cost. However, most of the existing methods neglect the high-order relationship among data samples. In addition, most of them can only deal with two modalities, e.g., image and text, without discussing the scenario of multiple modalities. To address these issues, in this paper, we propose a novel cross-modal hashing method, named Hypergraph Based Discrete Matrix Factorization Hashing (HDMFH), for multimodal retrieval. Different from most previous approaches, our method based on hypergraph regularization and matrix factorization can handle the cross-modal retrieval of more than two modalities, which is known as multimodal retrieval. Extensive experiments demonstrate that HDMFH outperforms the state-of-the-art cross-modal hashing methods.},
keywords={Cross-modal retrieval;multimodal retrieval;hashing;hypergraph learning},
doi={10.1109/ICASSP40776.2020.9054516},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053328,
author={Y. {Cheng} and C. {Zhang} and K. {Gu} and L. {Qi} and Z. {Gan} and W. {Zhang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Scale Deep Feature Fusion for Vehicle Re-Identification},
year={2020},
volume={},
number={},
pages={1928-1932},
abstract={Vehicle re-identification (re-id) is challenging due to the small inter-class distance. The differences between similar vehicles can be extremely subtle and only captured at particular scales and semantic levels. In this paper, we propose a novel Multi-Scale Deep Feature Fusion Network (MSDeep) to conduct both multi-scale and multi-level features for precise vehicle re-id. Based on the backbone deep CNN, MS-Deep mainly consists of two modules: 1) Multi-Scale Fusion (MSF) Block which encapsulates combination of multi-scale streams as MSF feature; 2) Multi-Level Fusion (MLF) Block which fuses MSF features of multiple levels to build the final descriptor. Importantly, in MSF, Multi-Scale Attention (MSA) is introduced to dynamically emphasize important channels of each scale, and Level-Wise Attention(LWA) is utilized in MLF to determine the different weightings for each MSF feature of different levels. As a result, experiments show that our MSDeep outperforms state-of-the-art algorithms on challenging VeRi and VehicleID benchmarks in terms of abundant and hierarchical hyper-descriptors.},
keywords={Vehicle Re-Identification;Multi-Scale;Multi-Level;Deep Neural Network},
doi={10.1109/ICASSP40776.2020.9053328},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053496,
author={Y. {Yu} and C. {Liang} and W. {Ruan} and L. {Jiang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Crowdsourcing-Based Ranking Aggregation for Person Re-Identification},
year={2020},
volume={},
number={},
pages={1933-1937},
abstract={Person re-identification (re-ID) is widely applied in surveillance and criminal detection applications. The existing research focus on devising the stand-alone re-ID methods, ignoring their practical application in the multi-person collaboration scenario. To improve the search efficiency, a group of investigators are usually assigned the same task to re-identify a suspect from a shared gallery set. Due to their personalized viewpoints and search feedback operations, different investigators may obtain diverse search results of the same query target. In this case, merging different rankings and generating an improved result is of great importance. To this end, this paper proposes a crowdsourcing-based ranking aggregation to adaptively fuse multiple ranking lists for re-ID problem. The method estimates the reliability of individual investigators, with a specifically designed long tail distribution to fit the top ranking demand, and is feasible for human-machine interaction. Extensive experiments conducted on four dataset-s demonstrate the superiority of the proposed method.},
keywords={Person Re-Identification;Crowdsourcing;Aggregation},
doi={10.1109/ICASSP40776.2020.9053496},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054178,
author={Q. {Zhou} and X. {Nie} and Y. {Shi} and X. {Liu} and Y. {Yin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Multi-Region Hashing},
year={2020},
volume={},
number={},
pages={1938-1942},
abstract={Hashing has been widely used for large-scale approximate nearest neighbors retrieval own to its high efficiency. In the existing hashing methods, deep supervised hashing methods have achieved the best performance by utilizing the semantic labels on data with deep learning. However, most of these methods only consider the semantics of whole image but ignore the local information which contains much more semantic details. Evidently, the semantic details are beneficial for hash learning. To address this issue, in this paper, we proposed a novel Deep Multi-Region Hashing (DMRH) method to fully utilize the semantic details, which uses overlapping N × N regions of an image to learn N2 hash codes for getting a final hash code. Extensive experimental results with three datasets show that DMRH can achieve state-of-the-art performance.},
keywords={Retrieval;hashing;deep learning;multi-region},
doi={10.1109/ICASSP40776.2020.9054178},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053661,
author={F. {Zhong} and Z. {Chen} and G. {Min} and F. {Xia}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Semantic Augmentation Hashing for Zero-Shot Image Retrieval},
year={2020},
volume={},
number={},
pages={1943-1947},
abstract={Hashing technique has been widely applied to large-scale image retrieval due to its efficacy in storage and retrieval. However, due to the explosive growth of multimedia data on the web, existing hashing approaches can hardly achieve satisfactory performance on the newly-emerging images of new classes. In this paper, we propose a novel Semantic Augmentation Hashing (SAH) for zero-shot image retrieval. The class semantic embeddings are used as an intermediate space between visual features and binary codes to align visual features to corresponding class semantics and to transfer knowledge from seen classes to unseen classes simultaneously. Extensive experiments conducted on two datasets with different scales demonstrate the superiority of our method as compared against the state-of-the-arts.},
keywords={Hashing;Image Retrieval;Zero-shot Learning},
doi={10.1109/ICASSP40776.2020.9053661},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054103,
author={S. E. {Eskimez} and R. K. {Maddox} and C. {Xu} and Z. {Duan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={End-To-End Generation of Talking Faces from Noisy Speech},
year={2020},
volume={},
number={},
pages={1948-1952},
abstract={Acoustic cues are not the only component in speech communication; if the visual counterpart is present, it is shown to benefit speech comprehension. In this work, we propose an end-to-end (no pre- or post-processing) system that can generate talking faces from arbitrarily long noisy speech. We propose a mouth region mask to encourage the network to focus on mouth movements rather than speech irrelevant movements. In addition, we use generative adversarial network (GAN) training to improve the image quality and mouth-speech synchronization. Furthermore, we employ noise-resilient training to make our network robust to unseen non-stationary noise. We evaluate our system with image quality and mouth shape (landmark) measures on noisy speech utterances with five types of unseen non-stationary noise between -10 dB and 30 dB signal-to-noise ratio (SNR) with increments of 1 dB SNR. Results show that our system outperforms a state-of-the-art baseline system significantly, and our noise-resilient training improves performance for noisy speech in a wide range of SNR.},
keywords={generative models;talking face from speech;speech animation;generative adversarial networks},
doi={10.1109/ICASSP40776.2020.9054103},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054129,
author={S. {Hwang} and H. {Byun}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unsupervised Image-to-Image Translation Via Fair Representation of Gender Bias},
year={2020},
volume={},
number={},
pages={1953-1957},
abstract={Fairness becomes a critical issue of computer vision to reduce discriminative factors in various systems. Among computer vision tasks, Image-to-Image translation for facial attributes editing can yield discriminative results. The unexpected gender changed results can be generated instead of editing target attributes due to the dataset imbalance problem. In this work, we propose a framework of unsupervised Image-to-Image translation that learns a fair representation by separating the latent space of our model into two purposes: 1) Target Attribute Editing, 2) Gender Preserving. We evaluate the proposed framework on CelebA dataset. Both quantitive and qualitative results demonstrate that our method improves image quality and fairness than the prior Image-to-Image translation method.},
keywords={fairness in computer vision;Image-to-Image translation;unsupervised learning;adversarial training},
doi={10.1109/ICASSP40776.2020.9054129},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054744,
author={M. {Park} and S. {Lee} and Y. M. {Ro}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Video Frame Interpolation Via Exceptional Motion-Aware Synthesis},
year={2020},
volume={},
number={},
pages={1958-1962},
abstract={In this paper, we propose a novel video frame interpolation method via exceptional motion-aware synthesis, in which accurate optical flow could be estimated even with exceptional motion patterns. Specifically, we devise two deep learning modules: exceptional motion detection and frame interpolation with refined flow. The motion detection module detects the position and intensity of exceptional motion patterns in current frame given the past frame sequence. The flow refinement module refines the pre-estimated optical flow for synthesizing the intermediate frame using the information of exceptional motion. The proposed modules improve the quality of the synthesized intermediate frame by making the optical flow robust against exceptional case of motion. Experimental results showed that the proposed method outperforms the state-of-the-art methods qualitatively and quantitatively.},
keywords={Video frame interpolation;exceptional motion detection;optical flow estimation;deep learning},
doi={10.1109/ICASSP40776.2020.9054744},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054553,
author={H. {Zhu} and Z. {Huang} and H. {Shan} and J. {Zhang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Look Globally, Age Locally: Face Aging With an Attention Mechanism},
year={2020},
volume={},
number={},
pages={1963-1967},
abstract={Face aging is of great importance for cross-age recognition and entertainment-related applications. Recently, conditional generative adversarial networks (cGANs) have achieved impressive results for face aging. Existing cGANs-based methods usually require a pixel-wise loss to keep the identity and background consistent. However, minimizing the pixel-wise loss between the input and synthesized images likely resulting in a ghosted or blurry face. To address this deficiency, this paper introduces an Attention Conditional GANs (AcGANs) approach for face aging, which utilizes attention mechanism to only alert the regions relevant to face aging. In doing so, the synthesized face can well preserve the background information and personal identity without using the pixel-wise loss, and the ghost artifacts and blurriness can be significantly reduced. Based on the benchmarked dataset Morph, both qualitative and quantitative experiment results demonstrate superior performance over existing algorithms in terms of image quality, personal identity, and age accuracy. Codes are available on https://github.com/JensonZhu14/AcGAN.},
keywords={Face Aging;Attention Mechanism;Conditional GANs;Pixel-wise Loss;Adversarial Training},
doi={10.1109/ICASSP40776.2020.9054553},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053880,
author={Y. {Lang} and Y. {He} and J. {Dong} and F. {Yang} and H. {Xue}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Design-Gan: Cross-Category Fashion Translation Driven By Landmark Attention},
year={2020},
volume={},
number={},
pages={1968-1972},
abstract={The rise of generative adversarial networks has boosted a vast interest in the field of fashion image-to-image translation. However, previous methods do not perform well in cross-category translation tasks, e.g., translating jeans to skirts in fashion images. The translated skirts are easier to lose the detail texture of the jeans, and the generated legs or arms often look unnatural. In this paper, we propose a novel approach, called DesignGAN, that utilizes the landmark guided attention and a similarity constraint mechanism to achieve fashion cross-category translation. Moreover, we can achieve texture editing on any customized input, which can even be used as an effective way to empower fashion designers. Experiments on fashion datasets verify that DesignGAN is superior to other image-to-image translation methods.},
keywords={DesignGAN;Fashion Translation;Landmark Attention;Texture Editing},
doi={10.1109/ICASSP40776.2020.9053880},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054439,
author={Y. {Chen} and W. {Chen} and X. {Cao} and Q. {Hua}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Intensity-Image Reconstruction for Event Cameras Using Convolutional Neural Network},
year={2020},
volume={},
number={},
pages={1973-1977},
abstract={Event cameras have many benefits than conventional cameras, such as high temporal resolution, high dynamic range. However, because the outputs of event cameras are asynchronous event streams than intensity images, Frame-based algorithms cannot be directly used. It is also necessary to present intensity images of event cameras on the display for human viewing. In this paper, "event frames" are recovered from event streams in an attenuation method and they are fed into the U-net network to generate intensity images. Our model is trained on a large amount of simulated data and gradually reduces the perceptual loss through training. In order to evaluate the model, we compare the generated image with the target image on the simulated data and the real data. This proves that our model can reconstruct intensity images of event cameras very well.},
keywords={event camera;dynamic vision sensor;image reconstruction;deep learning;U-net network},
doi={10.1109/ICASSP40776.2020.9054439},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053862,
author={M. {Krivokuća} and C. {Guillemot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Colour Compression of Plenoptic Point Clouds Using Raht-Klt with Prior Colour Clustering and Specular/Diffuse Component Separation},
year={2020},
volume={},
number={},
pages={1978-1982},
abstract={The recently introduced plenoptic point cloud representation marries a 3D point cloud with a light field. Instead of each point being associated with a single colour value, there can be multiple values to represent the colour at that point as perceived from different viewpoints. This representation was introduced together with a compression technique for the multi-view colour vectors, which is an extension of the RAHT method for point cloud attribute coding. In the current paper, we demonstrate that the best-proposed RAHT extension, RAHT-KLT, can be improved by performing a prior subdivision of the plenoptic point cloud into clusters based on similar colour values, followed by a separation of each cluster into specular and diffuse components, and coding each component separately with RAHT-KLT. Our proposed improvements are shown to achieve better rate-distortion results than the original RAHT-KLT method.},
keywords={Point clouds;light fields;colour coding;RAHT},
doi={10.1109/ICASSP40776.2020.9053862},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053971,
author={C. {Dinesh} and G. {Cheung} and I. V. {Bajić}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Super-Resolution of 3D Color Point Clouds Via Fast Graph Total Variation},
year={2020},
volume={},
number={},
pages={1983-1987},
abstract={3D point clouds acquired by low-cost sensors are often in lower spatial resolutions than desired for rendering images on high-resolution displays. In this paper, we propose a fast super-resolution (SR) algorithm for color 3D point clouds. We first populate a target low-res point cloud with added interior points. We refine the newly added 3D coordinates and their RGB values by minimizing a graph total variation (GTV) term of connected points’ surface normals and RGB values respectively. Unlike non-local methods that require computation-intensive searches of similar patches in a large defined space, our algorithm is inherently local and performs smoothing of newly inserted points only with respect to neighboring points. Moreover, differing from our previous GTV-based SR algorithm that employs gradient descent procedures with sensitive step size parameters due to GTV’s non-smooth l1-norm, we rewrite the l1 objective into a linear proxy, so that together with constraints on surface normals / RGB values, it can be solved efficiently as a parameter-free linear program (LP). Experimental results show that our algorithm outperforms competing non-graph-based point cloud SR schemes, and is significantly faster than our previous graph-based SR method.},
keywords={point cloud super-resolution;graph signal processing;convex optimization},
doi={10.1109/ICASSP40776.2020.9053971},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053408,
author={H. {Ren} and M. {El-khamy} and J. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Monocular Video Depth Estimation Using Temporal Attention},
year={2020},
volume={},
number={},
pages={1988-1992},
abstract={Monocular video depth estimation (MVDE) plays a crucial role in 3D computer vision. In this paper, we propose an end-to-end monocular video depth estimation network based on temporal attention. Our network starts by a motion compensation module where the spatial temporal transformer network (STN) is utilized to warp the input frames using the estimated optical flow. Next, a temporal attention module is used to combine features from the warped frames, while emphasizing the temporal consistency. A monocular depth estimation network is used to estimate the depth from the temporally combined features. Experimental results demonstrate that our proposed framework achieves better performance compared to the state-of-the-art single image depth estimation (SIDE) networks, as well as existing MVDE methods.},
keywords={Depth estimation;temporal attention;spatial temporal transformer;optical flow},
doi={10.1109/ICASSP40776.2020.9053408},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053724,
author={K. {Guo} and S. {Song} and S. {Chang} and T. {Kim} and S. {Han} and I. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Full-Fov Depth Estimation in Tele-Wide Camera System},
year={2020},
volume={},
number={},
pages={1993-1997},
abstract={Tele-wide camera system with different Field of View (FoV) lenses becomes very popular in recent mobile devices. Usually it is difficult to obtain full-FoV depth based on traditional stereo-matching methods. Pure Deep Neural Network (DNN) based depth estimation methods can obtain full-FoV depth, but have low robustness for scenarios which are not covered by training dataset. In this paper, to address the above problems we propose a hierarchical hourglass network for robust full-FoV depth estimation in tele-wide camera system, which combines the robustness of traditional stereo-matching methods with the accuracy of DNN. More specifically, the proposed network comprises three major modules: single image depth prediction module infers initial depth from input color image, depth propagation module propagates traditional stereo-matching tele-FoV depth to surrounding regions, and depth combination module fuses the initial depth with the propagated depth to generate final output. Each of these modules employs an hourglass model, which is a kind of encoder-decoder structure with skip connections. Experimental results compared with state-of-the-art depth estimation methods demonstrate that our method not only produces robust and better subjective depth quality on wild test images, but also obtains better quantitative results on standard datasets.},
keywords={Depth estimation;Hierarchical hourglass network;L1-norm scale-invariant loss function;Tele-wide camera;Full field of view},
doi={10.1109/ICASSP40776.2020.9053724},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053532,
author={Y. {Li} and L. {Zhang} and Q. {Wang} and G. {Lafruit}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Manet: Multi-Scale Aggregated Network For Light Field Depth Estimation},
year={2020},
volume={},
number={},
pages={1998-2002},
abstract={We present a novel end-to-end network, MANet, for light field depth estimation. MANet is a parameter-effective and effi-cient multi-scale aggregated network, which is about 3 times smaller and 3 times faster than the current top-performing method Epinet. The MANet architecture is performed for estimating depth from light field plenoptic cameras, and experimental results show that the proposed MANet outperforms state-of-the-art methods on HCI, CVIA-HCI and EPFL Lytro light field datasets.},
keywords={Light field;Depth estimation;Deep learning;Micro-lens array camera},
doi={10.1109/ICASSP40776.2020.9053532},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053664,
author={J. {Li} and X. {Jin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={EPI-Neighborhood Distribution Based Light Field Depth Estimation},
year={2020},
volume={},
number={},
pages={2003-2007},
abstract={In this paper, a novel depth estimation algorithm tackling foreground occlusion is proposed based on the neighborhood distribution in the sheared epipolar images (EPIs). First, the EPI is sheared to perform refocusing. Next a series of sheared EPI’s neighboring pixels in a local window are selected and the corresponding histogram distributions are analyzed by the proposed novel tensor, Kullback-Leibler Divergence (KLD). Then, depths calculated from vertical and horizontal EPIs’ tensors are fused according to the tensors’ variation scale for a high quality depth map. Finally, confident depth points are propagated to the whole image by global optimization. Experimental results show that the proposed algorithm achieves better performance relative to state-of-the-art algorithms.},
keywords={Depth estimation;light field;EPI neighborhood distribution},
doi={10.1109/ICASSP40776.2020.9053664},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054238,
author={M. {Wang} and H. {Cai} and J. {Zhou} and M. {Gong}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Stochastic Multi-Scale Aggregation Network for Crowd Counting},
year={2020},
volume={},
number={},
pages={2008-2012},
abstract={Crowd counting from unconstrained and congested scenes is an important task in computer vision. Its main difficulties stem from large scale/density variation and prone to over-fitting. This paper presents a novel end-to-end stochastic multi-scale aggregation network (SMANet) which carefully addresses these issues. Specifically, general features are first extracted by the front-end subnetwork and then fed into the back-end subnetwork which consists of stochastic multi-scale aggregation module, density map generator, and global prior encoder. The stochastic aggregation impels the multi-branch units to learn features at different scales effectively and reduces sensitivity to scale variations, whereas the global prior encoder is designed to encode global contextual information and guarantee density consistency of shared representations. Our proposed SMANet is the first work to fuse multi-scale features in a stochastic manner for crowd counting. Experimental results on four public datasets demonstrate that our SMANet consistently outperforms the state-of-the-arts.},
keywords={Crowd Counting;Multi-scale Dilated Convolutions;Stochastic Aggregation},
doi={10.1109/ICASSP40776.2020.9054238},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053243,
author={P. {Afshar} and A. {Oikonomou} and K. N. {Plataniotis} and A. {Mohammadi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies},
year={2020},
volume={},
number={},
pages={2013-2017},
abstract={Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as "MDR-SURV" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.},
keywords={Survival Prediction;Multi-scale;Deep Learningbased Radiomics;Lung Tumor.},
doi={10.1109/ICASSP40776.2020.9053243},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054644,
author={T. {Alt} and J. {Weickert}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning a Generic Adaptive Wavelet Shrinkage Function for Denoising},
year={2020},
volume={},
number={},
pages={2018-2022},
abstract={The rise of machine learning in image processing has created a gap between trainable data-driven and classical model- driven approaches: While learning-based models often show superior performance, classical ones are often more transparent. To reduce this gap, we introduce a generic wavelet shrinkage function for denoising which is adaptive to both the wavelet scales as well as the noise standard deviation. It is inferred from trained results of a tightly parametrised function which is inherited from nonlinear diffusion. Our proposed shrinkage function is smooth and compact while only using two parameters. In contrast to many existing shrinkage functions, it is able to enhance image structures by amplifying wavelet coefficients. Experiments show that it outperforms classical shrinkage functions by a significant margin.},
keywords={Wavelet Shrinkage;Adaptive Thresholding;Denoising;Interpretable Learning},
doi={10.1109/ICASSP40776.2020.9054644},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053478,
author={X. {Zhong} and O. {Gong} and W. {Huang} and J. {Yuan} and B. {Ma} and R. W. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Scale Residual Network for Image Classification},
year={2020},
volume={},
number={},
pages={2023-2027},
abstract={Multi-scale approach representing image objects at various levels-of-details has been applied to various computer vision tasks. Existing image classification approaches place more emphasis on multi-scale convolution kernels, and overlook multi-scale feature maps. As such, some shallower information of the network will not be fully utilized. In this paper, we propose the Multi-Scale Residual (MSR) module that integrates multi-scale feature maps of the underlying information to the last layer of Convolutional Neural Network. Our proposed method significantly enhances the characteristics of the information in the final classification. Extensive experiments conducted on CIFAR100, Tiny-ImageNet and large-scale CalTech-256 datasets demonstrate the effectiveness of our method compared with Res-Family.},
keywords={Image Classification;Multi-Scale;Branch;Down-Sampling;Residual Networks},
doi={10.1109/ICASSP40776.2020.9053478},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053804,
author={H. {Dong} and X. {Zhang} and Y. {Guo} and F. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Multi-Scale Gabor Wavelet Network for Image Restoration},
year={2020},
volume={},
number={},
pages={2028-2032},
abstract={Due to the limitations of the imaging processors and complex weather conditions, image degradation is often inevitable. Existing deep learning-based image restoration methods often rely on the powerful feature representation capacity of deep networks and pay less attention to the inherent properties of the degradation signal, e.g. variations in spatial scale and orientations across the image, which makes them ineffective for the image restoration tasks. In this paper, we propose a Multiscale Gabor Wavelet Network (MsGWN) for image restoration. We apply the multi-scale architecture to extract the contaminated feature from input at different spatial scales, and thus the contaminated feature can be effectively restored in a corse- to-fine manner. However, using multi-scale architecture alone cannot remove the degradations with different orientations. To overcome this problem, we introduce a Gabor Wavelet Module (GWM) to further extract the contaminated features from four orientations. By decomposing the features into four multi-orientation components, the restoration process can be facilitated by avoiding learning the mixed degradations all-in- one. We evaluate the proposed method on image demoirding, image deraining, and image dehazing. Experiments on these applications demonstrate that the proposed method can achieve favorable results against the state-of-the-art approaches.},
keywords={Image restoration;Multi-scale;Multiorientation;Gabor Wavelet;Deep learning},
doi={10.1109/ICASSP40776.2020.9053804},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053245,
author={J. {Liu} and Y. {Xie} and H. {Song} and W. {Yuan} and L. {Ma}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Residual Attention Network for Wavelet Domain Super-Resolution},
year={2020},
volume={},
number={},
pages={2033-2037},
abstract={Single-image super-resolution plays an important role in computer vision area. However, previous works using convolutional neural networks perform badly when reconstructing high frequency details, result in over-smooth and lacking of textural information in the output. At the same time, super-resolution computation always relays on convolutional neural networks with huge depth, which is super tricky to train and use. In this paper, we propose a novel network with better textural details in wavelet domain, which is composed of a feature extract layer, residual channel attention groups (RCAG) and a residual up-sampling layer based on inverse discrete wavelet transform. Channel attention and spatial attention layers are inserted into residual channel and spatial attention blocks (RCSAB), enhancing the learning of high frequency information with attention maps. Composed of a chain of RCSAB and a channel attention layer with short skip connection, RCAG is good at catching long-term high frequency information. Then the feature mapping component is composed of a chain of RCAG. Experiment shows that our method performs better than state-of-the-art methods on benchmark datasets in different scales.},
keywords={Single Image Super Resolution;Image Processing;Convolutional Neural Networks;Spatial Attention;Inverse Discrete Wavelet Transformation},
doi={10.1109/ICASSP40776.2020.9053245},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054067,
author={B. {Li} and J. {Han} and K. {Rose}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Adaptive Linear Estimator Based Approach to Bi-Directional Motion Compensated Prediction},
year={2020},
volume={},
number={},
pages={2038-2042},
abstract={Bi-directional motion compensated prediction is widely utilized in video coding. Conventionally, the encoder searches for two motion vectors pointing to reference frames in both directions, and transmits these motion vectors to the decoder. Recognizing that the two reference frames are already available to the decoder, prior work proposed decoder-side motion estimation to extract motion information or optical flow, at the cost of dramatic increase in decoder complexity. This paper proposes a novel bi-directional motion compensation mode that efficiently utilizes the motion information that is already available to the decoder, without recourse to extensive search. An estimation theory based approach is proposed and utilized to provide a high quality prediction, which adaptively combines contributions from multiple motion-compensated references. Experimental results show that the proposed method, while yielding a greatly reduced decoder side complexity, introduces a significant coding gain for a diverse set of video sequences.},
keywords={Video coding;motion compensation;hierarchical structure;linear estimation},
doi={10.1109/ICASSP40776.2020.9054067},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054211,
author={B. {Vishwanath} and K. {Rose}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Spherical Video Coding with Geometry and Region Adaptive Transform Domain Temporal Prediction},
year={2020},
volume={},
number={},
pages={2043-2047},
abstract={Many virtual and augmented reality applications depend critically on efficient compression of spherical videos. Current approaches apply a projection geometry to map a spherical video onto the plane(s), wherein a standard codec can be used for compression. Video coders employ simple pixel copying from reference frames for inter-prediction, which ignores underlying spatial correlations, and is hence suboptimal. A novel paradigm of transform domain temporal prediction (TDTP) was developed previously in our lab to effectively overcome this suboptimality of standard video coding. This paper is motivated by the observation that projected spherical videos exhibit significantly more statistical variation due to i) the choice of projection geometry and ii) position of the block on the sphere, which reflect variations in sampling density and various statistical features. To account for such variations, we propose geometry and region adaptive TDTP that is tailored to spherical videos. For a given geometry, the sphere is divided into regions, according to expected signal statistics, and prediction filters are designed for each region. Experimental results show significant performance gains as evidence for the efficacy of TDTP in spherical video coding.},
keywords={spherical video coding;virtual reality;motion compensation;temporal prediction},
doi={10.1109/ICASSP40776.2020.9054211},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054716,
author={C. {Bonnineau} and W. {Hamidouche} and J. {Travers} and O. {Déforges}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Versatile Video Coding and Super-Resolution for Efficient Delivery of 8k Video with 4k Backward-Compatibility},
year={2020},
volume={},
number={},
pages={2048-2052},
abstract={In this paper, we propose, through an objective study, to compare and evaluate the performance of different coding approaches allowing the delivery of an 8K video signal with 4K backward-compatibility on broadcast networks. Presented approaches include simulcast of 8K and 4K single-layer signals encoded using High-Efficiency Video Coding (HEVC) and Versatile Video Coding (VVC) standards, spatial scalability using SHVC with 4K base layer (BL) and 8K enhancement-layer (EL), and super-resolution applied on 4K VVC signal after decoding to reach 8K resolution. For up-scaling, we selected the deep-learning-based super-resolution method called Super-Resolution with Feedback Network (SRFBN) and the Lanczos interpolation filter. We show that the deep-learning-based approach achieves visual quality gain over simulcast, especially on bit-rates lower than 30Mb/s with average gain of 0.77dB, 0.015, and 7.97 for PSNR, SSIM, and VMAF, respectively and outperforms the Lanczos filter in average by 29% of BD-rate savings.},
keywords={8K;HEVC;VVC;SHVC;Super-Resolution},
doi={10.1109/ICASSP40776.2020.9054716},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054028,
author={A. {Henkel} and I. {Zupancic} and B. {Bross} and M. {Winken} and H. {Schwarz} and D. {Marpe} and T. {Wiegand}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Alternative Half-Sample Interpolation Filters for Versatile Video Coding},
year={2020},
volume={},
number={},
pages={2053-2057},
abstract={To reduce the residual energy of a video signal, motion compensated prediction with fractional-sample accuracy has been successfully employed in modern video coding technology. In contrast to the fixed quarter-sample motion vector resolution for the luma component in High Efficiency Video Coding standard, the current draft of a new Versatile Video Coding standard introduces a block-level adaptive motion vector resolution (AMVR) scheme. The AMVR allows coding of motion vector difference at different precisions. Nevertheless, the interpolation filters for each fractional sample position are fixed. In this paper, alternative half-luma-sample interpolation filters are proposed. Enabling the interpolation filter selection at a fine granularity allows to better adapt to the local image characteristics. Experimental results show that the proposed method can improve the average BD-rate of VTM 5.0 reference software by −0.40% for Random Access configuration and by −0.76% for Low Delay P configuration.},
keywords={switched half-sample interpolation filter;adaptive motion vector resolution;versatile video coding},
doi={10.1109/ICASSP40776.2020.9054028},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053580,
author={X. {Shen} and X. {Zhang} and S. {Wang} and S. {Kwong} and G. {Zhu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Just Noticeable Distortion Based Perceptually Lossless Intra Coding},
year={2020},
volume={},
number={},
pages={2058-2062},
abstract={Perceptual video coding plays a very important role in video codec optimization aiming at removing the perceptual redundancies in video content. In this paper, a just noticeable distortion (JND) guided perceptually lossless coding framework is proposed for Versatile Video Coding (VVC) intra coding. Within this framework, a pattern-based pixel wise JND model is employed to guide the distortion distribution, and subsequently the most appropriate quantization parameter is chosen for each Coding Tree Unit (CTU). The content adaptive Laplacian distribution based D-Q model based on a two pass coding framework is established to derive the most proper QP that satisfies the perceptually lossless coding criteria. The whole framework is integrated into the H.266/VVC intra coding framework. Experimental results demonstrate that the proposed scheme can achieve high accuracy prediction and efficient perceptually lossless intra coding, leading to around 10% bitrate savings comparing with the frame level QP derivation scheme.},
keywords={Perceptual video coding;just noticeable distortion;intra coding},
doi={10.1109/ICASSP40776.2020.9053580},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053102,
author={J. {Kim} and J. {Choi} and J. {Chang} and J. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Efficient Deep Learning-Based Lossy Image Compression Via Asymmetric Autoencoder and Pruning},
year={2020},
volume={},
number={},
pages={2063-2067},
abstract={Recently, deep learning-based lossy image compression methods have been proposed. However, their efficiency in terms of storage and computational costs has not been addressed adequately. In this paper, we propose efficient lossy image compression methods based on asymmetric autoencoder and decoder pruning. Experimental results demonstrate the effectiveness of our methods.},
keywords={Lossy image compression;asymmetric autoencoder;weight pruning;filter pruning},
doi={10.1109/ICASSP40776.2020.9053102},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054527,
author={M. A. {Shah} and B. {Raj}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deriving Compact Feature Representations Via Annealed Contraction},
year={2020},
volume={},
number={},
pages={2068-2072},
abstract={It is common practice to use pretrained image recognition models to compute feature representations for visual data. The size of the feature representations can have a noticeable impact on the complexity of the models that use these representations, and by extension on their deployablity and scalability. Therefore it would be beneficial to have compact visual representations that carry as much information as their high-dimensional counterparts. To this end we propose a technique that shrinks a layer by an iterative process in which neurons are removed from the and network is fine tuned. Using this technique we are able to remove 99% of the neurons from the penultimate layer of AlexNet and VGG16, while suffering less than 5% drop in accuracy on CIFAR10, Caltech101 and Caltech256. We also show that our method can reduce the size of AlexNet by 95% while only suffering a 4% reduction in accuracy on Caltech101.},
keywords={model compression;compact representations;distillation},
doi={10.1109/ICASSP40776.2020.9054527},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053820,
author={F. {Nie} and S. {Pei} and R. {Wang} and X. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast Clustering With Co-Clustering Via Discrete Non-Negative Matrix Factorization for Image Identification},
year={2020},
volume={},
number={},
pages={2073-2077},
abstract={How to effectively cluster large-scale image data sets is a challenge and is receiving more and more attention. To address this problem, a novel clustering method called fast clustering with co-clustering via discrete non-negative matrix factorization, is proposed. Inspired by co-clustering, our algorithm reduces computational complexity by transforming clustering tasks into co-clustering tasks. Although our model has the same form of objective function as normalized cut, we relax it to a matrix decomposition problem, which is different from most graph-based approaches. In addition, an efficient optimization algorithm is proposed to solve the relaxed problem, where a discrete solution corresponding to the clustering result can be directly obtained. Extensive experiments have been conducted on several synthetic data sets and real word data sets. Compared with the state-of-the-art clustering methods, the proposed algorithm achieves very promising performance.},
keywords={Fast;clustering;non-negative matrix factorization;co-clustering},
doi={10.1109/ICASSP40776.2020.9053820},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053275,
author={P. {Nair} and R. G. {Gavaskar} and K. N. {Chaudhury}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Compressive Adaptive Bilateral Filtering},
year={2020},
volume={},
number={},
pages={2078-2082},
abstract={We propose a fast algorithm for an adaptive variant of the classical bilateral filter, where the range kernel is allowed to vary from pixel to pixel. Several fast and accurate algorithms have been proposed for bilateral filtering, but they assume that the same range kernel is used at each pixel and hence cannot be used for adaptive bilateral filtering (ABF). Only recently, it was shown that fast algorithms for ABF can be developed by approximating the local histogram around each pixel using polynomials. The present algorithm is derived using an entirely different approximation, namely, the range kernels across all pixels are jointly approximated (compressed) using singular value decomposition (SVD). The SVD involves a very large matrix and cannot be computed exactly; however, we are able to get a sufficiently accurate approximation using the Nyström method (without populating/storing the entire matrix). We show that this SVD-type decomposition allows us to approximate the adaptive bilateral filter using fast convolutions. To demonstrate the speed and accuracy of the proposed algorithm in relation to existing algorithms, we use it for texture filtering, JPEG deblocking, and detail enhancement.},
keywords={adaptive bilateral filter;fast algorithm;singular value decomposition;Nyström approximation},
doi={10.1109/ICASSP40776.2020.9053275},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053332,
author={B. {Zhang} and S. {Jin} and Y. {Xia} and Y. {Huang} and Z. {Xiong}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Attention Mechanism Enhanced Kernel Prediction Networks for Denoising of Burst Images},
year={2020},
volume={},
number={},
pages={2083-2087},
abstract={Deep learning based image denoising methods have been extensively investigated. In this paper, attention mechanism enhanced kernel prediction networks (AME-KPNs) are proposed for burst image denoising, in which, nearly cost-free attention modules are adopted to first refine the feature maps and to further make a full use of the inter-frame and intra-frame redundancies within the whole image burst. The proposed AME-KPNs output per-pixel spatially-adaptive kernels, residual maps and corresponding weight maps, in which, the predicted kernels roughly restore clean pixels at their corresponding locations via an adaptive convolution operation, and subsequently, residuals are weighted and summed to compensate the limited receptive field of predicted kernels. Simulations and real-world experiments are conducted to illustrate the robustness of the proposed AME-KPNs in burst image denoising.},
keywords={Burst image denoising;attention mechanism;kernel prediction networks;adaptive convolution},
doi={10.1109/ICASSP40776.2020.9053332},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053547,
author={P. {Mu} and J. {Chen} and R. {Liu} and W. {Zhong} and X. {Fan} and Z. {Luo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Image Restoration Via Data-Dependent Proximal Averaged Optimization},
year={2020},
volume={},
number={},
pages={2088-2092},
abstract={Maximum A Posterior (MAP) acts as one of the most popular modeling scheme in image restoration and is usually reduced to a separable optimization model. Unfortunately, it is challenging to establish exact regularization term and the model with complex priors is hard to optimize. In additionally, it is still hard to incorporate different domain knowledge and data-dependent information into MAP model without changing the property of the objective. To partially address the above issues, we develop a Data-dependent Proximal Averaged (DPA) paradigm through optimizing objective and data-dependent feasibility constraint for the challenging Image Restoration (IR) tasks. Both visual and quantitative comparison results demonstrate that our method outperforms the state of the art.},
keywords={Image restoration;data-dependent;proximal averaged;non-convex optimization},
doi={10.1109/ICASSP40776.2020.9053547},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052958,
author={Z. {Fang} and S. {Zhou} and X. {Li} and H. {Zhu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Way Multi-View Deep Autoencoder for Image Feature Learning with Multi-Level Graph Regularization},
year={2020},
volume={},
number={},
pages={2093-2097},
abstract={Multi-view feature learning has garnered much attention recently since many real world data are comprised of different representations or views. How to explore the consensus structure and eliminate the inconsistency noise in different views remains a challenging problem in multi-view feature learning. In this paper, we propose a multi-way deep autoencoder for multi-view feature learning to explore the deep consensus structure and reconcile the efficiency of encoding process meanwhile. Through a multi-way encoding process, we embed the original data feature views to nonnegative representations of multiple levels which are structured hierarchically. Along the structure of embedded representations, we recover the diversity and important information layer by layer in the decoding process. The experiments on two image datasets show the superior performance of our method.},
keywords={multi-view;feature learning;autoencoder;hierarchical;deep structure},
doi={10.1109/ICASSP40776.2020.9052958},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053741,
author={C. {Zheng} and Z. {Li} and Y. {Yang} and S. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Exposure Interpolation Via Hybrid Learning},
year={2020},
volume={},
number={},
pages={2098-2102},
abstract={Deep learning based methods have become dominant solutions to many image processing problems. A natural question would be "Is there any space for conventional methods on these problems?" In this paper, exposure interpolation is taken as an example to answer this question and the answer is "Yes". A new hybrid learning framework is introduced to interpolate a medium exposure image for two large-exposure-ratio images from an emerging high dynamic range (HDR) video capturing device. The framework is set up by fusing conventional and deep learning methods. Experimental results indicate that the deep learning method can be used to improve the quality of interpolated image via the conventional method significantly. The conventional method can be adopted to increase the convergence speed of the deep learning method and to reduce the number of samples which is required by the deep learning method. They compensate each other.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053741},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054200,
author={G. {Huang} and A. G. {Bors}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Spatio-Temporal Representations With Temporal Squeeze Pooling},
year={2020},
volume={},
number={},
pages={2103-2107},
abstract={In this paper, we propose a new video representation learn¬ing method, named Temporal Squeeze (TS) pooling, which can extract the essential movement information from a long sequence of video frames and map it into a set of few im¬ages, named Squeezed Images. By embedding the Tempo¬ral Squeeze pooling as a layer into off-the-shelf Convolution Neural Networks (CNN), we design anew video classification model, named Temporal Squeeze Network (TeSNet). The re¬sulting Squeezed Images contain the essential movement in¬formation from the video frames, corresponding to the op¬timization of the video classification task. We evaluate our architecture on two video classification benchmarks, and the results achieved are compared to the state-of-the-art.},
keywords={Convolution Neural Networks (CNN);Temporal Squeeze pooling;video representation},
doi={10.1109/ICASSP40776.2020.9054200},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052960,
author={R. {Ding} and L. {Wang} and Q. {Zhang} and Z. {Niu} and N. {Zheng} and G. {Hud}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fine-Grained Giant Panda Identification},
year={2020},
volume={},
number={},
pages={2108-2112},
abstract={The image-based fine-grained identification of individual giant pandas (Ailuropoda melanoleuca) is an emerging technology, and it is extraordinarily challenging due to the extremely subtle visual differences between individual giant pandas and limited annotated training data. To address these challenges, we propose the Feature-Fusion Convolutional Neural Network with Patch Detector (FFCNN-PD) algorithm, which exploits the discriminative local patches and builds a hierarchical representation generated by fusing both global and local features. Specifically, an attentional cross-channel pooling is embedded in the FFCNN-PD to improve the class- specific patch detectors. In addition, we propose a new giant panda identification dataset (iPanda-30) to establish a benchmark. Experiments on the proposed iPanda-30 dataset and other fine-grained recognition datasets demonstrate the effectiveness of the FFCNN-PD algorithm against the existing state-of-the-arts.},
keywords={Fine-grained recognition;panda identification;feature fusion;patch detector},
doi={10.1109/ICASSP40776.2020.9052960},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054086,
author={H. {Ho} and M. {Shim} and D. {Wee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning from Dances: Pose-Invariant Re-Identification for Multi-Person Tracking},
year={2020},
volume={},
number={},
pages={2113-2117},
abstract={Most existing multi-person tracking approaches rely on appearance based re-identification (re-ID) to resolve fragmented tracklets. However, simply using appearance information could be insufficient for videos containing severe pose changes, such as sports or dance videos. With the goal of learning pose-invariant representations, we propose an end-to-end deep learning framework Sparse-Temporal ReID Network. Our proposed network not only realizes human pose disentanglement in an image recovery manner, but also makes efficient linkages between the identical subjects via a unique Sparse temporal identity sampling technique across time steps. Experimental results demonstrate the effectiveness of our proposed method on both multi-view re-ID benchmarks and our newly collected dance video dataset DanceReID1.},
keywords={Pose-Invariant Features;Person Re-Identification;Multi-Person Tracking;Deep Learning},
doi={10.1109/ICASSP40776.2020.9054086},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054449,
author={Y. {Yuan} and J. {Li} and Y. {Li} and J. {Qiang} and B. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Fractional Orthogonal Latent Consistent Features for Face Hallucination and Recognition},
year={2020},
volume={},
number={},
pages={2118-2122},
abstract={Face hallucination (FH) is a powerful technique to reconstruct high-resolution (HR) faces from low-resolution (LR) faces. Most of conventional FH techniques ignore the influence of small training data, which may lead to the bias of variance and covariance. In this paper, we propose a novel FH method via fractional orthogonal latent consistent features that we call fractional orthogonal partial least squares based FH (FOPLSFH). In the proposed FOPLS-FH, intra- and cross-resolution covariance matrices are re-estimated through fractional-order eigenvalues and singular values modeling. Experimental results on real-world face datasets demonstrate the effectiveness of the proposed FOPLS-FH method.},
keywords={face hallucination;face super-resolution;orthogonal features;fractional order;consistency},
doi={10.1109/ICASSP40776.2020.9054449},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053229,
author={Y. {BANDOH} and T. {NAKACHI} and H. {KIYA}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sparse Modeling on Distributed Encryption Data},
year={2020},
volume={},
number={},
pages={2123-2127},
abstract={Big-data analysis by edge/cloud systems is becoming more important. However, when information may lead to personal identification, such information tends to be encrypted and restricted to its owners to ensure privacy protection. The resulting data is often insufficiently detailed to permit useful analysis. As a result, the desired analysis accuracy may not be achieved. To deal with this issue, several studies have examined encryptions based on the random unitary transform. This is because the random unitary transform has lower computational complexity than other encryption schemes, and its encryption domain supports several signal processing algorithms. However, analysis models on distributed encrypted data, have not been studied deeply enough. In this paper, we construct an analysis model for data encrypted with the random unitary transform by deriving a LASSO solution for encrypted data. The analytical model can derive the same LASSO solution as that yielded by processing the original data (i.e. without encryption). The analytical model supports distributed encryption, where a data set consists of different components that are encrypted at different sites independently. The collaboration enables us to improve the accuracy of analysis for distributed privacy-sensitive information.},
keywords={LASSO;coordinate descent algorithm;random unitary transform},
doi={10.1109/ICASSP40776.2020.9053229},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052953,
author={H. {Lee} and S. {Eum} and H. {Kwon}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={S-DOD-CNN: Doubly Injecting Spatially-Preserved Object Information for Event Recognition},
year={2020},
volume={},
number={},
pages={2128-2132},
abstract={We present a novel event recognition approach called Spatially-preserved Doubly-injected Object Detection CNN (S-DOD-CNN), which incorporates the spatially preserved object detection information in both a direct and an indirect way. Indirect injection is carried out by simply sharing the weights between the object detection modules and the event recognition module. Meanwhile, our novelty lies in the fact that we have preserved the spatial information for the direct injection. Once multiple regions-of-intereset (RoIs) are acquired, their feature maps are computed and then projected onto a spatially-preserving combined feature map using one of the four Rol Projection approaches we present. In our architecture, combined feature maps are generated for object detection which are directly injected to the event recognition module. Our method provides the state-of-the-art accuracy for malicious event recognition.},
keywords={IOD-CNN;DOD-CNN;malicious crowd dataset;malicious event classification;multi-task CNN},
doi={10.1109/ICASSP40776.2020.9052953},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053675,
author={B. {Wu} and H. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Angular Discriminative Deep Feature Learning for Face Verification},
year={2020},
volume={},
number={},
pages={2133-2137},
abstract={Thanks to the development of deep Convolutional Neural Network (CNN), face verification has achieved great success rapidly. Specifically, Deep Distance Metric Learning (DDML), as an emerging area, has achieved great improvements in computer vision community. Softmax loss is widely used to supervise the training of most available CNN models. Whereas, feature normalization is often used to compute the pair similarities when testing. In order to bridge the gap between training and testing, we require that the intra-class cosine similarity of the inner-product layer before softmax loss is larger than a margin in the training step, accompanied by the supervision signal of softmax loss. To enhance the discriminative power of the deeply learned features, we extend the intra-class constraint to force the intra-class cosine similarity larger than the mean of nearest neighboring inter-class ones with a margin in the normalized exponential feature projection space. Extensive experiments on Labeled Face in the Wild (LFW) and Youtube Faces (YTF) datasets demonstrate that the proposed approaches achieve competitive performance for the open-set face verification task.},
keywords={deep distance metric learning;cosine similarity;face verification},
doi={10.1109/ICASSP40776.2020.9053675},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054733,
author={A. E. {Rahman Shabayek} and D. {Aouada} and K. {Cherenkova} and G. {Gusev} and B. {Ottersten}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={3d Deformation Signature for Dynamic Face Recognition},
year={2020},
volume={},
number={},
pages={2138-2142},
abstract={This work proposes a novel 3D Deformation Signature (3DS) to represent a 3D deformation signal for 3D Dynamic Face Recognition. 3DS is computed given a non-linear 6D-space representation which guarantees physically plausible 3D deformations. A unique deformation indicator is computed per triangle in a triangulated mesh as a ratio derived from scale and in-plane deformation in the canonical space. These indicators, concatenated, construct the 3DS for each temporal instance. There is a pressing need of non-intrusive bio-metric measurements in domains like surveillance and security. By construction, 3DS is a non-intrusive facial measurement that is resistant to common security attacks like presentation, template and adversarial attacks. Two dynamic datasets (BU4DFE and COMA) were examined, in a standard classification framework, to evaluate 3DS. A first rank recognition accuracy of 99.9%, that outperforms existing literature, was achieved. Assuming an open-world setting, 99.97% accuracy was attained in detecting unseen distractors.},
keywords={3D Dynamic Face Recognition;3D Deformation Signature;Lie Groups},
doi={10.1109/ICASSP40776.2020.9054733},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054253,
author={T. {Afouras} and J. S. {Chung} and A. {Zisserman}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={ASR is All You Need: Cross-Modal Distillation for Lip Reading},
year={2020},
volume={},
number={},
pages={2143-2147},
abstract={The goal of this work is to train strong models for visual speech recognition without requiring human annotated ground truth data. We achieve this by distilling from an Automatic Speech Recognition (ASR) model that has been trained on a large-scale audio-only corpus. We use a cross-modal distillation method that combines Connectionist Temporal Classification (CTC) with a frame-wise cross-entropy loss. Our contributions are fourfold: (i) we show that ground truth transcriptions are not necessary to train a lip reading system; (ii) we show how arbitrary amounts of unlabelled video data can be leveraged to improve performance; (iii) we demonstrate that distillation significantly speeds up training; and, (iv) we obtain state-of-the-art results on the challenging LRS2 and LRS3 datasets for training only on publicly available data.},
keywords={Lip reading;cross-modal distillation},
doi={10.1109/ICASSP40776.2020.9054253},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053088,
author={O. V. {Thanh} and T. {Canham} and J. {Vazquez-Corral} and R. G. {Rodríguez} and M. {Bertalmío}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Color Stabilization for Multi-Camera Light-Field Imaging},
year={2020},
volume={},
number={},
pages={2148-2152},
abstract={By capturing a more complete rendition of scene light than standard 2D cameras, light-field technology represents an important step towards closing the gap between live action cinematography and computer graphics. Light-field cameras accomplish this by simultaneously capturing the same scene under different angular configurations, providing directional information that allows for a multitude of post-production effects. Among the practical challenges related to capturing multiple images simultaneously, a very important problem is the fact that the different images do not perfectly match in terms of color, which severely complicates all further processing. In this work we adapt and extend to the light-field scenario a color stabilization method previously proposed for standard multi-camera shoots, and demonstrate experimentally that it provides an improvement over the state-of-the-art techniques for light-field imaging.},
keywords={light field imaging;color stabilization;color matching;camera post-processing},
doi={10.1109/ICASSP40776.2020.9053088},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054757,
author={H. {Chen} and X. {Xing} and X. {Xu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Spatio-Temporal Convolutional Network for Real-Time Object Tracking},
year={2020},
volume={},
number={},
pages={2153-2157},
abstract={Siamese series of tracking networks have shown great potentials in achieving balanced accuracy and beyond real-time speed. However, most of existing siamese trackers only consider appearance features of first frame, and hardly benefit from interframe information. The lack of latest temporal transformation degrades the tracking performance during challenges such as deformation and partial occlusion. In this paper we focus on making using of the rich information in latest consecutive frames to improve the feature representation of initial template frame. Specifically, the latest frames after 3d convolution are used to generate an attention map, which is then point-wise multiplied by the features of first frame to obtain the updated template. With the attention map, the template can adaptively cope with the deformation and occlusion of the target. Since the first frame is always used as the basis of the template, there is no cumulative error when using the latest frames for attention. Due to the shared 2d convolution of all frames, the feature map results can be reused so that the added module has almost no time-consuming effects. This module is easily embedded into different siamese trackers. Through verification, the module has significantly improved tracking performance in different backbone situations.},
keywords={Online adaption;3d convolution;realtime tracking;siamese},
doi={10.1109/ICASSP40776.2020.9054757},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053413,
author={Z. {Cheng} and H. {Sun} and M. {Takeuchi} and J. {Katto}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learned Lossless Image Compression with A Hyperprior and Discretized Gaussian Mixture Likelihoods},
year={2020},
volume={},
number={},
pages={2158-2162},
abstract={Lossless image compression is an important task in the field of multimedia communication. Traditional image codecs typically support lossless mode, such as WebP, JPEG2000, FLIF. Recently, deep learning based approaches have started to show the potential at this point. HyperPrior is an effective technique proposed for lossy image compression. This paper generalizes the hyperprior from lossy model to lossless compression, and proposes a L2-norm term into the loss function to speed up training procedure. Besides, this paper also investigated different parameterized models for latent codes, and propose to use Gaussian mixture likelihoods to achieve adaptive and flexible context models. Experimental results validate our method can outperform existing deep learning based lossless compression, and outperform the JPEG2000 and WebP for JPG images.},
keywords={Lossless Image Compression;Deep Learning;HyperPrior;Gaussian Mixture Model},
doi={10.1109/ICASSP40776.2020.9053413},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053885,
author={T. {Chen} and Z. {Ma}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Variable Bitrate Image Compression with Quality Scaling Factors},
year={2020},
volume={},
number={},
pages={2163-2167},
abstract={Recently, learned image compression has emerged with significant coding efficiency improvement, and even shown noticeable gains over the state-of-the-art traditional codecs. In the mean time, most existing methods need to train separate models for different bitrate target. In this paper, we propose to embed a set of quality scaling factors (SFs) into learned image compression network, by which we can encode images across an entire bitrate range with a single model. This solution offers the comparable performance with those default approaches requiring multiple bitrate dependent models, and reduces the complexity significantly for practical implementation. Our work also demonstrates the generalization for various compression network structures, image contents, and training loss functions.},
keywords={Learned image compression;variable bitrates;quality scaling factor},
doi={10.1109/ICASSP40776.2020.9053885},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053997,
author={T. {LADUNE} and P. {PHILIPPE} and W. {HAMIDOUCHE} and L. {ZHANG} and O. {DÉFORGES}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Binary Probability Model for Learning Based Image Compression},
year={2020},
volume={},
number={},
pages={2168-2172},
abstract={In this paper, we propose to enhance learned image compression systems with a richer probability model for the latent variables. Previous works model the latents with a Gaussian or a Laplace distribution. Inspired by binary arithmetic coding, we propose to signal the latents with three binary values and one integer, with different probability models.A relaxation method is designed to perform gradient-based training. The richer probability model results in a better entropy coding leading to lower rate. Experiments under the Challenge on Learned Image Compression (CLIC) test conditions demonstrate that this method achieves 18 % rate saving compared to Gaussian or Laplace models.},
keywords={Image Coding;Autoencoder;Entropy Coding;Convolutional Neural Network},
doi={10.1109/ICASSP40776.2020.9053997},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053526,
author={T. {Strutz}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improved Probability Modelling for Exception Handling in Lossless Screen Content Coding},
year={2020},
volume={},
number={},
pages={2173-2177},
abstract={Competitive methods for lossless screen content coding are based on modelling of probability distributions. The most effective approach for losslessly compressing images with up to 90 000 colours is known as ‘soft context formation’ (SCF). It scans the image to be compressed for repeating patterns and creates corresponding colour histograms. This yields excellent compression results as long as the actual pattern-based histogram contains the colour of the next pixel to be processed. If this colour is not contained, an exception handling is required by sending a special non-colour symbol (escape).This paper proposes an enhanced version of this exception handling coding. Instead of only using a pattern-based estimation, the probability of the escape symbol is now additionally modelled based on local dependencies of exception events. The combination of both estimates leads to 2.0% decrease of bitrate. In comparison to FLIF, FP8v3, and HEVC (HM-16.20+SCM-8.8), the entire SCF method achieves savings of about 26%, 8%, and 12% on average for images with less than 90 000 different colours (for 720p format).},
keywords={probability distribution modelling;lossless coding;colour image compression;screen content coding},
doi={10.1109/ICASSP40776.2020.9053526},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053374,
author={I. {Storch} and B. {Zatt} and L. {Agostini} and G. {Correa} and L. A. {da Silva Cruz} and D. {Palomino}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Spatially Adaptive Intra Mode Pre-Selection for ERP 360 Video Coding},
year={2020},
volume={},
number={},
pages={2178-2182},
abstract={In this work, we propose a spatially adaptive HEVC intra mode pre-selection for equirectangular (ERP) 360 video coding. The proposed technique exploits the spatial characteristics of 360 video in the ERP projection to reduce the complexity of intra prediction mode selection. The number of intra modes evaluated in Rate-Distortion Optimization is reduced based on a score technique that is adaptive to the frame region being encoded. Results show that the proposed technique achieves a complexity reduction of 16.5% with low coding efficiency penalties.},
keywords={360 video coding;complexity reduction;intra prediction},
doi={10.1109/ICASSP40776.2020.9053374},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054166,
author={X. {Jiang} and C. {Yang} and G. {Cheung} and S. {Takamura}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Semi-Regular Geometric Kernel Encoding Reconstruction for Video Compression},
year={2020},
volume={},
number={},
pages={2183-2187},
abstract={Conventional video coding schemes employ a hybrid motion prediction / residual transform coding paradigm, which only exploits redundancy in individual pairs of video frames for compression gain. However, rigid geometric structures in 3D space—e.g., a building in a scene’s background—persist across time in a large frame group. Thus if one can extract and encode the geometric structure, then redundancy across the entire frame group can be removed in one shot. In this paper, we extract a best-fitting "semi-regular" geometric structure from a target spatial region in a frame group, which is encoded separately as a unified signal predictor for these frames. By semi-regular, we mean its geometry is simple enough that its shape parameters can be encoded cheaply. This semi-regular structure kernel approximates the 3D shape of an object in the video, on which we project pixels from the frame group to a carefully spaced 2D grid overlaid on the kernel. We encode the projected pixels as an intra-frame using HEVC. The decoded pixels are then back-projected to each frame as the predictor, and the resulting prediction residuals are transform-coded. Experimental results show that employing a semi-regular geometric kernel—a folded 2D plane in our realization—improves coding performance over native HEVC implementation and our previous regular kernel based scheme.},
keywords={Video coding;structure-from-motion;3D model reconstruction},
doi={10.1109/ICASSP40776.2020.9054166},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053851,
author={A. {Ahmmed} and M. {Murshed} and M. {Paul}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Leveraging Cuboids for Better Motion Modeling in High Efficiency Video Coding},
year={2020},
volume={},
number={},
pages={2188-2192},
abstract={In conventional video compression systems, motion model is used to approximate the geometry of moving object boundaries. It is possible to relieve motion model from describing discontinuities in the underlying motion field, by incorporating motion hint that can predict the spatial structure of future frames using the structure of reference frames. However, formation of highly accurate motion hint is computationally demanding, in particular for high resolution video sequences. Cuboids, rectangular regions derived using statistical features, attempt to separate out different objects present in the scene; they are computationally efficient and have sparse representation. Leveraging on the advantages of cuboids, in this paper, we propose to discover homogeneous motion regions and their associated motion based on cuboids. Afterwards, the estimated motion models and their domains are applied to form a prediction of the current frame. Experimental results show that a savings in bit rate of 3.96% is achievable over standalone HEVC reference, if this predicted frame is used as an additional reference frame for the current frame.},
keywords={Homogeneous motion;cuboid;HEVC},
doi={10.1109/ICASSP40776.2020.9053851},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054165,
author={S. {Kim} and J. S. {Park} and C. G. {Bampis} and J. {Lee} and M. K. {Markey} and A. G. {Dimakis} and A. C. {Bovik}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adversarial Video Compression Guided by Soft Edge Detection},
year={2020},
volume={},
number={},
pages={2193-2197},
abstract={We propose a video compression framework using conditional Generative Adversarial Networks (GANs). We rely on two encoders: one that deploys a standard video codec and another one which generates low-level soft edge maps. For decoding, we use a standard video decoder as well as a decoder that is trained using a conditional GAN. Recent "deep" approaches to video compression require multiple videos to pre-train generative networks that conduct interpolation. By contrast, our scheme trains a generative decoder that requires only a small number of key frames and edge maps taken from a single video, without any interpolation. Experiments on two video datasets demonstrate that the proposed GAN-based compression engine is a promising alternative to traditional video codec approaches that can achieve higher quality reconstructions for very low bitrates.},
keywords={Video codec;Soft edge detector;conditional Generative Adversarial Networks},
doi={10.1109/ICASSP40776.2020.9054165},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054255,
author={F. {Jost} and P. {Peter} and J. {Weickert}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Compressing Flow Fields with Edge-Aware Homogeneous Diffusion Inpainting},
year={2020},
volume={},
number={},
pages={2198-2202},
abstract={In spite of the fact that efficient compression methods for dense two-dimensional flow fields would be very useful for modern video codecs, hardly any research has been performed in this area so far. Our paper addresses this problem by proposing the first lossy diffusion-based codec for this purpose. It keeps only a few flow vectors on a coarse grid. Additionally stored edge locations ensure the accurate representation of discontinuities. In the decoding step, the missing information is recovered by homogeneous diffusion inpainting that incorporates the stored edges as reflecting boundary conditions. In spite of the simple nature of this codec, our experiments show that it achieves remarkable quality for compression ratios up to 800 : 1.},
keywords={Flow Fields;Inpainting-based Compression;Homogeneous Diffusion;Discontinuity Preservation},
doi={10.1109/ICASSP40776.2020.9054255},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054522,
author={J. {Schneider} and J. {Sauer} and C. {Rohlfing}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adaptive Resolution Change Using Uncoded Areas and Dictionary Learning-Based Super-Resolution in Versatile Video Coding},
year={2020},
volume={},
number={},
pages={2203-2207},
abstract={The concept of Adaptive Resolution Change (ARC) in video coding is already known from former international standards such as MPEG-4 [1]. However, in MPEG-4 linear filters are used for upsampling, which is crucial to coding video at varying resolution. With the rise of machine learning-based super-resolution methods in the last decade, powerful algorithms outperforming conventional upsampling methods were developed. This contribution introduces an ARC concept using un-coded areas within a frame of a video sequence and a Dictionary Learning (DL)-based Super-Resolution (SR) scheme. In this concept, a frame contains a picture at different resolution levels, which are spatially separated by the use of slices and tiles. Generally, a tile can be marked as coded or un-coded. Slices which do not hold any coded tiles are omitted from the bitstream. Thus, only one resolution level needs to be coded, while the other is generated at the decoder side. At the encoder a rate-distortion decision is made in order to decide, which resolution level should be coded. Simulation results show that gains with respect to the Versatile Video Coding (VVC) standard in development can be achieved at low bitrates.},
keywords={video compression;adaptive resolution change;dictionary learning;versatile video coding},
doi={10.1109/ICASSP40776.2020.9054522},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052943,
author={Í. D. {Machado} and M. S. {de Aguiar} and M. {Porto} and G. {Corrêa} and D. {Palomino} and B. {Zatt}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={RDE-MOGA: Automatic Selection of Rate-Distortion-Energy Control Points for Video Encoders Using Muti-Objetive Genetic Algorithm},
year={2020},
volume={},
number={},
pages={2208-2212},
abstract={Controlling energy consumption of video encoders is a complex multi-objective optimization problem of great importance. In this work we propose the RDE-MOGA, an multi-objective genetic algorithm capable of finding energetically efficient configurations for the HEVC encoder and replacing the current sensitivity analysis methodologies in the development of energy controllers. The utilization of our algorithm improved its efficiency in 60% whereas increasing the range of achievable reductions of the controller in at least 50%. Furthermore, the algorithm proved capable of sustaining 30% energy reduction at a cost of 3.45 BD-BR loss.},
keywords={Energy Control;Video Coding;Genetic Algorithm},
doi={10.1109/ICASSP40776.2020.9052943},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054651,
author={W. {Pu} and B. {Sober} and N. {Daly} and C. {Higgitt} and I. {Daubechies} and M. R. D. {Rodrigues}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Connected Auto-Encoders Based Approach for Image Separation with Side Information: With Applications to Art Investigation},
year={2020},
volume={},
number={},
pages={2213-2217},
abstract={X-radiography is a widely used imaging technique in art investigation, whether to investigate the condition of a painting or provide insights into artists’ techniques and working methods. In this paper, we propose a new architecture based on the use of ‘connected’ auto-encoders in order to separate mixed X-ray images acquired from double-sided paintings, where in addition to the mixed X-ray image one can also exploit the two RGB images associated with the front and back of the painting. This proposed architecture uses convolutional auto-encoders that extract features from the RGB images that can be employed to (1) reproduce both of the original RGB images, (2) reconstruct the associated separated X-ray images, and (3) regenerate the mixed X-ray image. It operates in a totally self-supervised fashion without the need for examples containing both the mixed X-ray images and the separated ones. Based on images from the double-sided wing panels from the famous Ghent Altarpiece, painted in 1432 by the brothers Hubert and Jan Van Eyck, the proposed algorithm has been experimentally verified to outperform state-of-the-art X-ray separation methods in art investigation applications.},
keywords={X-ray imaging;Painting;Decoding;Art;Image reconstruction;Deep learning;Training;Image separation with side information;deep neural networks;convolutional neural networks;auto-encoders},
doi={10.1109/ICASSP40776.2020.9054651},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054475,
author={K. {Chen} and Y. {Chen} and H. {Zhou} and X. {Mao} and Y. {Li} and Y. {He} and H. {Xue} and W. {Zhang} and N. {Yu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Self-Supervised Adversarial Training},
year={2020},
volume={},
number={},
pages={2218-2222},
abstract={Recent work has demonstrated that neural networks are vulnerable to adversarial examples. To escape from the predicament, many works try to harden the model in various ways, in which adversarial training is an effective way which learns robust feature representation so as to resist adversarial attacks. Meanwhile, the self-supervised learning aims to learn robust and semantic embedding from data itself. With these views, we introduce self-supervised learning to against adversarial examples in this paper. Specifically, the self-supervised representation coupled with k-Nearest Neighbour is proposed for classification. To further strengthen the defense ability, self-supervised adversarial training is proposed, which maximizes the mutual information between the representations of original examples and the corresponding adversarial examples. Experimental results show that the self-supervised representation outperforms its supervised version in respect of robustness and self-supervised adversarial training can further improve the defense ability efficiently.},
keywords={Adversarial training;self-supervised;defense;kNN},
doi={10.1109/ICASSP40776.2020.9054475},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054432,
author={M. M. {Johari} and H. {Behroozi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Gray-Scale Image Colorization Using Cycle-Consistent Generative Adversarial Networks with Residual Structure Enhancer},
year={2020},
volume={},
number={},
pages={2223-2227},
abstract={The colorization of gray-scale images has always been a challenging task in computer vision. Recently, novel approaches have been introduced for unsupervised image translation between two domains using Generative Adversarial Networks (GANs). Since one can consider the gray-scale and colorful images as two separate domains, we propose a two-stage cycle-consistent network architecture to produce convincible images. First, an intermediate image is generated with a relatively uncomplicated objective function at the output. Next, at the second stage, the intermediate image is enhanced via a residual network structure with a more complicated objective function. Furthermore, by employing two inverse networks, a cycle-consistent architecture is formed at both stages. The proposed model is trained on the ImageNet dataset, and the achieved outcomes demonstrate exceptional performance comparing with the state-of-the-art models.},
keywords={Cycle-Consistency;Generative Adversarial Networks;Image Colorization;Residual Structure},
doi={10.1109/ICASSP40776.2020.9054432},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053679,
author={M. {Cao} and Y. {Zou}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={All You Need is a Second Look: Towards Tighter Arbitrary Shape Text Detection},
year={2020},
volume={},
number={},
pages={2228-2232},
abstract={Deep learning-based scene text detection methods have progressed substantially over the past years. However, there remain several problems to be solved. Generally, long curve text instances tend to be fragmented because of the limited receptive field size of CNN. Besides, simple representations using rectangle or quadrangle bounding boxes fall short when dealing with more challenging arbitrary-shaped texts. In addition, the scale of text instances varies greatly which leads to the difficulty of accurate prediction through a single segmentation network. To address these problems, we innovatively propose a two-stage segmentation based arbitrary text detector named NASK (Need A Second looK). Specifically, NASK consists of a Text Instance Segmentation network namely TIS (1st stage), a Text RoI Pooling module and a Fiducial pOint eXpression module termed as FOX (2nd stage). Firstly, TIS conducts instance segmentation to obtain rectangle text proposals with a proposed Group Spatial and Channel Attention module (GSCA) to augment the feature expression. Then, Text RoI Pooling transforms these rectangles to the fixed size. Finally, FOX is introduced to reconstruct text instances with a more tighter representation using the predicted geometrical attributes including text center line, text line orientation, character scale and character orientation. Experimental results on two public benchmarks including Total-Text and SCUT-CTW1500 have demonstrated that the proposed NASK achieves state-of-the-art results.},
keywords={Scene text detection;Self-attention;Two-stage segmentation;Curve text},
doi={10.1109/ICASSP40776.2020.9053679},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053509,
author={L. {Ke} and M. {Pan} and W. {Wen} and D. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Compare Learning: Bi-Attention Network for Few-Shot Learning},
year={2020},
volume={},
number={},
pages={2233-2237},
abstract={Learning with few labeled data is a key challenge for visual recognition, as deep neural networks tend to overfit using a few samples only. One of the Few-shot learning methods called metric learning addresses this challenge by first learning a deep distance metric to determine whether a pair of images belong to the same category, then applying the trained metric to instances from other test set with limited labels. This method makes the most of the few samples and limits the overfitting effectively. However, extant metric networks usually employ Linear classifiers or Convolutional neural networks (CNN) that are not precise enough to globally capture the subtle differences between vectors. In this paper, we propose a novel approach named Bi-attention network to compare the instances, which can measure the similarity between embeddings of instances precisely, globally and efficiently. We verify the effectiveness of our model on two benchmarks. Experiments show that our approach achieved improved accuracy and convergence speed over baseline models.},
keywords={Few-shot;Bi-attention;Compare learning;Metric learning},
doi={10.1109/ICASSP40776.2020.9053509},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054318,
author={R. {Li} and H. {Liu} and Y. {Zhu} and Z. {Bai}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Arnet: Attention-Based Refinement Network for Few-Shot Semantic Segmentation},
year={2020},
volume={},
number={},
pages={2238-2242},
abstract={Semantic segmentation is a challenging task for computer vision which aims to classify the objects from the pixel level. Previous methods based on deep learning have made some progress but the labeling work is very time-consuming. Few-shot semantic segmentation can alleviate this problem. In this paper, we propose an Attention-based Refinement Network (ARNet) for few-shot semantic segmentation, which consists of three branches: the guidance branch, the segmentation branch and the refinement branch. The Residual Attention Module (RAM) can highlight the features from segmentation branch, giving a better guidance to refinement brach. And the Parallel Dilated Convolution Module (PDCM) in the end of refinement branch can refine the segmentation results. Experiments on PASCAL VOC 2012 dataset show that our model achieves a mean Intersection-over-Union (mIoU) score of 48.1% for one-shot segmentation and 49.1% for five-shot segmentation, outperforming state-of-the-art methods by 1.8% and 2.0%, respectively.},
keywords={Semantic Segmentation;Few-shot Learning;Attention Mechanism;Deep learning},
doi={10.1109/ICASSP40776.2020.9054318},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054101,
author={Q. {Tang} and J. {Li} and Z. {Shi} and Y. {Hu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Lightdet: A Lightweight and Accurate Object Detection Network},
year={2020},
volume={},
number={},
pages={2243-2247},
abstract={The extensive computational burden limits the usage of accurate but complex object detectors in resource-bounded scenarios. In this paper, we present a lightweight object detector, named LightDet, to address this dilemma. We design a lightweight backbone that is able to capture rich low-level features by the proposed Detail-Preserving Module. To effectively aggregate bottom and top-down features, we introduce an efficient Feature-Preserving and Refinement Module. A lightweight prediction head is employed to further reduce the entire network complexity. Experimental results show that our LightDet achieves 75.5% mAP on PASCAL VOC 2007 at the speed of 250 FPS and 24.0% mAP on MS COCO dataset.},
keywords={Lightweight;object detection;real-time},
doi={10.1109/ICASSP40776.2020.9054101},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054191,
author={C. {Chao} and P. {Hsu} and H. {Lee} and Y. F. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Self-Supervised Deep Learning for Fisheye Image Rectification},
year={2020},
volume={},
number={},
pages={2248-2252},
abstract={To rectify fisheye distortion from a single image, we advance self-supervised learning strategies and propose a unique deep learning model of Fisheye GAN (FE-GAN). Our FE-GAN learns pixel-level distortion flow from sets of fisheye distorted images and distortion-free ones (but not requiring such correspondences), with unique cross-rotation and intra-warping consistency introduced. With such novel self-supervised learning techniques, our FEGAN is able to recover the distortion-free image directly from the single fisheye image input. Our experiments quantitatively and qualitative confirm the effectiveness and robustness of our proposed model, which performs favorably against recent GAN-based image translation models.},
keywords={fisheye camera;image rectification;generative adversarial network;deep learning;self-supervised learning},
doi={10.1109/ICASSP40776.2020.9054191},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053553,
author={X. {Zhu} and Y. {Xiao} and Y. {Zheng} and G. {Tan} and S. {Zhou}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sketchppnet: A Joint Pixel and Point Convolutional Neural Network For Low Resolution Sketch Image Recognition},
year={2020},
volume={},
number={},
pages={2253-2257},
abstract={Sketch recognition using deep neural networks have become a recent trend. However, traditional pixel (image) based convolutional neural networks show poor recognizing performance on low resolution (LR) sketch image due to the loss of image details. To solve this problem, we propose a joint pixel and point convolutional neural network for LR sketch image recognition. The network, equipped with both image convolution and point convolution, can simultaneously handle both the image and point representation of sketches. Furthermore, we propose a hybrid classifier, a corresponding loss function, and a training scheme to better extract features for recognition. Experimental results show that our method outperforms state-of-art deep neural networks.},
keywords={Low resolution;Sketch recognition;Pixel;Point;Convolutional neural network},
doi={10.1109/ICASSP40776.2020.9053553},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053659,
author={D. {Yang} and X. {Li} and X. {Dai} and R. {Zhang} and L. {Qi} and W. {Zhang} and Z. {Jiang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={All In One Network for Driver Attention Monitoring},
year={2020},
volume={},
number={},
pages={2258-2262},
abstract={Nowadays, driver drowsiness and driver distraction is considered as a major risk for fatal road accidents around the world. As a result, driver monitoring identifying is emerging as an essential function of automotive safety systems. Its basic features include head pose, gaze direction, yawning and eye state analysis. However, existing work has investigated algorithms to detect these tasks separately and was usually conducted under laboratory environments. To address this problem, we propose a multi-task learning CNN framework which simultaneously solve these tasks. The network is implemented by sharing common features and parameters of highly related tasks. Moreover, we propose Dual-Loss Block to decompose the pose estimation task into pose classification and coarse-to-fine regression and Objectcentric Aware Block to reduce orientation estimation errors. Thus, with such novel designs, our model not only achieves SOA results but also reduces the complexity of integrating into automotive safety systems. It runs at 10 fps on vehicle embedded systems which marks a momentous step for this field. More importantly, to facilitate other researchers, we publish our dataset FDUDrivers which contains 20000 images of 100 different drivers and covers various real driving environments. FDUDrivers might be the first comprehensive dataset regarding driver attention monitoring.},
keywords={Driver attention;Driver monitoring system;Drowsiness;Distraction;CNN},
doi={10.1109/ICASSP40776.2020.9053659},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053388,
author={Z. {Li} and R. {Togo} and T. {Ogawa} and M. {Haseyama}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unsupervised Domain Adaptation for Semantic Segmentation with Symmetric Adaptation Consistency},
year={2020},
volume={},
number={},
pages={2263-2267},
abstract={Unsupervised domain adaptation, which leverages label information from other domains to solve tasks on a domain without any labels, can alleviate the problem of the scarcity of labels and expensive labeling costs faced by supervised semantic segmentation. In this paper, we utilize adversarial learning and semi-supervised learning simultaneously to solve the task of unsupervised domain adaptation in semantic segmentation. We propose a new approach that trains two segmentation models with the adversarial learning symmetrically and further introduces the consistency between the outputs of the two models into the semi-supervised learning to improve the accuracy of pseudo labels which significantly affect the final adaptation performance. We achieve state-of-the-art semantic segmentation performance on the GTA5-to-Cityscapes scenario, a widely used benchmark setting in unsupervised domain adaptation.},
keywords={Unsupervised domain adaptation;urban scene semantic segmentation;adversarial learning;semi-supervised learning;symmetric adaptation consistency},
doi={10.1109/ICASSP40776.2020.9053388},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053966,
author={C. {Zhang} and Q. {Wang} and X. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={IQ-STAN: Image Quality Guided Spatio-Temporal Attention Network for License Plate Recognition},
year={2020},
volume={},
number={},
pages={2268-2272},
abstract={License plate recognition (LPR) is one of the essential components in intelligent transportation systems. Although the image processing algorithms for LPR have been extensively studied in the past several years, the recognition performance is still not satisfactory especially in unconstrained complex scenes. In order to tackle this issue, a novel deep multi-task learning-based method is proposed in this paper by introducing contextual information in multiple license plate frames. Specifically, an end-to-end trainable multi-task architecture, namely IQ-STAN, is developed by joint license plate recognition and image quality scoring. Moreover, we propose an image quality-guided spatio-temporal attention mechanism, which is utilized in the frame-level feature representation during the phase of plate recognition. Extensive experiments are conducted and the competitive results demonstrate the effectiveness of our proposed framework.},
keywords={Spatio-Temporal Attention;License Plate Recognition;Image Quality;Multi-Task;Multi-Frame},
doi={10.1109/ICASSP40776.2020.9053966},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053384,
author={E. {Moliner} and L. S. {Romero} and V. {Vilaplana}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Weakly Supervised Semantic Segmentation For Remote Sensing Hyperspectral Imaging},
year={2020},
volume={},
number={},
pages={2273-2277},
abstract={This paper studies the problem of training a semantic segmentation neural network with weak annotations, in order to be applied in aerial vegetation images from Teide National Park. It proposes a Deep Seeded Region Growing system which consists on training a semantic segmentation network from a set of seeds generated by a Support Vector Machine. A region growing algorithm module is applied to the seeds to progressively increase the pixel-level supervision. The proposed method performs better than an SVM, which is one of the most popular segmentation tools in remote sensing image applications.},
keywords={Weakly-supervised segmentation;remote sensing;hyperspectral image},
doi={10.1109/ICASSP40776.2020.9053384},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053705,
author={J. {Xia} and J. {Tian} and J. {Xing} and J. {Cheng} and J. {Zhang} and J. {Wen} and Z. {Li} and J. {Lou}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Social Data Assisted Multi-Modal Video Analysis For Saliency Detection},
year={2020},
volume={},
number={},
pages={2278-2282},
abstract={Video saliency should be taken into consideration to facilitate optimization of the end-to-end video production, delivery and consumption ecosystem to improve user experience at lowered cost. Although recent studies have significantly increased the accuracy of saliency prediction, the approaches are mostly video-centric, without considering any prior "bias" that viewers may have with regard to the video contents. In this paper, we propose a novel learning-based multi-modal method for optimizing user-oriented video analysis. In particular, we generate a face-popularity mask using face recognition results and popularity information obtained from social media, and combine it with conventional content-only saliency analysis to produce multi-modal popularity-motion features. A convolutional long short-term memory (ConvL- STM) network discovers temporal correlation of human attention across frames. Experiments show that our method outperforms the state-of-the-art video saliency prediction approaches in representing human viewing preferences in real world applications, and demonstrate the necessity as well as the potential for integrating user bias information into attention detection.},
keywords={Multi-modal analysis;video saliency;popularity;eye tracking},
doi={10.1109/ICASSP40776.2020.9053705},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054668,
author={X. {Zhang} and C. {Huo} and C. {Pan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={View-Angle Invariant Object Monitoring Without Image Registration},
year={2020},
volume={},
number={},
pages={2283-2287},
abstract={Object monitoring can be performed by change detection algorithms. However, for the image pair with a large perspective difference, the change detection performance is usually impacted by inaccurate image registration. To address the above difficulties, a novel object-specific change detection approach is proposed for object monitoring in this paper. In contrast to traditional approaches, the proposed approach is robust to view angle variation and does not require explicit image registration. Experiments demonstrate the effectiveness and advantages of the proposed approach.},
keywords={Object monitoring;object-specific change detection;view-angle invariant},
doi={10.1109/ICASSP40776.2020.9054668},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054195,
author={D. {Chen} and X. {Wu} and J. {Dong} and Y. {He} and H. {Xue} and F. {Mao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Hierarchical Sequence Representation with Graph Network},
year={2020},
volume={},
number={},
pages={2288-2292},
abstract={Video classification problem is a challenging task in computer vision. The performance of this task is highly relied on the scale of training data and the effectiveness of video embedding via a robust embedding network. Unsupervised solutions such as feature average pooling technique, as a simple label-independent and parameter-free based method, cannot efficiently represent the video sequences. While supervised methods, such as RNN, can improve the recognition accuracy. The performance of RNN based methods, however, is decreased with the increasing length of the videos and the hierarchical relationships between frames across events in the video. In this paper, we propose a novel video classification method based on a deep convolutional graph neural network (DCGN). The proposed method utilizes the characteristics of the hierarchical structure of the video, and performed multi-level embedding feature extraction on the video frame sequence through the graph network, and obtained a video representation which reflects the event semantics hierarchically. Experiments on YouTube-8M Large-Scale Video Understanding dataset show that our proposed model outperforms the commonly used RNN based models, verifying its effectiveness for video classification.},
keywords={Video Classification;Sequence Representation;Graph Neural Network;Deep Convolutional Neural Network},
doi={10.1109/ICASSP40776.2020.9054195},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054346,
author={G. {Song} and Y. {Kim} and K. {Chun} and K. M. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi Image Depth from Defocus Network with Boundary Cue for Dual Aperture Camera},
year={2020},
volume={},
number={},
pages={2293-2297},
abstract={In this paper, we estimate depth information using two defocused images from dual aperture camera. Recent advances in deep learning techniques have increased the accuracy of depth estimation. Besides, methods of using a defocused image in which an object is blurred according to a distance from a camera have been widely studied. We further improve the accuracy of the depth estimation by training the network using two images with different degrees of depth-of-field. Using images taken with different apertures for the same scene, we can determine the degree of blur in an image more accurately. In this work, we propose a novel deep convolutional network that estimates depth map using dual aperture images based on boundary cue. Our proposed method achieves state-of-the-art performance on a synthetically modified NYU-v2 dataset. In addition, we built a new camera using fast variable apertures to build a test environment in the real world. In particular, we collected a new dataset which consists of real world vehicle driving scenes. Our proposed work shows excellent performance in the new dataset.},
keywords={Depth from Defocus;dual aperture camera;depth estimation;deep network;boundary cue},
doi={10.1109/ICASSP40776.2020.9054346},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053363,
author={C. Y. {Altinigne} and D. {Thanou} and R. {Achanta}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Height and Weight Estimation from Unconstrained Images},
year={2020},
volume={},
number={},
pages={2298-2302},
abstract={We address the difficult problem of estimating the attributes of weight and height of individuals from pictures taken in completely unconstrained settings. We present a deep learning scheme that relies on simultaneous prediction of human silhouettes and skeletal joints as strong regularizers that improve the prediction of attributes such as height and weight. Apart from imparting robustness to the prediction of attributes, our regularization also allows for better visual interpretability of the attribute prediction. For height estimation, our method shows lower mean average error compared to the state of the art despite using a simpler approach. For weight estimation, which has hardly been addressed in the literature, we set a new benchmark.},
keywords={Biometrics;deep learning;height and weight prediction;skeletal joint prediction;segmentation;interpretability;regularization},
doi={10.1109/ICASSP40776.2020.9053363},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054677,
author={B. {Bhattarai} and S. {Baek} and R. {Bodur} and T. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sampling Strategies for GAN Synthetic Data},
year={2020},
volume={},
number={},
pages={2303-2307},
abstract={Generative Adversarial Networks (GANs) have been used widely to generate large volumes of synthetic data. This data is being utilised for augmenting with real examples in order to train deep Convolutional Neural Networks (CNNs). Studies have shown that the generated examples lack sufficient realism to train deep CNNs and are poor in diversity. Unlike previous studies of randomly augmenting the synthetic data with real data, we present our simple, effective and easy to implement synthetic data sampling methods to train deep CNNs more efficiently and accurately. To this end, we propose to maximally utilise the parameters learned during training of the GAN itself. These include discriminator’s realism confidence score and the confidence on the target label of the synthetic data. In addition to this, we explore reinforcement learning (RL) to automatically search a subset of meaningful synthetic examples from a large pool of GAN synthetic data. We evaluate our method on two challenging face attribute classification data sets viz. AffectNet and CelebA. Our extensive experiments clearly demonstrate the need of sampling synthetic data before augmentation, which also improves the performance of one of the state-of-the-art deep CNNs in vitro.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054677},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054252,
author={B. {Bhattarai} and R. {Bodur} and T. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Auglabel: Exploiting Word Representations to Augment Labels for Face Attribute Classification},
year={2020},
volume={},
number={},
pages={2308-2312},
abstract={Augmenting data in image space (eg. flipping, cropping etc) and activation space (eg. dropout) are being widely used to regularise deep neural networks and have been successfully applied on several computer vision tasks. Unlike previous works, which are mostly focused on doing augmentation in the aforementioned domains, we propose to do augmentation in label space. In this paper, we present a simple, yet effective novel method to generate fixed dimensional labels with continuous values for images by exploiting the word2vec – semantic representations – of the existing categorical labels. We then append these representations to existing categorical labels and train the model. We validated our idea on two challenging face attribute classification data sets viz. CelebA and LFWA. Our extensive experiments show that the augmented labels improve the performance of the competitive deep learning baseline and also attains the state-of-the-art performance.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054252},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053764,
author={C. {Du} and S. {Graham} and S. {Jin} and C. {Depp} and T. {Nguyen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Task Center-Of-Pressure Metrics Estimation from Skeleton Using Graph Convolutional Network},
year={2020},
volume={},
number={},
pages={2313-2317},
abstract={Center of pressure (COP) is an important measurement of postural and gait control in human biomechanical studies. A vision-based estimation of COP metrics offers a way to obtain these gold-standard metrics for the detection of balance and gait problems. In this paper, we propose an end-to-end framework to estimate the COP path length and the COP positions from the 3D skeleton, utilizing the spatial-temporal features learned by graph convolutional networks. We propose two single-task models for each metric and a multi-task approach jointly learning two metrics. To facilitate this line of research, we also release a novel 3D skeleton dataset containing a wide variety of action patterns with synchronized COP labels. The experiments on the dataset validate that our framework achieves state-of-the-art accuracies for both COP path length and COP position estimations, while the multitask approach could yield more accurate and robust performance on COP path length estimation compared to the single-task model.},
keywords={center of pressure;balance control;graph convolutional network;multi-task learning},
doi={10.1109/ICASSP40776.2020.9053764},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053319,
author={C. {Jin} and T. {Zhang} and W. {Kong} and T. {Li} and G. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Regression Before Classification for Temporal Action Detection},
year={2020},
volume={},
number={},
pages={1-5},
abstract={Action classification combined with location regression is a widely-utilized mechanism in existing temporal action detection methods. However, there exists an inconsistency problem between locations and categories of action instances in this mechanism. More specifically, while the location of the proposal has been refined by the regressor, the action classifier still uses input and loss corresponding to the outdated unrefined proposal to predict category. In this paper, we propose to eliminate this inconsistency by making two modifi-cations to the action classifier: 1) redirecting the classification loss to the refined proposal, and 2) rearranging the location regressor before the action classifier so that the feature of the refined proposal is fed to the classifier. Extensive experiments show that eliminating the inconsistency problem can significantly promote the detection performance. Our method achieves state-of-the-art performance for temporal action detection on the challenging THUMOS’14 dataset.},
keywords={Video Analysis;Temporal Action Detection;Action Classification;Location Regression},
doi={10.1109/ICASSP40776.2020.9053319},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054132,
author={M. {Zhai} and X. {Xiang} and N. {Lv} and A. E. {Saddik}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Task Learning in Autonomous Driving Scenarios Via Adaptive Feature Refinement Networks},
year={2020},
volume={},
number={},
pages={2323-2327},
abstract={Many deep learning applications benefit from multi-task learning with several related objectives. In autonomous driving scenarios, being able to accurately infer motion and spatial information is essential for scene understanding. In this paper, we combine an adaptive feature refinement module and a unified framework for joint learning of optical flow, depth and camera pose estimation in an unsupervised manner. The feature refinement module is embedded into motion estimation and depth prediction sub-networks, which can exploit more channel-wise relationships and contextual information for feature learning. Given a monocular video, our network firstly estimates depth and camera motion, and calculates rigid optical flow. Then, we design an auxiliary flow network for inferring non-rigid flow fields. In addition, a forward-backward consistency check is adopted for occlusion reasoning. Extensive experiments on KITTI dataset demonstrate that the proposed method achieves potential results comparing to recent deep learning networks.},
keywords={Deep learning;optical flow;depth estimation;feature refinement;monocular video},
doi={10.1109/ICASSP40776.2020.9054132},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053780,
author={X. {Shi} and X. {Li} and C. {Wu} and S. {Kong} and J. {Yang} and L. {He}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Real-Time Deep Network for Crowd Counting},
year={2020},
volume={},
number={},
pages={2328-2332},
abstract={Automatic analysis of highly crowded people has attracted extensive attention from computer vision research. Previous approaches for crowd counting have already achieved promising performance across various benchmarks. However, to deal with the real situation, we hope the model run as fast as possible while keeping accuracy. In this paper, we propose a compact convolutional neural network for crowd counting which learns a more efficient model with a small number of parameters. With three parallel filters executing the convolutional operation on the input image simultaneously at the front of the network, our model could achieve nearly real-time speed and save more computing resources. Experiments on two benchmarks show that our proposed method not only takes a balance between performance and efficiency which is more suitable for actual scenes but also is superior to existing light-weight models in speed.},
keywords={Crowd counting;compact convolutional neural network},
doi={10.1109/ICASSP40776.2020.9053780},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054179,
author={T. {Ghoniemy} and M. A. {Amer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Drift Detection and Correction Post-Tracking},
year={2020},
volume={},
number={},
pages={2333-2337},
abstract={Accurate object tracking is a challenging problem due to numerous factors, that may cause the tracker to drift away from the target object. Typically, the output of a tracker is a bounding box (BB); such BB may not well discriminate the object from its background and may not be centered correctly around the object. This paper proposes a method that first detects, at each frame, if a tracker tends to drift by analyzing saliency features of the output BB of a tracker, and then applies automatic seeded object segmentation on the BB to correct the drift once detected. Such segmentation is meant to relocate (recenter) the BB adaptive to the object segmented. As seeds, we propose to use SIFT and salient points conditioned they are non-background pixels. Different than related work, our approach thus models drift external to a base tracker by examining its output BB at each and corrects drift, as needed, by updating that BB adaptive to segmentation. We show the ability of the proposed method to significantly improve the tracking quality of base trackers. We also show that the proposed method outperforms by far segmentation-based trackers.},
keywords={Object tracking;drift detection;drift correction;saliency;SIFT points;object segmentation;video},
doi={10.1109/ICASSP40776.2020.9054179},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053783,
author={Y. {Liu} and Y. {Hsieh} and M. {Chen} and C. -. H. {Yang} and J. {Tegner} and Y. -. J. {Tsai}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Interpretable Self-Attention Temporal Reasoning for Driving Behavior Understanding},
year={2020},
volume={},
number={},
pages={2338-2342},
abstract={Performing driving behaviors based on causal reasoning is essential to ensure driving safety. In this work, we investigated how state-of-the-art 3D Convolutional Neural Networks (CNNs) perform on classifying driving behaviors based on causal reasoning. We proposed a perturbation-based visual explanation method to inspect the models’ performance visually. By examining the video attention saliency, we found that existing models could not precisely capture the causes (e.g., traffic light) of the specific action (e.g., stopping). Therefore, the Temporal Reasoning Block (TRB) was proposed and introduced to the models. With the TRB models, we achieved the accuracy of 86.3%, which outperform the state-of-the-art 3D CNNs from previous works. The attention saliency also demonstrated that TRB helped models focus on the causes more precisely. With both numerical and visual evaluations, we concluded that our proposed TRB models were able to provide accurate driving behavior prediction by learning the causal reasoning of the behaviors.},
keywords={Self-driving Vehicles;Driving Behaviors Reasoning;Action Recognition;Self-attention Models;Video Saliency},
doi={10.1109/ICASSP40776.2020.9053783},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053175,
author={K. {Guo} and N. {Kim} and D. {Seo} and I. {Kim} and S. {Chang} and D. {Min} and S. {Lim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Non-Uniform Video Time-Lapse Method Based on Motion Scenario and Stabilization Constraint},
year={2020},
volume={},
number={},
pages={2343-2347},
abstract={Time-lapse of user captured video becomes popular in many applications recently, non-uniform sampling and digital video stabilization (VS) are usually two independent steps to keep meaningful contents and provide stabilized output. However, non-uniform sampling may produce large sampling interval and then result in larger motion, this would beyond the stabilization ability of VS and produce unpleasant output. To address this problem, we propose a new auto time-lapse framework, which selects frames not only based on camera motion scenarios, but also refer to the smoothed camera trajectory and considering VS ability according to Field of View (FOV) loss constraint. More specific, we introduce an advanced Markov chain (MC) model, in which smoothed camera trajectory, FOV loss constraint of VS, camera motion scenario, and sampling interval similarity between consecutive frames are encoded as potential functions. Finally, dynamic programming (DP) is employed to find the best non-uniform sampling. Experimental results demonstrate that the proposed method not only achieves pleasant and meaningful non-uniform sampling, but also provides satisfactory stabilization performance. The whole algorithm can work in real time during video recording on mobile device.},
keywords={Hyper-lapse;Stabilization;Real time.},
doi={10.1109/ICASSP40776.2020.9053175},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054316,
author={H. {Li} and L. {Gao} and R. {Han} and L. {Wan} and W. {Feng}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Key Action and Joint CTC-Attention based Sign Language Recognition},
year={2020},
volume={},
number={},
pages={2348-2352},
abstract={Sign Language Recognition (SLR) translates sign language video into natural language. In practice, sign language video, owning a large number of redundant frames, is necessary to be selected the essential. However, unlike common video that describes actions, sign language video is characterized as continuous and dense action sequence, which is difficult to capture key actions corresponding to meaningful sentence. In this paper, we propose to hierarchically search key actions by a pyramid BiLSTM. Specifically, we first construct three BiL-STMs to produce temporal relationships among input video sequence. Then, we associate these BiLSTMs by searching the salient responses in two groups of fixed-scale sliding window and capture key actions. Additionally, in order to balance the sequence alignment and dependency, we propose to jointly train Connectionist Temporal Classification (CTC) and Long Short-Term Memory (LSTM). Experimental results demonstrate the effectiveness of the proposed method.},
keywords={Sign language recognition;Key action extraction;CTC and LSTM;Joint training},
doi={10.1109/ICASSP40776.2020.9054316},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054392,
author={T. {Huynh-The} and C. {Hua} and N. A. {Tu} and D. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Geometric Features with Dual–stream CNN for 3D Action Recognition},
year={2020},
volume={},
number={},
pages={2353-2357},
abstract={Recently, regarding several beneficial properties of depth camera, numerous 3D action recognition frameworks have studied high-level features by exploiting deep learning techniques, but nevertheless they cannot seize the meaningful characteristics of static human pose and dynamic action motion of a whole sequence. This paper introduces a deep network configured by two parallel streams of convolutional stacks for fully learning the deep intra-frame joint associations and inter-frame joint correlations, wherein the structure of each stream is learned from Inception-v3. In experiments, besides the compatibility verification with various backbone networks, the proposed approach achieves the state-of-theart performance in battle with several deep learning-based methods on the updated NTU RGB+D 120 dataset.},
keywords={Action recognition;deep convolutional neural network;geometric pose-transition feature.},
doi={10.1109/ICASSP40776.2020.9054392},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054167,
author={S. {Thermos} and P. {Daras} and G. {Potamianos}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Deep Learning Approach to Object Affordance Segmentation},
year={2020},
volume={},
number={},
pages={2358-2362},
abstract={Learning to understand and infer object functionalities is an important step towards robust visual intelligence. Significant research efforts have recently focused on segmenting the object parts that enable specific types of human-object interaction, the so-called "object affordances". However, most works treat it as a static semantic segmentation problem, focusing solely on object appearance and relying on strong supervision and object detection. In this paper, we propose a novel approach that exploits the spatio-temporal nature of human-object interaction for affordance segmentation. In particular, we design an autoencoder that is trained using ground-truth labels of only the last frame of the sequence, and is able to infer pixel-wise affordance labels in both videos and static images. Our model surpasses the need for object labels and bounding boxes by using a soft-attention mechanism that enables the implicit localization of the interaction hotspot. For evaluation purposes, we introduce the SOR3D-AFF corpus, which consists of human-object interaction sequences and supports 9 types of affordances in terms of pixel-wise annotation, covering typical manipulations of tool-like objects. We show that our model achieves competitive results compared to strongly supervised methods on SOR3D-AFF, while being able to predict affordances for similar unseen objects in two affordance image-only datasets.},
keywords={affordance;segmentation;human-object interaction;soft attention;deep neural networks},
doi={10.1109/ICASSP40776.2020.9054167},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054112,
author={A. {Xompero} and R. {Sanchez-Matilla} and A. {Modas} and P. {Frossard} and A. {Cavallaro}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-View Shape Estimation of Transparent Containers},
year={2020},
volume={},
number={},
pages={2363-2367},
abstract={The 3D localisation of an object and the estimation of its properties, such as shape and dimensions, are challenging under varying degrees of transparency and lighting conditions. In this paper, we propose a method for jointly localising container-like objects and estimating their dimensions using two wide-baseline, calibrated RGB cameras. Under the assumption of circular symmetry along the vertical axis, we estimate the dimensions of an object with a generative 3D sampling model of sparse circumferences, iterative shape fitting and image re-projection to verify the sampling hypotheses in each camera using semantic segmentation masks. We evaluate the proposed method on a novel dataset of objects with different degrees of transparency and captured under different backgrounds and illumination conditions. Our method, which is based on RGB images only, outperforms in terms of localisation success and dimension estimation accuracy a deep-learning based approach that uses depth maps.},
keywords={Object localisation;Dimension estimation;Transparency.},
doi={10.1109/ICASSP40776.2020.9054112},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053794,
author={J. {Wang} and S. {Li} and Z. {Duan} and Z. {Yuan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Rethinking Temporal-Related Sample for Human Action Recognition},
year={2020},
volume={},
number={},
pages={2368-2372},
abstract={Temporal-related samples always have huge intra-class appearance variation, on which lots of existing action recognition algorithms have poor performance. In this paper, our motivation is to address this issue by utilizing temporal information more effectively. A novel light-weight Voting-based Temporal Correlation module (VTC) is proposed to enhance temporal cues. VTC integrates sparse temporal sampling strategy into feature sequences, so it mitigates the effect of redundant information and focuses more on temporal modeling. Furthermore, we propose a simple and intuitive Similarity Loss (SL) to guide the training procedure for VTC. Introducing confusion in the predicted vector intentionally, SL eases intra-class variation by discovering class-specific common motion pattern rather than sample-specific discriminative information. Combining VTC and SL with complementary advances in this field, we clearly outperform state-of-the-art results on HMDB51, UCF101, and Something-something-v1 dataset. The code has been made publicly available on https://github.com/FingerRec/TRS.},
keywords={action recognition;temporal correlation;class-specific;temporal sampling},
doi={10.1109/ICASSP40776.2020.9053794},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053838,
author={J. {Liu} and Q. {Zhou} and Y. {Qiang} and B. {Kang} and X. {Wu} and B. {Zheng}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={FDDWNet: A Lightweight Convolutional Neural Network for Real-Time Semantic Segmentation},
year={2020},
volume={},
number={},
pages={2373-2377},
abstract={This paper introduces a lightweight convolutional neural network, called FDDWNet, for real-time accurate semantic segmentation. In contrast to recent advances of lightweight networks that prefer to utilize shallow structure, FDDWNet makes an effort to design more deeper network architecture, while maintains faster inference speed and higher segmentation accuracy. Our network uses factorized dilated depth-wise separable convolutions (FDDWC) to learn feature representations from different scale receptive fields with fewer model parameters. Additionally, FDDWNet has multiple branches of skipped connections to gather context cues from intermediate convolution layers. The experiments show that FDDWNet only has 0.8M model size, while achieves 60 FPS running speed on a single RTX 2080Ti GPU with a 1024 × 512 input image. The comprehensive experiments demonstrate that our model achieves state-of-the-art results in terms of available speed and accuracy trade-off on CityScapes and CamVid datasets.},
keywords={Lightweight network;Semantic segmentation;Factorized convolution;Dilated depthwise convolution},
doi={10.1109/ICASSP40776.2020.9053838},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053473,
author={S. {Paul} and C. {Torres} and S. {Chandrasekaran} and A. K. {Roy-Chowdhury}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Complex Pairwise Activity Analysis Via Instance Level Evolution Reasoning},
year={2020},
volume={},
number={},
pages={2378-2382},
abstract={Video activity analysis systems are often trained on large datasets. Activities and events in the real world do not occur in isolation, instead, they occur as interactions between related objects. This work introduces a novel method that jointly exploits relational information between pairs of objects and temporal dynamics of each object. The proposed method effectively leverages a new simple architecture that is flexible and easily trained to detect relational activities and events using small datasets (hundreds of samples). The solution is constructed and tested using synthetic videos of car-collision events. The annotated datasets in this work will be made available online to the research community. Experimental results demonstrate the efficacy of the network to perform complex activity analysis.},
keywords={activity recognition;pairwise activity;relational reasoning;temporal reasoning},
doi={10.1109/ICASSP40776.2020.9053473},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054269,
author={X. {Du} and T. {Ma} and Y. {Zheng} and H. {Ye} and X. {Wu} and L. {He}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Scene Text Recognition with Temporal Convolutional Encoder},
year={2020},
volume={},
number={},
pages={2383-2387},
abstract={Texts from scene images typically consist of several characters and exhibit a characteristic sequence structure. Existing methods capture the structure with the sequence-to-sequence models by an encoder to have the visual representations and then a decoder to translate the features into the label sequence. In this paper, we study text recognition framework by considering the long-term temporal dependencies in the encoder stage. We demonstrate that the proposed Temporal Convolutional Encoder with increased sequential extents improves the accuracy of text recognition. We also study the impact of different attention modules in convolutional blocks for learning accurate text representations. We conduct comparisons on seven datasets and the experiments demonstrate the effectiveness of our proposed approach.},
keywords={Scene text recognition;temporal convolutions;sequence model},
doi={10.1109/ICASSP40776.2020.9054269},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054394,
author={Y. {Wu} and H. {Wang} and S. {Wang} and Q. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Enhanced Action Tubelet Detector for Spatio-Temporal Video Action Detection},
year={2020},
volume={},
number={},
pages={2388-2392},
abstract={Current spatio-temporal action detection methods usually employ a two-stream architecture, a RGB stream for raw images and an auxiliary motion stream for optical flow. Training is required individually for each stream and more efforts are necessary to improve the precision of RGB stream. To this end, a single stream network named enhanced action tubelet (EAT) detector is proposed in this work based on RGB stream. A modulation layer is designed to modulate RGB features with conditional information from the visual clues of optical flow and human pose. This network is end-to-end and the proposed layer can be easily applied into other action detectors. Experiments show that EAT detector outperforms traditional RGB stream and is competitive to existing two-stream methods while free from the trouble of training streams separately. By being embedded in a new three-stream architecture, the resulting three-stream EAT detector achieves impressive performances among the best competitors on UCF-Sports, JHMDB and UCF-101.},
keywords={Action detection;single stream network;conditional information;modulation layer},
doi={10.1109/ICASSP40776.2020.9054394},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052992,
author={Y. {Wang} and T. {Nakachi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Secure Face Recognition in Edge and Cloud Networks: From the Ensemble Learning Perspective},
year={2020},
volume={},
number={},
pages={2393-2397},
abstract={Offloading the computationally intensive workloads to the edge and cloud not only improves the quality of computation, but also creates an extra degree of diversity by collecting information from devices in service, which, in turn, has raised significant concerns on privacy as the aggregated information could be misused without the permission by the third party. Sparse coding, which has been successful in computer vision, is finding application in this new domain. In this paper, we develop a secure face recognition framework to orchestrate sparse coding in edge and cloud networks. Specifically, 1). To protect the privacy, we develop a low-complexity encrypting algorithm based on random unitary transform, where its influence on dictionary learning and sparse representation is analysed. We further prove that such influence will not affect the accuracy of face recognition. 2). To fully utilize the multi-device diversity, we extract deeper features in an intermediate space, expanded according to the dictionaries from each device, and perform classification in this new feature space to combat the noise and modeling error.},
keywords={Face Recognition;Security;Edge and Cloud;Diversity;Sparse Representation},
doi={10.1109/ICASSP40776.2020.9052992},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053516,
author={M. {Zou} and J. {Tang} and G. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low Complexity Single Image Super-Resolution with Channel Splitting and Fusion Network},
year={2020},
volume={},
number={},
pages={2398-2402},
abstract={Recently, deep convolutional neural networks (CNNs) have made remarkable progress on single image super-resolution (SISR). However, many of these methods use very deep or wide convolutional layers to achieve good performance, which treat all feature channels indiscriminately and neglect the difference among the contribution of each channel to the output results. In this paper, we propose a low complexity solution based on channel splitting and fusion network (CSFN) to address this problem. Our method uses channel splitting and channel fusion to enhance feature maps and make full use of valuable information, and then multiple residual channel splitting and fusion blocks (CSFB) are cascaded to continuously extract more important information for reconstruction. To further minimize redundant parameters and improve efficiency, we adopt group and recursive con-volutional layer strategy in CSFB. Experiments demonstrate that our proposed CSFN could achieve higher performance with low computational complexity than most state-of-the-art methods.},
keywords={Image super-resolution;convolutional neural networks;deep learning},
doi={10.1109/ICASSP40776.2020.9053516},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054539,
author={X. {Wang} and J. {Chen} and C. {Richard} and D. {Brie}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Spectral-Spatial Prior Via 3DDNCNN for Hyperspectral Image Deconvolution},
year={2020},
volume={},
number={},
pages={2403-2407},
abstract={Hyperspectral image (HSI) deconvolution is an ill-posed problem aiming at recovering sharp images with tens or hundreds of spectral channels from blurred and noisy observations. In order to successfully conduct the deconvolution, proper priors are required to regularize the optimization problem. However, handcrafting a good regularizer may not be trivial and complex regularizers lead to difficulties in solving the optimization problem. In this paper, we use the alternating direction method of multipliers (ADMM) to decompose the optimization problem into iterative subproblems where the prior only appears in a denoising subproblem. Then a 3D denoising convolutional neural network (3DDnCNN) is designed and trained with data for solving this problem. In this way, the hyperspectral image deconvolution is then solved with a framework that integrates the optimization techniques and deep learning. Experimental results demonstrate the superiority of the proposed method with several blurring settings in both quantitative and qualitative comparisons.},
keywords={Hyperspectral image deconvolution;ADMM;spectral-spatial prior;3D convolution;deep learning},
doi={10.1109/ICASSP40776.2020.9054539},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053673,
author={D. {Manandhar} and M. {Bastan} and K. {Yap}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Dynamically Modulated Deep Metric Learning for Visual Search},
year={2020},
volume={},
number={},
pages={2408-2412},
abstract={This paper proposes dynamically modulated metric learning (DMML) for learning a tiered similarity space to perform visual search. Existing methods often treat the training samples having different degree of information with equal importance which hinders in capturing the underlying granularities in visual similarity. Proposed DMML automatically exploits the informativeness of samples during training by leveraging correlation between image attributes and embedding that are learned jointly. The two tasks are interlinked by supervising signals where the predicted attribute vectors are used to dynamically learn the loss function. To this end, we propose a new soft-binomial deviance loss that helps to capture the feature similarity space at multiple granularities. Compared to recent ensemble and attention based methods, our DMML framework is conceptually simple yet effective, and achieves state-of-the-art performances on standard benchmark datasets; e.g. an improvement of 4% Recall@1 over the SOTA [1] on DeepFashion dataset.},
keywords={Visual Search;Metric Loss;Embedding},
doi={10.1109/ICASSP40776.2020.9053673},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053201,
author={Z. {Pan} and B. {Li} and H. {Cheng} and Y. {Bao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Residual Network for MSFA Raw Image Denoising},
year={2020},
volume={},
number={},
pages={2413-2417},
abstract={Multispectral filter arrays (MSFA) is increasingly used in multispectral imaging. While many previous works studied the denoising algorithms for CFA based cameras, denoising MSFA raw images is little discussed. The major challenges for denoising MSFA data include 1) more channels than CFA and no predominant channel; 2) compatibility between denoising and the subsequent demosaicking process. To overcome these challenges, we propose a new deep residual network designed to account for the uniqueness of MSFA mosaic patterns. First, a split and stride convolution layer is innovated to match the mosaic pattern of the MSFA raw image. Then, data augmentation using MSFA shifting and dynamic noise is proposed to make the model robust to different noise levels. In addition, a new network optimization criteria is also suggested by using the noise standard deviation to normalize the L1 loss function. Comprehensive experiments demonstrate that the proposed deep residual network outperforms the state-of-the-art denoising algorithms in MSFA field.},
keywords={image denoising;multispectral filter array;deep residual network},
doi={10.1109/ICASSP40776.2020.9053201},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054479,
author={B. {Wei} and Y. {Yuan} and Q. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={MSPNET: Multi-Supervised Parallel Network for Crowd Counting},
year={2020},
volume={},
number={},
pages={2418-2422},
abstract={Crowd counting has a wide range of applications such as video surveillance and public safety. Many existing methods only focus on improving the accuracy of counting but ignore the importance of density maps. It’s no doubt that a high-quality density map contains more information such as localization and movement of the crowd. In this paper, we propose a multi-supervised parallel network (MSPNet) to achieve high accuracy of crowd counting and generate high-quality density maps. We conduct multiple supervisions in the training process, which can supplement the details lost in pooling and up-sampling operations to improve the quality of density maps. In addition, to reduce the impact of background noise, the attention mechanism is employed to help the network focus on the crowd. Extensive experiments on two mainstream benchmarks show that MSPNet achieves significantly improvement over the state-of-the-art in terms of counting accuracy and the quality of density maps.},
keywords={Crowd counting;high-quality density map;multiple supervisions;attention mechanism},
doi={10.1109/ICASSP40776.2020.9054479},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053476,
author={Y. {Wang} and H. {Su} and C. {Chang} and Z. {Liu} and W. H. {Hsu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning},
year={2020},
volume={},
number={},
pages={2423-2427},
abstract={We introduce a novel task, Video Question Generation (Video QG). A Video QG model automatically generates questions given a video clip and its corresponding dialogues. Video QG requires a range of skills – sentence comprehension, temporal relation, the interplay between vision and language, and the ability to ask meaningful questions. To address this, we propose a novel semantic rich cross-modal self-attention (SR-CMSA) network to aggregate the multi-modal and diverse features. To be more precise, we enhance the video frames semantic by integrating the object-level information, and we jointly consider the cross-modal attention for the video question generation task. Excitingly, our proposed model remarkably improves the baseline from 7.58 to 14.48 in the BLEU-4 score on the TVQA dataset. Most of all, we arguably pave a novel path toward understanding the challenging video input and we provide detailed analysis in terms of diversity, which ushers the avenues for future investigations.},
keywords={Video Question Generation;Cross-Modal Attention},
doi={10.1109/ICASSP40776.2020.9053476},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053341,
author={Z. {Dou} and K. {Gao} and X. {Zhang} and H. {Wang} and J. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Blind Hyperspectral Unmixing using Dual Branch Deep Autoencoder with Orthogonal Sparse Prior},
year={2020},
volume={},
number={},
pages={2428-2432},
abstract={Blind hyperspectral unmixing has become an important task for hyperspectral applications. In this paper, we propose a dual branch autoencoder with a novel sparse prior to simultaneously extract endmembers and abundances from the raw HSI. The dual branch structure extends the linear mixing model by only modeling linear mixtures of the endmembers and treating the bilinear interactions as error. In this way, the proposed model doesn’t require the assumptions of explicit forms of bilinear interactions. The proposed sparse prior, named as orthogonal sparse prior, is based on the key observation that the abundance vector of one pixel is very sparse, there are often no more than two non-zero elements. Different from the conventional norm-based sparse prior which assumes the abundance maps are independent, the orthogonal sparse prior explores the orthogonality between the abundance maps. Extensive experiments on two real datasets show that the proposed method significantly and consistently outperforms the compared state-of-the-art methods, with up to 50% improvements.},
keywords={Hyperspectral unmixing;Sparse prior;Autoencoder},
doi={10.1109/ICASSP40776.2020.9053341},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053904,
author={Z. {Li} and X. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Classification of Depth and Surface Edges with Deep Features},
year={2020},
volume={},
number={},
pages={2433-2437},
abstract={Edges in 2D images fall into two categories: depth edges and surface edges, depending on if the edge corresponds to an abrupt change in depth (the distance from the camera). This edge type is an efficient, robust, and effective information in many applications. In this paper we study the problem of automatic classification of the two types of edges. We use features discovered by deep convolutional neural networks to predict the edge type. The labeled sample edges for training our classifiers are semiautomatically generated via a technique of the edge-segment geometrical duality. Experiments are carried out to demonstrate the effectiveness of the proposed edge type classification methods.},
keywords={Edge detection and classification;machine learning},
doi={10.1109/ICASSP40776.2020.9053904},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052933,
author={X. {Mao} and Y. {Chen} and Y. {Li} and Y. {He} and H. {Xue}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning to Characterize Adversarial Subspaces},
year={2020},
volume={},
number={},
pages={2438-2442},
abstract={Deep Neural Networks (DNNs) are known to be vulnerable to the maliciously generated adversarial examples. To detect these adversarial examples, previous methods use artificially designed metrics to characterize the properties of adversarial subspaces where adversarial examples lie. However, we find these methods are not working in practical attack detection scenarios. Because the artificially defined features are lack of robustness and show limitation in discriminative power to detect strong attacks. To solve this problem, we propose a novel adversarial detection method which identifies adversaries by adaptively learning reasonable metrics to characterize adversarial subspaces. As auxiliary context information, k nearest neighbors are used to represent the surrounded subspace of the detected sample. We propose an innovative model called Neighbor Context Encoder (NCE) to learn from k neighbors context and infer if the detected sample is normal or adversarial. We conduct thorough experiment on CIFAR-10, CIFAR-100 and ImageNet dataset. The results demonstrate that our approach surpasses all existing methods under three settings: attack-aware black-box detection, attack-unaware black-box detection and white-box detection.},
keywords={Adversarial examples;subspaces;attack detection},
doi={10.1109/ICASSP40776.2020.9052933},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054514,
author={F. {Yang} and L. {Xiao} and J. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Video Deblurring Via 3d CNN and Fourier Accumulation Learning},
year={2020},
volume={},
number={},
pages={2443-2447},
abstract={Camera shake and target movement often leads to undesirable image blurring in videos. How to exploit spatial-temporal information of adjacent frames and reduce the processing time of deblurring are two major issues in video deblurring. In this paper, we propose a simple yet effective Fourier accumulation embedded 3D convolutional encoder-decoder network for video deblurring. Firstly, a 3D convolutional encoder-decoder module is constructed to extract multiscale spatial-temporal deep features and generate intermediate deblurred frames with complementary information which is beneficial for the deblurring of each frame. Then we embed a Fourier accumulation module following the 3D convolutional encoder-decoder, the Fourier accumulation module could fuse intermediate deblurred frames with learned weights in Fourier domain and then produce shaper deblurred frames. Experimental results show that our method has competitive performance compared with other state-of-the-art methods.},
keywords={video deblurring;encoder-decoder;3D convolution;Fourier accumulation},
doi={10.1109/ICASSP40776.2020.9054514},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054630,
author={H. {Ma} and G. {Liu} and Y. {Yuan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Enhanced Non-Local Cascading Network with Attention Mechanism for Hyperspectral Image Denoising},
year={2020},
volume={},
number={},
pages={2448-2452},
abstract={Because of the complexity of imaging environment, hyper-spectral remote sensing images (HSIs) often suffer from different kinds of noise. Despite the success in natural image denoising, most of the existing CNN-based HSIs denoising methods still suffer from the problem of inadequate noise suppression and insufficient feature extraction. In this paper, a novel HSIs denoising algorithm based on an enhanced non-local cascading network with attention mechanism (ENCAM) is proposed, which can extract the joint spatial-spectral feature more effectively. The main contributions include: (1) the non-local structure is introduced to enlarge the receptive field to extract the spatial features more effectively; (2) multi-scale convolutions and channel attention module are applied to enhance extracted multi-scale features; (3) a cascading residual dense structure is used to extract different frequency features. Both of the theoretical analysis and the experiments indicate that the proposed method is superior to the other state-of-the-art methods on HSIs denoising.},
keywords={Hyperspectral Images;Denoising;Enhanced Non-local Cascading Network;Channel Attention},
doi={10.1109/ICASSP40776.2020.9054630},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053151,
author={A. {Aidini} and G. {Tsagkatakis} and P. {Tsakalides}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Quantized Tensor Robust Principal Component Analysis},
year={2020},
volume={},
number={},
pages={2453-2457},
abstract={High-dimensional data structures, known as tensors, are fundamental in many applications, including multispectral imaging and color video processing. Compression of such huge amount of multidimensional data collected over time is of paramount importance, necessitating the process of quantization of measurements into discrete values. Furthermore, noise and issues related to the acquisition and transmission of signals frequently lead to unobserved, lost or corrupted measurements. In this paper, we introduce a tensor robust principal component analysis algorithm in order to recover a tensor with real-valued entries from a partly observed set of quantized and sparsely corrupted entries. We formulate the problem as a constrained maximum likelihood estimation of the sum of a low-rank tensor and a sparse tensor, through matricizations in each mode, in combination with a quantization and statistical measurement model. Experimental results on satellite derived land surface time-series demonstrate that directly operating with the quantized measurements, rather than treating them as real values, results in a low recovery error, while the proposed method is also capable of detecting temperature anomalies (e.g., forest fires).},
keywords={Tensors;Robust PCA;Quantization;Missing values;Image time-series},
doi={10.1109/ICASSP40776.2020.9053151},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054135,
author={S. {Long} and Y. {Guan} and K. {Bian} and C. {Yao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A New Perspective for Flexible Feature Gathering in Scene Text Recognition Via Character Anchor Pooling},
year={2020},
volume={},
number={},
pages={2458-2462},
abstract={Irregular scene text recognition has attracted much attention from the research community, mainly due to the complexity of shapes of text in natural scene. However, recent methods either rely on shape-sensitive modules such as bounding box regression, or discard sequence learning. To tackle these issues, we propose a pair of coupling modules, termed as Character Anchoring Module (CAM) and Anchor Pooling Module (APM), to extract high-level semantics from two-dimensional space to form feature sequences. The proposed CAM localizes the text in a shape-insensitive way by design by anchoring characters individually. APM then interpolates and gathers features flexibly along the character anchors which enables sequence learning. The complementary modules realize a harmonic unification of spatial information and sequence learning. With the proposed modules, our recognition system surpasses previous state-of-the-art scores on irregular and perspective text datasets, including, ICDAR 2015, CUTE, and Total-Text, while paralleling state-of-the-art performance on regular text datasets.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054135},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054627,
author={X. {Fu} and B. {Fang} and M. {Zhou} and J. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Hybrid Active Contour Driven by Double-Weighted Signed Pressure Force for Image Segmentation},
year={2020},
volume={},
number={},
pages={2463-2467},
abstract={In this paper, we proposed a novel hybrid active contour driven by double-weighted signed pressure force method for image segmentation. First, the Legendre polynomials and global information are integrated into the signed pressure force (SPF) function and a coefficient is applied to weight the effect degrees of the Legendre term and global term. Second, by introducing a weighted factor as the coefficient of inside and outside region fitting center, the curve can be optimally evolved to the interior and branches of the region of interest (ROI). Third, a new edge stopping function is adopted to robustly capture the edge of ROI and speed up the multi-object image segmentation. Experiments show that the proposed method can achieve better accuracy for images with noise, inhomogeneous intensity, blur edge and complex branches, in the meanwhile, it also controls the time-consuming effectively and is insensitive to the initial contour position.},
keywords={Image segmentation;active contour;double-weighted factor;SPF function;Legendre polynomial},
doi={10.1109/ICASSP40776.2020.9054627},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054584,
author={S. {Harrigan} and S. {Coleman} and D. {Kerr} and P. {Yogarajah} and Z. {Fang} and C. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Neural Coding Strategies for Event-Based Vision Data},
year={2020},
volume={},
number={},
pages={2468-2472},
abstract={Neural coding schemes are powerful tools used within neuroscience. This paper introduces three different neural coding scheme formations for event-based vision data which are designed to emulate the neural behaviour exhibited by neurons under stimuli. Presented are phase-of-firing and two sparse neural coding schemes. It is determined that machine learning approaches, i.e. Convolutional Neural Network combined with a Stacked Autoencoder network, produce powerful descriptors of the patterns within events. These coding schemes are deployed in an existing action recognition template and evaluated using two popular event-based data sets.},
keywords={event-based vision;convolutional neural network;encoding scheme;feature extraction;object recognition},
doi={10.1109/ICASSP40776.2020.9054584},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054183,
author={Q. {An} and Y. {Shen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Camera Configuration Design in Cooperative Active Visual 3d Reconstruction: A Statistical Approach},
year={2020},
volume={},
number={},
pages={2473-2477},
abstract={Visual 3D reconstruction is an essential technique in computer vision which restores the 3D model of the scene from multi-view images. In this paper, we propose a statistical framework for the active visual 3D reconstruction. We first derive a closed-form expression to characterize the dependence of the reconstruction performance on 3D point’s position and camera setting parameters under the binocular camera setting. Then we derive several closed-form solutions and propose an efficient optimization algorithm to find the optimal camera configurations with the best reconstruction quality under various settings. Numerical results validate the performance of our methods in terms of both reconstruction accuracy and computational efficiency.},
keywords={Optimal camera configuration;Active visual 3D reconstruction;Fisher information},
doi={10.1109/ICASSP40776.2020.9054183},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053321,
author={Z. {Zhao} and T. {Wang} and S. {Xia} and Y. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Hand-3d-Studio: A New Multi-View System for 3d Hand Reconstruction},
year={2020},
volume={},
number={},
pages={2478-2482},
abstract={This paper proposes a new system named as Hand-3D-Studio to capture the 3D hand pose and shape information. Our system includes 15 synchronized DSLR cameras, which can acquire high quality multi-view 4K resolution color images in a circular manner. We then introduce a 2D hand keypoints guided iterative pixel growth matching strategy for 3D reconstruction, where the 2D keypoints are obtained via convolution neural network. We find that the pre-detected 2D hand keypoints can greatly remove the matching noise, and thus improve the performance of reconstruction. After that, a non-rigid iterative closest points algorithm is performed to drive a template hand to fit the point clouds and register all the hand meshes. As a consequence, we captured more than 20K high quality hand color images, annotated 2D hand key-points, 3D point cloud as well as the registered hand meshes (>200). All the data are public on the website http://www.yangangwang.com for future research.},
keywords={Multi-view;3D Reconstruction;3D Hand Pose Estimation;Dataset},
doi={10.1109/ICASSP40776.2020.9053321},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053787,
author={L. {Drumetz} and M. D. {Mura} and G. {Tochon} and R. {Fablet}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Endmember Dynamics in Multitemporal Hyperspectral Data Using A State-Space Model Formulation},
year={2020},
volume={},
number={},
pages={2483-2487},
abstract={Hyperspectral image unmixing is an inverse problem aiming at recovering the spectral signatures of pure materials of interest (called endmembers) and estimating their proportions (called abundances) in every pixel of the image. However, in spite of a tremendous applicative potential and the avent of new satellite sensors with high temporal resolution, multitemporal hyperspectral unmixing is still a relatively underexplored research avenue in the community, compared to standard image unmixing. In this paper, we propose a new framework for multitemporal unmixing and endmember extraction based on a state-space model, and present a proof of concept on simulated data to show how this representation can be used to inform multitemporal unmixing with external prior knowledge, or on the contrary to learn the dynamics of the quantities involved from data using neural network architectures adapted to the identification of dynamical systems.},
keywords={Hyperspectral imaging;time series;spectral un-mixing;data assimilation;recurrent neural networks},
doi={10.1109/ICASSP40776.2020.9053787},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054402,
author={S. K. {Yarlagadda} and S. {Baireddy} and D. {Güera} and C. J. {Boushey} and D. A. {Kerr} and F. {Zhu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Eating Environments Through Scene Clustering},
year={2020},
volume={},
number={},
pages={2488-2492},
abstract={It is well known that dietary habits have a significant influence on health. While many studies have been conducted to understand this relationship, little is known about the relationship between eating environments and health. Yet researchers and health agencies around the world have recognized the eating environment as a promising context for improving diet and health. In this paper, we propose an image clustering method to automatically extract the eating environments from eating occasion images captured during a community dwelling dietary study. Specifically, we are interested in learning how many different environments an individual consumes food in. Our method clusters images by extracting features at both global and local scales using a deep neural network. The variation in the number of clusters and images captured by different individual makes this a very challenging problem. Experimental results show that our method performs significantly better compared to several existing clustering approaches.},
keywords={Image Clustering;Eating Environment;Deep Learning},
doi={10.1109/ICASSP40776.2020.9054402},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053968,
author={Z. {Zha} and X. {Yuan} and J. {Zhou} and C. {Zhu} and B. {Wen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Hybrid Structural Sparse Error Model for Image Deblocking},
year={2020},
volume={},
number={},
pages={2493-2497},
abstract={Inspired by the image nonlocal self-similarity (NSS) prior, structural sparse representation (SSR) models exploit each group as the basic unit for sparse representation, which have achieved promising results in various image restoration applications. However, conventional SSR models only exploited the group within the input degraded (internal) image for image restoration, which can be limited by over-fitting to data corruption. In this paper, we propose a novel hybrid structural sparse error (HSSE) model for image deblocking. The proposed HSSE model exploits image NSS prior over both the internal image and external image corpus, which can be complementary in both feature space and image plane. Moreover, we develop an alternating minimization with an adaptive parameter setting strategy to solve the proposed HSSE model. Experimental results demonstrate that the proposed HSSE-based image deblocking algorithm outperforms many state-of-the-art image deblocking methods in terms of objective and visual perception.},
keywords={Hybrid structural sparse error;nonlocal self-similarity;structural sparse representation;image deblocking},
doi={10.1109/ICASSP40776.2020.9053968},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054004,
author={X. {Wu} and T. {Kawanishi} and K. {Kashino}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Reflectance-Guided, Contrast-Accumulated Histogram Equalization},
year={2020},
volume={},
number={},
pages={2498-2502},
abstract={Existing image enhancement methods fall short of expectations because with them it is difficult to improve global and local image contrast simultaneously. To address this problem, we propose a histogram equalization-based method that adapts to the data-dependent requirements of brightness enhancement and improves the visibility of details without losing the global contrast. This method incorporates the spatial information provided by image context in density estimation for discriminative histogram equalization. To minimize the adverse effect of non-uniform illumination, we propose defining spatial information on the basis of image reflectance estimated with edge preserving smoothing. Our method works particularly well for determining how the background brightness should be adaptively adjusted and for revealing useful image details hidden in the dark.},
keywords={Histogram equalization;image decomposition;image enhancement;image sharpening;reflectance},
doi={10.1109/ICASSP40776.2020.9054004},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053081,
author={W. {Shang} and P. {Zhu} and D. {Ren} and H. {Shi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Bilateral Recurrent Network for Single Image Deraining},
year={2020},
volume={},
number={},
pages={2503-2507},
abstract={Single image deraining has been widely studied in recent years. Motivated by residual learning, most deep learning based deraining approaches devote research attention to extracting rain streaks, usually yielding visual artifacts in final deraining images. To address this issue, we in this paper propose bilateral recurrent network (BRN) to simultaneously exploit rain streak layer and background image layer. Generally, we employ dual residual networks (ResNet) that are recursively unfolded to sequentially extract rain streaks and predict clean background image. Furthermore, we propose bilateral LSTMs into dual ResNets, which not only can respectively propagate deep features across multiple stages, but also bring the interplay between rain streak layer and background image layer. The experimental results demonstrate that our BRN notably outperforms state-of-the-art deep deraining networks on both synthetic datasets and real rainy images. All the source code and pre-trained models are available at https://github.com/shangwei5/BRN.},
keywords={Image deraining;CNN;LSTM},
doi={10.1109/ICASSP40776.2020.9053081},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054533,
author={J. {Choi} and J. {Kim} and J. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Srzoo: An Integrated Repository For Super-Resolution Using Deep Learning},
year={2020},
volume={},
number={},
pages={2508-2512},
abstract={Deep learning-based image processing algorithms, including image super-resolution methods, have been proposed with significant improvement in performance in recent years. However, their implementations and evaluations are dispersed in terms of various deep learning frameworks and various evaluation criteria. In this paper, we propose an integrated repository for the super-resolution tasks, named SRZoo, to provide state-of-the-art super-resolution models in a single place. Our repository offers not only converted versions of existing pre-trained models, but also documentation and toolkits for converting other models. In addition, SRZoo provides platform-agnostic image reconstruction tools to obtain super-resolved images and evaluate the performance in place. It also brings the opportunity of extension to advanced image-based researches and other image processing models. The software, documentation, and pre-trained models are publicly available on GitHub 1.},
keywords={Super-resolution;deep learning;image enhancement},
doi={10.1109/ICASSP40776.2020.9054533},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054270,
author={A. {Sagel} and A. {Roumy} and C. {Guillemot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sub-Dip: Optimization On A Subspace With Deep Image Prior Regularization And Application To Superresolution},
year={2020},
volume={},
number={},
pages={2513-2517},
abstract={The Deep Image Prior has been recently introduced to solve inverse problems in image processing with no need for training data other than the image itself. However, the original training algorithm of the Deep Image Prior constrains the reconstructed image to be on a manifold described by a convolutional neural network. For some problems, this neglects prior knowledge and can render certain regularizers ineffective. This work proposes an alternative approach that relaxes this constraint and fully exploits all prior knowledge. We evaluate our algorithm on the problem of reconstructing a high-resolution image from a downsampled version and observe a significant improvement over the original Deep Image Prior algorithm.},
keywords={Image reconstruction;Image restoration;Inverse problems;Neural networks},
doi={10.1109/ICASSP40776.2020.9054270},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053398,
author={C. {Wang} and Z. {Zhong} and J. {Jiang} and D. {Zhai} and X. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Parsing Map Guided Multi-Scale Attention Network For Face Hallucination},
year={2020},
volume={},
number={},
pages={2518-2522},
abstract={Face hallucination that aims to transform a low-resolution (LR) face image to a high-resolution (HR) one is an active domain-specific image super-resolution problem. The performance of existing methods is usually not satisfactory, especially when the upscaling factor is large, such as 8×. In this paper, we propose an effective two- step face hallucination method based on a deep neural network with multi-scale channel and spatial attention mechanism. Specifically, we develop a ParsingNet to extract the prior knowledge of an input LR face, which is then fed into a carefully designed FishSRNet to recover the target HR face. Experimental results demonstrate that our method outperforms the state-of-the-arts in terms of quantitative metrics and visual quality.},
keywords={Super-resolution;face hallucination;multi-scale;attention mechanism;face parsing map},
doi={10.1109/ICASSP40776.2020.9053398},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054515,
author={V. H. {Tang} and A. {Bouzerdoum} and S. L. {Phung}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Variational Bayesian Approach for Multichannel Through-Wall Radar Imaging with Low-Rank and Sparse Priors},
year={2020},
volume={},
number={},
pages={2523-2527},
abstract={This paper considers the problem of multichannel through-wall radar (TWR) imaging from a probabilistic Bayesian perspective. Given the observed radar signals, a joint distribution of the observed data and latent variables is formulated by incorporating two important beliefs: low-dimensional structure of wall reflections and joint sparsity among channel images. These priors are modeled through probabilistic distributions whose hyperparameters are treated with a full Bayesian formulation. Furthermore, the paper presents a variational Bayesian inference algorithm that captures wall clutter and provides channel images as full posterior distributions. Experimental results on real data show that the proposed model is very effective at removing wall clutter and enhancing target localization.},
keywords={Through-the-wall radar imaging;wall clutter mitigation;sparse Bayesian learning;variational inference},
doi={10.1109/ICASSP40776.2020.9054515},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053087,
author={J. {Liu} and Y. {Zou} and D. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Semanticgan: Generative Adversarial Networks For Semantic Image To Photo-Realistic Image Translation},
year={2020},
volume={},
number={},
pages={2528-2532},
abstract={Generative Adversarial Networks (GANs) have shown remarkable success in Semantic label map to Photo-realistic image Translation (S2PT) task. However, the results of the state-of-the-art approaches are often limited to blurriness and artifacts, and still far from realistic, since these methods lack effective semantic constrains to preserve the semantic information and ignore the structural correlations between the textures. To address those problems, we propose a SemanticGAN to synthesize high resolution image with fine details and realistic textures from the semantic label map. Specifically, we propose a Semantic Information Preserved Loss (SIPL) to maintain semantic information in the process of the generation via a segmentation model. Furthermore, we develop a novel generator to obtain the correlations between the image textures using newly-designed Correlated Residual Block (CRB). Experiments evaluated on Cityscapes dataset show that SemanticGAN outperforms many recent state-of-the-art methods in terms of qualitative and quantitative performance.},
keywords={Generative adversarial networks;S2PT;Semantic information preserved loss;Structural correlations between images textures;Correlated residual blocks},
doi={10.1109/ICASSP40776.2020.9053087},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053539,
author={M. {Chen} and Y. {Chang} and S. {Cao} and L. {Yan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Blind Denoising Network for Noisy Image Deblurring},
year={2020},
volume={},
number={},
pages={2533-2537},
abstract={Noisy image deblurring is to recover the blurry image in the presence of the random noise. One key to this problem is to know the noise level in each iteration. The existing methods manually adjust the regularization parameter for varying noise levels, which is quite inaccuracy and tedious for practical application. In this work, we discover that the noise level and the denoiser is tightly coupled. Consequently, we propose efficient blind denoising convolutional neural network (BDCNN) consisting of two stages: a down-sampling regression network for estimating noise level and a fully convolutional network for denoising, such that our model could adaptively handle the unknown noise level during iteration. Further, the BDCNN functions as a discriminative prior and is plugged into the iterative deblurring framework for noisy image deblurring. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods in terms of practicability and performance.},
keywords={Noisy image deblurring;blind denoising network;plug-and-play;iterative deblurring framework},
doi={10.1109/ICASSP40776.2020.9053539},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054293,
author={W. {Lin} and J. {Gao} and Q. {Wang} and X. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Pixel-Level Self-Paced Learning For Super-Resolution},
year={2020},
volume={},
number={},
pages={2538-2542},
abstract={Recently, lots of deep networks are proposed to improve the quality of predicted super-resolution (SR) images, due to its widespread use in several image-based fields. However, with these networks being constructed deeper and deeper, they also cost much longer time for training, which may guide the learners to local optimization. To tackle this problem, this paper designs a training strategy named Pixel-level Self-Paced Learning (PSPL) to accelerate the convergence velocity of SISR models. PSPL imitating self-paced learning gives each pixel in the predicted SR image and its corresponding pixel in ground truth an attention weight, to guide the model to a better region in parameter space. Extensive experiments proved that PSPL could speed up the training of SISR models, and prompt several existing models to obtain new better results. Furthermore, the source code is available at https://github.com/Elin24/PSPL},
keywords={super-resolution;training strategy;self-paced learning},
doi={10.1109/ICASSP40776.2020.9054293},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053743,
author={B. {Magnier} and A. {Aberkane} and N. {Gorrity}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Recursive Edge Detector For Color Filter Array Image},
year={2020},
volume={},
number={},
pages={2543-2547},
abstract={Most of embedded cameras use a single sensor to capture images through a color filter. They produce special images with only one color component per pixel. Missing data are usually estimated through a demosaicking process, but this takes undesirable computation time and may generate undesirable color artifacts. Many embedded systems under real-time constraints could use this type of camera for purposes like edge detection. In this paper, a new edge detection method is proposed for the computation of partial derivative images. This algorithm is tested on a large set of synthetic images where edge ground truth is unquestionable. The exploitation of the raw data of images allows not only to drastically reduce computational time, but also to get edge detection results even more precise than using certain demosaicking-based methods.},
keywords={Edge detection;Bayer CFA;rotated filter},
doi={10.1109/ICASSP40776.2020.9053743},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052926,
author={J. {He} and L. {Yu} and W. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Image De-Raining Via RDL: When Reweighted Convolutional Sparse Coding Meets Deep Learning},
year={2020},
volume={},
number={},
pages={2548-2552},
abstract={Over the past few decades, image de-raining has witnessed substantial progress due to the development of priors and deep learning based methods. However, few studies combine the merits of both. In this paper, we argue that domain expertise of conventional convolutional sparse coding (CSC) is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. Specifically, motivated by the success of reweighting algorithms, we propose solving the CSC model by learning weighted iterative soft thresholding algorithm (LwISTA) in a convolutional manner where the reweighted ℓ1-norm is introduced. Based on this, we present a novel framework for single image de-raining, in which the channel attention is employed to learn the weight. Extensive experiments demonstrate the superiority of our method over recent state-of-the-art image de-raining methods, in terms of both quantitative and qualitative results.},
keywords={Image de-raining;convolutional sparse coding;reweighted ℓ1-minimization;deep learning},
doi={10.1109/ICASSP40776.2020.9052926},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053689,
author={Y. {Guo} and Y. {Li} and S. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={CS-R-FCN: Cross-Supervised Learning for Large-Scale Object Detection},
year={2020},
volume={},
number={},
pages={2553-2557},
abstract={Generic object detection is one of the most fundamental problems in computer vision, yet it is difficult to provide all the bounding-box-level annotations aiming at large-scale object detection for thousands of categories. In this paper, we present a novel cross-supervised learning pipeline for large-scale object detection, denoted as CS-R-FCN. First, we propose to utilize the data flow of image-level annotated images in the fully-supervised two-stage object detection framework, leading to cross-supervised learning combining bounding-box-level annotated data and image-level annotated data. Second, we introduce a semantic aggregation strategy utilizing the relationships among the cross-supervised categories to reduce the unreasonable mutual inhibition effects during the feature learning. Experimental results show that the proposed CS-R-FCN improves the mAP by a large margin compared to previous related works.},
keywords={Object detection;cross-supervised learning;proposal generation;semantic aggregation},
doi={10.1109/ICASSP40776.2020.9053689},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053888,
author={F. {Dai} and Y. {Zhang} and Y. {Ma} and H. {Li} and Q. {Zhao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Dilated Convolutional Neural Networks for Panoramic Image Saliency Prediction},
year={2020},
volume={},
number={},
pages={2558-2562},
abstract={Saliency prediction is an important way to understand human’s behavior and has a wide range of applications. Although lots of algorithms have been designed to predict saliency for planar images, there are few works for 360° images. In this paper, we propose an encoder-decoder network for panoramic image saliency prediction. Dilated convolutional layers are deployed in the network, which can extract more representative features and improve the accuracy of saliency prediction. To deal with the image distortions in 360° images, our network takes cube map format as input and processes six faces of cube map simultaneously. Respecting the saliency distribution of ground truth, we also propose a new data augmentation method to train the network, which is validated to be helpful for performance improvement. Extensive experiments show that our method gives new state-of-the-art results on 360° image saliency prediction.},
keywords={Panoramic images;Saliency prediction;Cube map;Data augmentation;Hybrid dilated convolution},
doi={10.1109/ICASSP40776.2020.9053888},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053928,
author={X. {Gu} and X. {Xue} and F. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fine-Grained Action Recognition on a Novel Basketball Dataset},
year={2020},
volume={},
number={},
pages={2563-2567},
abstract={Currently most works on action recognition focus on the coarsely-grained actions, while the fine-grained action recognition is seldom addressed which is of vital importance in many applications such as video retrieval. To tackle this issue, in this paper, we release a challenging dataset by annotating the fine-grained actions in basketball game videos. A benchmark evaluation of the state-of-the-art approaches for action recognition is also provided on our dataset. Furthermore, we propose an approach by integrating the NTS-Net into two-stream network so as to locate the most informative regions and extract more discriminative features for fine-grained action recognition. Our experiments show that the proposed approach significantly outperforms the existing approaches.},
keywords={Fine-grained action recognition;Convolutional neural networks;Basketball},
doi={10.1109/ICASSP40776.2020.9053928},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053761,
author={X. {Pan} and H. {Mo} and Z. {Zhou} and W. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Attention Guided Region Division for Crowd Counting},
year={2020},
volume={},
number={},
pages={2568-2572},
abstract={Crowd counting has drawn more and more attention in computer vision. There are two mainstream approaches to deal with crowd counting tasks, regression and detection. Regression-based methods usually overestimate the count in sparse areas, while detection-based methods tend to underestimation in dense areas. In this paper, we propose a two-branch network combining regression and detection. We introduce the attention mechanism to make the network adaptively divide dense and sparse areas and employ appropriate methods on them respectively. The regression branch predicts density map in extremely dense areas. An improved detection network is applied to detect multi-scale heads in relatively sparse areas. Our method is able to obtain precise head bounding boxes in sparse areas with ensuring counting accuracy in dense areas. Experimental results show that our method achieves state-of-the-art on challenging public crowd counting datasets.},
keywords={Crowd counting;Head detection;Density regression},
doi={10.1109/ICASSP40776.2020.9053761},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054140,
author={T. {Suzuki}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Superpixel Segmentation Via Convolutional Neural Networks with Regularized Information Maximization},
year={2020},
volume={},
number={},
pages={2573-2577},
abstract={We propose an unsupervised superpixel segmentation method by optimizing a randomly-initialized convolutional neural network (CNN) in inference time. Our method generates superpixels via CNN from a single image without any labels by minimizing a proposed objective function for superpixel segmentation in inference time. There are three advantages to our method compared with many of existing methods: (i) leverages an image prior of CNN for superpixel segmentation, (ii) adaptively changes the number of superpixels according to the given images, and (iii) controls the property of superpixels by adding an auxiliary cost to the objective function. We verify the advantages of our method quantitatively and qualitatively on BSDS500 dataset.},
keywords={unsupervised segmentation;superpixels;convolutional neural networks},
doi={10.1109/ICASSP40776.2020.9054140},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053070,
author={S. {Huang} and X. {Li} and Z. {Cheng} and Z. {Zhang} and A. {Hauptmann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Stacked Pooling for Boosting Scale Invariance of Crowd Counting},
year={2020},
volume={},
number={},
pages={2578-2582},
abstract={In this work, we take insight into the dense crowd counting problem by exploring the phenomenon of cross-scale visual similarity caused by perspective distortions. It is a quite common phenomenon in crowd scenarios, suggesting the crowd counting model to enable a good performance of scale invariance. Existing deep crowd counting approaches mainly focus on the multi-scale techniques over convolutional layers to capture scale-adaptive features, resulting in high computing costs. In this paper, we propose simple but effective pooling variants, i.e., multi-kernel pooling and stacked pooling, to take place of the vanilla pooling layers in convolutional neural networks (CNNs) for boosting the scale invariance. Our proposed pooling modules do not introduce extra parameters and can be easily implemented in practice. Empirical studies on two benchmark crowd counting datasets show that the proposed pooling modules beat the vanilla pooling layer in most experimental cases.},
keywords={Crowd counting;scale invariance;pooling},
doi={10.1109/ICASSP40776.2020.9053070},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053939,
author={H. {Liu} and L. {Zhang} and L. {Guan} and M. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={GFNet: A Lightweight Group Frame Network for Efficient Human Action Recognition},
year={2020},
volume={},
number={},
pages={2583-2587},
abstract={Human action recognition aims at assigning an action label to a well-segmented video. Recent work using two-stream or 3D convolutional neural networks achieves high recognition rates at the cost of huge computation complexity, memory footprint, and parameters. In this paper, we propose a lightweight neural network called Group Frame Network (GFNet) for human action recognition, which imposes intra-frame spatial information sparsity on spatial dimension in a simple yet effective way. Benefit from two core components, namely Group Temporal Module (GTM) and Group Spatial Module (GSM), GFNet decreases irrelevant motion inside frames and duplicate texture features among frames, which can extract the spatial-temporal information of frames at a minuscule cost. Experimental results on NTU RGB+D dataset and Varying-view RGB-D Action dataset show that our method without any pre-training strategy reaches a reasonable trade-off among computation complexity, parameters and performance, which is more cost-efficient than state-of-the-art methods.},
keywords={Human Action Recognition;Lightweight Network;Convolutional Neural Network},
doi={10.1109/ICASSP40776.2020.9053939},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053829,
author={W. {Lin} and J. {Zhong} and S. {Liu} and T. {Li} and G. {Li}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={ROIMIX: Proposal-Fusion Among Multiple Images for Underwater Object Detection},
year={2020},
volume={},
number={},
pages={2588-2592},
abstract={Generic object detection algorithms have proven their excellent performance in recent years. However, object detection on underwater datasets is still less explored. In contrast to generic datasets, underwater images usually have color shift and low contrast; sediment would cause blurring in underwater images. In addition, underwater creatures often appear closely to each other on images due to their living habits. To address these issues, our work investigates augmentation policies to simulate overlapping, occluded and blurred objects, and we construct a model capable of achieving better generalization. We propose an augmentation method called RoIMix, which characterizes interactions among images. Proposals extracted from different images are mixed together. Previous data augmentation methods operate on a single image while we apply RoIMix to multiple images to create enhanced samples as training data. Experiments show that our proposed method improves the performance of region-based object detectors on both Pascal VOC and URPC datasets.},
keywords={Object Detection;Data Augmentation;Under-water Image Analysis},
doi={10.1109/ICASSP40776.2020.9053829},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054378,
author={J. {Baderot} and M. {Desvignes} and L. {Condat} and M. D. {Mura}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Tree of Shapes Cut for Material Segmentation Guided by a Design},
year={2020},
volume={},
number={},
pages={2593-2597},
abstract={In manufacturing, the monitoring of the fabrication process is crucial in order to be sure that objects are compliant. For nano-objects, most of this monitoring is done manually. In this paper, we propose a method to segment different materials in a manufactured object. The method uses design information which represent the ideal object to manufacture. This representation visually gathers information about materials, shapes and relationships between these shapes. In our segmentation method we choose to encode this information in the tree of shapes to enforce the design characteristics into a real image of the object. To achieve such segmentation, we perform graph cuts on this particular tree structure using additional information such as the position in the design or the order of inclusion of the shapes.},
keywords={material segmentation;tree of shapes;manufacturing},
doi={10.1109/ICASSP40776.2020.9054378},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054590,
author={P. {Liu} and X. {Yan} and Y. {Jiang} and S. {Xia}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Flow Collaborative Network for Online Visual Tracking},
year={2020},
volume={},
number={},
pages={2598-2602},
abstract={The deep learning-based visual tracking algorithms such as MDNet achieve high performance leveraging to the feature extraction ability of a deep neural network. However, the tracking efficiency of these trackers is not very high due to the slow feature extraction for each frame in a video. In this paper, we propose an effective tracking algorithm to alleviate the time-consuming problem. Specifically, we design a deep flow collaborative network, which executes the expensive feature network only on sparse keyframes and transfers the feature maps to other frames via optical flow. Moreover, we raise an effective adaptive keyframe scheduling mechanism to select the most appropriate keyframe. We evaluate the proposed approach on large-scale datasets: OTB2013 and OTB2015. The experiment results show that our algorithm achieves considerable speedup and high precision as well.},
keywords={Feature extraction;Target tracking;Adaptation models;Robustness;Computational modeling;Visualization;Deep learning;Visual tracking;Deep flow collaboration;Online learning;Keyframe scheduling},
doi={10.1109/ICASSP40776.2020.9054590},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053118,
author={B. {Cao} and X. {Meng} and S. {Zhu} and B. {Zengv}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Salient Object Detection Based On Image Bit-Map},
year={2020},
volume={},
number={},
pages={2603-2607},
abstract={In this paper, we propose a novel salient object detection framework, which makes full use of the essential image compression. More specifically, we first compose an intuitive measure of compressibility from JPEG compression, namely bit-map. Then, depending on the relationship between bitmap and salient object, we generate the salient object window directly from bit-map without utilizing any features from the compressed image. Finally, the saliency map is calculated according to the salient object window and with a ranking algorithm. The proposed method achieves good performance as well as low complexity. The experimental results demonstrate the effectiveness of our proposed method compared with other existing approaches.},
keywords={visual attention;JPEG;bit-map;salient object window;saliency map},
doi={10.1109/ICASSP40776.2020.9053118},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054489,
author={L. {Zhang} and C. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Novel Saliency-Driven Oil Tank Detection Method for Synthetic Aperture Radar Images},
year={2020},
volume={},
number={},
pages={2608-2612},
abstract={Synthetic Aperture Radar (SAR) imaging system plays an important role in earth observation research. This leads to the significance of target detection in SAR image. In this paper, we propose a novel saliency-driven oil tank detection method (SDD) for SAR images. First, we use the enhanced directional smoothing (EDS) to remove speckle noise from SAR images; in the step of saliency analysis, the integer wavelet transforms (IWT) and the DoG filter are used to obtain orientation and intensity features, respectively. Then, the orientation feature map and the intensity feature map resulting from these two kinds of features are utilized to compute the final saliency map; after segmenting the saliency map, the obtained connected domain guide the Active Contour Model (ACM) to acquire accurate contours of tops of oil tanks, and the bottoms of the oil tanks can be detected by the strong scattering points around the tops. Experimental results show that the proposed model outperforms the classical/state-of-the-art models in maintaining complete targets and accurate boundaries.},
keywords={Object detection;saliency analysis;Synthetic Aperture Radar (SAR);active contour model},
doi={10.1109/ICASSP40776.2020.9054489},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053987,
author={H. {Li} and Y. {Yuan} and Q. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Video Frame Interpolation Via Residue Refinement},
year={2020},
volume={},
number={},
pages={2613-2617},
abstract={Video frame interpolation achieves temporal super-resolution by generating smooth transitions between frames. Although great success has been achieved by deep neural networks, the synthesized images stills suffer from poor visual appearance and unsatisfactory artifacts. In this paper, we propose a novel network structure that leverages residue refinement and adaptive weight to synthesize in-between frames. The residue refinement technique is used for optical flow and image generation for higher accuracy and better visual appearance, while the adaptive weight map combines the forward and backward warped frames to reduce the artifacts. Moreover, all submodules in our method are implemented by U-Net with less depths, so the efficiency is guaranteed. Experiments on public datasets demonstrate the effectiveness and superiority of our method over the state-of-the-art approaches.},
keywords={Video frame interpolation;residue refinement;adaptive weight map;U-Net},
doi={10.1109/ICASSP40776.2020.9053987},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053754,
author={K. {Jiang} and Z. {Wang} and P. {Yi} and C. {Chen} and Y. {Yang} and X. {Tian} and J. {Jiang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Attention-Guided Deraining Network Via Stage-Wise Learning},
year={2020},
volume={},
number={},
pages={2618-2622},
abstract={Due to diverse rain shapes, directions, densities as well as different distances to cameras, rain streaks in the air are interweaved and overlapped. However, most existing deraining methods are inherently oblivious this phenomenon and tend to learn a single rain streak layer to simulate this complex distribution, consequently failing to restore high-quality rain-free images. To solve this problem, along with the stage-wise learning, we propose a novel attention-guided deraining network (ADN) for rain streak removal. Specially, we decompose the rain streaks into multiple rain streak layers, and individually model them along the stages of the network to match the increasing abstracts. Moreover, the attention mechanism is utilized to guide the fusion of these rain streak layers by handling the overlaps between them. Extensive experiments on several benchmark datasets and real-world scenarios show substantial improvements both on quantitative indicators and visual effects over the current top-performing methods.},
keywords={Rain streak removal;attention mechanism;stage-wise learning;recurrent learning;residual learning},
doi={10.1109/ICASSP40776.2020.9053754},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053180,
author={K. {Metwaly} and V. {Monga}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Attention-Mask Dense Merger (Attendense) Deep HDR for Ghost Removal},
year={2020},
volume={},
number={},
pages={2623-2627},
abstract={High Dynamic Range (HDR) reconstruction is the process of producing an HDR image from a set of Standard Dynamic Range (SDR) images with different exposure times. This is a particularly challenging problem when relative camera or object motion exists between the available SDR images. Recently, deep learning methods, specifically those based on convolutional neural networks (CNNs) have been developed for HDR and shown to achieve unprecedented quality gains. Invariably an image alignment phase precedes the CNN mapping and merging. In practice, this alignment step greatly increases the computational burden of deep HDR methods often rendering them unsuitable for real-time composition. We propose a new deep HDR technique that does not need any explicit alignment of SDR images. Instead, a novel attention mask is developed that enables the network to focus on parts of the scene with considerable motion. Further, a dense merger is proposed that leads to an economical network. Evaluation over benchmark databases reveals that the proposed AttenDense network achieves high quality HDR results with significantly reduced computation time than state of the art. Further, the incorporation of domain knowledge (development of a custom attention mask) allows a more graceful decay in performance in the face of limited training.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053180},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053920,
author={H. {Yang} and C. H. {Yang} and Y. {James Tsai}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Y-Net: Multi-Scale Feature Aggregation Network With Wavelet Structure Similarity Loss Function For Single Image Dehazing},
year={2020},
volume={},
number={},
pages={2628-2632},
abstract={Single image dehazing is the ill-posed two-dimensional signal reconstruction problem. Recently, deep convolutional neural networks (CNN) have been successfully used in many computer vision problems. In this paper, we propose a Y-net that is named for its structure. This network reconstructs clear images by aggregating multi-scale features maps. Additionally, we propose a Wavelet Structure SIMilarity (W-SSIM) loss function in the training step. In the proposed loss function, discrete wavelet transforms are applied repeatedly to divide the image into differently sized patches with different frequencies and scales. The proposed loss function is the accumulation of SSIM loss of various patches with respective ratios. Extensive experimental results demonstrate that the proposed Y-net with the W-SSIM loss function restores high-quality clear images and outperforms state-of-the-art algorithms. Code and models are available at https://github.com/dectrfov/Y-net},
keywords={Single image dehazing;Y-net;discrete wavelet transform;structure similarity;multi-scale feature aggregation},
doi={10.1109/ICASSP40776.2020.9053920},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054003,
author={K. {Liu} and Z. {Han} and J. {Chen} and C. {Liu} and J. {Chen} and Z. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Image Super-Resolution Using Residual Global Context Network},
year={2020},
volume={},
number={},
pages={2633-2637},
abstract={Recent studies have showed that convolutional neural networks (CNN) can effectively improve the performance of single image super-resolution (SR). However, previous methods rarely considered long-range dependencies between pixels and channel-wise interdependencies at the same time. They ignores the fact that natural images have strong internal data repetition which requires the network to capture long-range dependencies between pixels and considering the interdepen-dencies between channels can better exploit the input information of the network. In addition, although past studies have proved that deep convolutional neural network benefit the performance of image super-resolution, it also means that the network needs more memory consumption and higher computational complexity. To solve these problem,we introduce Global Context block (GCB) and design a comparative shallow network called Residual Global Context Networks (RGC-N). It achieves a better trade-off between the amount of parameter and the quality of image reconstruction. Extensive experiments demonstrate that the proposed method is superior to the state-of-the-art methods.},
keywords={Super-resolution;long-range dependencies;channel-wise interdependencies},
doi={10.1109/ICASSP40776.2020.9054003},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053261,
author={J. {Zhang} and R. {Liu} and L. {Ma} and W. {Zhong} and X. {Fan} and Z. {Luo}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Principle-Inspired Multi-Scale Aggregation Network for Extremely Low-Light Image Enhancement},
year={2020},
volume={},
number={},
pages={2638-2642},
abstract={The under-exposure and low-light environments are common to degrade the image-quality with invisible information. To ameliorate this case, a copious of low-light image enhancement methods are developed. However, these existing works are hard to handle extremely low-light conditions with noises, even well-known network-based methods. To address this issue, we develop a Principle-inspired Multi-scale Aggregation Network (PMA-Net) to simultaneously achieve the exposure enhancement and noises removal. Specifically, we establish a pioneering principle-inspired connection to present the physical principle in the inside of the network, to strengthen the structural depict. Subsequently, we propose a multi-scale aggregation strategy to eliminate the noises in the enhanced results. Sufficient ablation studies manifest the effectiveness of our PMA-Net. Extensive qualitative and quantitative comparisons with other state-of-the-art methods are conducted to fully indicates our outstanding performance.},
keywords={Low-light Image Enhancement;Deep Learning;Physical Principle;Multi-Scale Aggregation},
doi={10.1109/ICASSP40776.2020.9053261},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054687,
author={W. {Xie} and J. {Zhang} and Z. {Lu} and M. {Cao} and Y. {Zhao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Non-Local Nested Residual Attention Network for Stereo Image Super-Resolution},
year={2020},
volume={},
number={},
pages={2643-2647},
abstract={Nowadays CNN-based stereo image super-resolution(SR) methods have obtained remarkable performance. However, most of existing methods only superficially portrayed the low layer features without considering the uneven distribution of information, which is insufficient because stereo image warping and sub-pixel upsampling require discriminative features to identify corresponding pixels. To address this problem, in this paper, we propose a novel network named Non-local Nested Residual Attention Network (NNRANet). Specifically, a non-local dilated attention module (NDAM) is developed to exploit the rich hierarchical feature and capture the long-range dependencies between pixels. Moreover, we present a nested residual group (NRG) with dense connections and multiple nested residual sub-network, which not only continuously remembers and extracts the stereo fusion feature, but also enables training a deeper and more stable network. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on the Middlebury, KITTI 2012 and KITTI 2015 datasets.},
keywords={stereo image super-resolution;non-local;residual;attention mechanism},
doi={10.1109/ICASSP40776.2020.9054687},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053937,
author={F. {Lemarchand} and E. F. {Montesuma} and M. {Pelcat} and E. {Nogues}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Opendenoising: An Extensible Benchmark for Building Comparative Studies of Image Denoisers},
year={2020},
volume={},
number={},
pages={2648-2652},
abstract={Image denoising has recently taken a leap forward due to machine learning. However, image denoisers, both expert-based and learning-based, are mostly tested on well-behaved generated noises (usually Gaussian) rather than on real-life noises, making performance comparisons difficult in real-world conditions. This is especially true for learning-based denoisers which performance depends on training data. Hence, choosing which method to use for a specific denoising problem is difficult.This paper proposes a comparative study of existing denoisers, as well as an extensible open tool that makes it possible to reproduce and extend the study. MWCNN is shown to outperform other methods when trained for a real-world image interception noise, and additionally is the second least compute hungry of the tested methods. To evaluate the robustness of conclusions, three test sets are compared. A Kendall’s Tau correlation of only 60% is obtained on methods ranking between noise types, demonstrating the need for a benchmarking tool.},
keywords={Image Denoiser Benchmarking;Complex Image Noises},
doi={10.1109/ICASSP40776.2020.9053937},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053119,
author={L. {Zhang} and S. {Wang} and X. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={SDTCN: Similarity Driven Transmission Computing Network for Image Dehazing},
year={2020},
volume={},
number={},
pages={2653-2657},
abstract={Transmission similarity is an important feature which can greatly increase the capability of convolutional neural network (CNN) to fit transmission map. However, it is not sufficiently utilized in existing algorithms. In this paper, we propose a novel light-weight similarity driven transmission computing network called SDTCN that is guided by the attributes of transmission similarity. First, we adopt a non-data-driven image segmentation method to acquire the transmission similarity. Compared with CNN based segmentation approaches, our method can not only greatly save computing resources, but also separate the objects and background precisely. Second, a full convolutional network is introduced to reduce blocky effects in SDTCN. Finally, unlike previous "first airlight then transmission" mode, a dependable airlight estimation approach is designed drawing on the transmission map generated by SDTCN, which can improve the accuracy of airlight effectively. Extensive experiments demonstrate that the proposed algorithm outperforms the state-of-the-art methods on synthetic and real-world images.},
keywords={Image restoration;image dehazing;CNN;transmission similarity;airlight estimation},
doi={10.1109/ICASSP40776.2020.9053119},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053027,
author={L. {Yu} and H. {Su} and C. {Jung}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Joint Enhancement And Denoising of Low Light Images Via JND Transform},
year={2020},
volume={},
number={},
pages={2658-2662},
abstract={Low light images suffer from low dynamic range and severe noise due to low signal-to-noise ratio (SNR). In this paper, we propose joint enhancement and denoising of low light images via just-noticeable-difference (JND) transform. We achieve contrast enhancement and noise reduction simultaneously based on human visual perception. First, we perform contrast enhancement based on perceptual histogram to effectively allocate a dynamic range while preventing over-enhancement. Second, we generate JND map based on an HVS response model from foreground and background luminance, called JND transform. Then, we refine JND map using Weber’s law and visual masking. Weber’s law enhances the JND map based on the luminance variation after enhancement, while visual masking provides noise suppression for smooth regions and detail enhancement for texture regions. Finally, we conduct chroma denoising that transfers texture information of the denoised luma channel to the chroma channels by guided image filtering. Experimental results show that the proposed method achieves good performance in contrast enhancement and noise reduction while successfully preserving details.},
keywords={Contrast enhancement;noise reduction;JND transform;visual masking;HVS response model},
doi={10.1109/ICASSP40776.2020.9053027},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054360,
author={C. {Geng} and L. {Chen} and X. {Zhang} and Z. {Gao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adversarial Text Image Super-Resolution using Sinkhorn Distance},
year={2020},
volume={},
number={},
pages={2663-2667},
abstract={Convolutional neural network-based methods have demonstrated promising results for single image super-resolution. However, existing methods usually approach the problem on natural scenes rather than texts, whereas the latter can provide more informative messages to viewers. In this paper, instead of using the Lp-norm as the supervision metric, we propose a novel one for better preserving semantic information in text images. Our new metric combines optimal transport in a primal form with Sinkhorn distance defined in an adversarially learned feature space. Since the Sinkhorn distance measures the similarity between two features in terms of both feature components and spatial locations, our metric can maintain the spatial structure of texts during network optimization. Experimental results on text datasets show that our method performs favorably against state-of-the-art approaches in both quantitative and qualitative evaluations. We will publish the code, datasets, and models upon acceptance.},
keywords={Text images;Super-Resolution;Adversarial Learning;Sinkhorn Distance},
doi={10.1109/ICASSP40776.2020.9054360},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054658,
author={Y. {Zhao} and D. {Zhai} and J. {Jiang} and X. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={ADRN: Attention-Based Deep Residual Network for Hyperspectral Image Denoising},
year={2020},
volume={},
number={},
pages={2668-2672},
abstract={Hyperspectral image (HSI) denoising is of crucial importance for many subsequent applications, such as HSI classification and interpretation. In this paper, we propose an attention-based deep residual network to directly learn a mapping from noisy HSI to the clean one. To jointly utilize the spatial-spectral information, the current band and its K adjacent bands are simultaneously exploited as the input. Then, we adopt convolution layer with different filter sizes to fuse the multi-scale feature, and use shortcut connection to incorporate the multi-level information for better noise removal. In addition, the channel attention mechanism is employed to make the network concentrate on the most relevant auxiliary information and features that are beneficial to the de-noising process best. To ease the training procedure, we reconstruct the output through a residual mode rather than a straightforward prediction. Experimental results demonstrate that our proposed ADRN scheme outperforms the state-of-the-art methods both in quantitative and visual evaluations.},
keywords={HSI denoising;Spatial-spectral;Channel attention;Residual learning},
doi={10.1109/ICASSP40776.2020.9054658},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053082,
author={C. {Zhang} and G. {Wang} and X. {Chen} and P. {Xie} and T. {Yamasaki}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Weakly Supervised Segmentation Guided Hand Pose Estimation During Interaction with Unknown Objects},
year={2020},
volume={},
number={},
pages={2673-2677},
abstract={Hand pose estimation is important for human computer interaction, but the performance is not satisfying when the hand is interacting with objects. To alleviate the influence of unknown objects, we propose a novel weakly supervised segmentation guided scheme to estimate hand poses. Approximate hand masks generated from annotations of sparse hand joints are used to supervise the segmentation task. Better features can be extracted since they are shared between the two tasks of hand segmentation and hand pose estimation. With the guidance of weakly supervised segmentation, the network can learn intermediate features balanced between focusing on the foreground and preserving contextual information. Finally the xy and z coordinates are estimated in different branches but utilizing shared feature maps. Experimental results of three different tasks on the publicly available FHAD dataset demonstrate the effectiveness of the proposed architecture.},
keywords={Convolution Neural Network;Hand Pose Estimation;Hand Object Interaction;Human Computer Interaction;Weakly Supervision},
doi={10.1109/ICASSP40776.2020.9053082},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053598,
author={X. {Zhang} and G. {Cheung} and P. {Le Callet} and J. Z. G. {Tan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Sparse Directed Graph Learning for Head Movement Prediction in 360 Video Streaming},
year={2020},
volume={},
number={},
pages={2678-2682},
abstract={High-definition 360 videos encoded in fine quality are typically too large in size to stream in its entirety over bandwidth (BW)-constrained networks. One popular remedy is to interactively extract and send a spatial sub-region corresponding to a viewer’s current field-of-view (FoV) in a head-mounted display (HMD) for more BW-efficient streaming. Due to the non-negligible round-trip-time (RTT) delay between server and client, accurate head movement prediction that foretells a viewer’s future FoVs is essential. Existing approaches are either overly simplistic in modelling and predict poorly when RTT is large, or are over-reliant on data-driven learning, resulting in inflexible models that are not robust to RTT heterogeneity. In this paper, we cast the head movement prediction task as a sparse directed graph learning problem, where three sources of relevant information—a 360 image saliency map, collected viewers’ head movement traces, and a biological head rotation model—are aggregated into a unified Markov model. Specifically, we formulate a constrained optimization problem to minimize an l2-norm fidelity term and a sparsity term, corresponding to trace data / saliency consistency and a sparse graph model prior respectively. We solve the problem alternately using a hybrid iterative reweighted least square (IRLS) and Frank-Wolfe optimization strategy. Extensive experiments show that our head movement prediction scheme noticeably outperforms existing proposals across a wide range of RTTs.},
keywords={Virtual reality;interactive video streaming;graph learning;convex optimization},
doi={10.1109/ICASSP40776.2020.9053598},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053041,
author={J. {Tang} and A. {Yellepeddi} and S. {Demirtas} and C. {Barber}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Tracking to Improve Detection Quality in Lidar For Autonomous Driving},
year={2020},
volume={},
number={},
pages={2683-2687},
abstract={Enabling Lidar systems to detect objects at very long ranges has the potential to be extremely valuable for autonomous driving applications, but is challenging due to noise. In this work, we leverage information from multiple consecutive frames to improve the detection capabilities of Lidar systems. We develop a mathematical model whose solution gives a low memory and low computation algorithm that detects some fraction of the objects present while keeping the number of false positives small. Performance of the proposed method is characterized using simulations of a realistic Lidar chain.},
keywords={Lidar;autonomous vehicles;object detection;hypothesis testing},
doi={10.1109/ICASSP40776.2020.9053041},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053046,
author={Y. {Chen} and M. {Zhang} and S. {Li} and Z. {Wang} and X. {Tian}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cartoon-Texture Decomposition-Based Variational Pansharpening},
year={2020},
volume={},
number={},
pages={2688-2692},
abstract={Pansharpening is widely used to increase the spatial resolution of a multispectral (MS) image by fusing with a panchromatic (PAN) image that has high-spatial resolution and the same scene. In this paper, the similarities of MS and PAN images in cartoon-texture space are exploited. The cartoon and texture components of an image always contain the global structure information and the locally-patterned information, respectively. Therefore, the global and local spatial details (i.e., high-order information) could be preserved well in the fused high-spatial resolution MS image after leveraging the similarities of these images. Pansharpening is formulated as the optimization problem with respect to the cartoon-texture similarities between the MS and the PAN images in this work based on the aforementioned observation. Specifically, cartoon similarity is determined through gradient sparsity and formulated as a total variation term, whereas texture similarity is described according to the low-rank property. The alternative direction multiplier method is used to solve the optimization problem. In the experiment, the Gaofen-1 satellite dataset is used to compare the proposed method with other classical pansharpening methods. Experimental results demonstrate that our method outperforms the comparison methods in terms of visual and quantitative qualities.},
keywords={Pansharpening;cartoon-texture decomposition;total variation;low-rank;alternating direction method of multipliers},
doi={10.1109/ICASSP40776.2020.9053046},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053484,
author={C. P. {Bailey} and S. {Chamadia} and D. A. {Pados}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Alternative Signature Design Using L1 Principal Components for Spread-Spectrum Steganography},
year={2020},
volume={},
number={},
pages={2693-2696},
abstract={As methods for detecting hidden data evolve, there exits an ever increasing need to develop new steganographic solutions. This paper introduces novel spread spectrum (SS) and improved spread spectrum (ISS) multimedia data embedding techniques using L1 principal component signatures. The design presented performs well in terms of bit error rate and the structural similarity index metric.},
keywords={Steganography;spread-spectrum;L1 PCA;data hiding},
doi={10.1109/ICASSP40776.2020.9053484},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053463,
author={T. {Nakachi} and Y. {Wang} and H. {Kiya}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Privacy-Preserving Pattern Recognition Using Encrypted Sparse Representations in L0 Norm Minimization},
year={2020},
volume={},
number={},
pages={2697-2701},
abstract={In this paper, we propose a privacy-preserving pattern recognition method that uses encrypted sparse representations in L0 norm minimization. We prove, theoretically, that the proposal has exactly the same dictionary and sparse coefficient estimation performance as the Label Consistent K-Singular Value Decomposition (LC-KSVD) algorithm for non-encrypted signals. It can be directly implemented by the LC-KSVD algorithm without any modification. Finally, we demonstrate its excellent recognition performance and security strength for the face recognition task using the Extended YaleB database.},
keywords={Sparse Representation;Label Consistent K-SVD;Random Unitary Transform;Pattern Recognition;Face Recognition},
doi={10.1109/ICASSP40776.2020.9053463},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053759,
author={S. {Shimizu} and T. {Suzuki}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Flexibly-Tunable Bitcube-Based Perceptual Encryption Within Jpeg Compression},
year={2020},
volume={},
number={},
pages={2702-2706},
abstract={We propose a perceptual encryption within JPEG compression (EWJ). Although some of the conventional EWJs have the ‘tun-ability,’ which is a property of how many perceptual degradation levels can be provided with the single encryption technique, it is insufficient because either strong level of security or low bitrate overhead is regarded as important. The proposed EWJ detects the specific subspaces of the bits in the quantized discrete cosine transform (QDCT) coefficient blocks (‘bitcubes’), such that the non-zero bits crowd, and then the bits are permuted within each bitcube, i.e., not only within each bitplane like the conventional EWJs but also between adjacent bitplanes. The experiments show that the bitcube-based EWJ actually provides more flexible tunability than the conventional EWJs, while compromising the relation between the bitrate overhead and the attack robustness.},
keywords={bitplane;flexible tunability;JPEG;perceptual encryption;QDCT domain.},
doi={10.1109/ICASSP40776.2020.9053759},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054373,
author={X. {Hu} and D. {Olesen} and P. {Knudsen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Gyroscope Aided Video Stabilization Using Nonlinear Regression on Special Orthogonal Group},
year={2020},
volume={},
number={},
pages={2707-2711},
abstract={This paper presents a novel approach for gyroscope aided video stabilization. With the raw 3D rotational motion captured by a gyroscope, it is then smoothed through nonlinear regression directly on the Special Orthogonal Group. Instead of solving a variational problem, the regression problem is discretized with finite forward difference, which makes it an optimization problem on manifold. We derive a quadratic approximation of the objective function using Lie algebra. To address the black border problem caused by smoothing, we model it as linear inequality constraints. The resulting quadratic programming problem can be efficiently solved. Experiments on synthetic data and real video sequences demonstrate that our method performs better than the compared method in terms of motion smoothing and video stabilization.},
keywords={Video stabilization;Manifold Optimization;Matrix Lie Group;Lie Algebra},
doi={10.1109/ICASSP40776.2020.9054373},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053634,
author={Z. {Tu} and J. {Lin} and Y. {Wang} and B. {Adsumilli} and A. C. {Bovik}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Bband Index: a No-Reference Banding Artifact Predictor},
year={2020},
volume={},
number={},
pages={2712-2716},
abstract={Banding artifact, or false contouring, is a common video compression impairment that tends to appear on large flat regions in encoded videos. These staircase-shaped color bands can be very noticeable in high-definition videos. Here we study this artifact, and propose a new distortion-specific no-reference video quality model for predicting banding artifacts, called the Blind BANding Detector (BBAND index). BBAND is inspired by human visual models. The proposed detector can generate a pixel-wise banding visibility map and output a banding severity score at both the frame and video levels. Experimental results show that our proposed method outperforms state-of-the-art banding detection algorithms and delivers better consistency with subjective evaluations.},
keywords={Video quality predictor;compression artifact;banding artifact;false contour;human visual model},
doi={10.1109/ICASSP40776.2020.9053634},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053056,
author={S. V. {Reddy Dendi} and C. {Dev} and N. {Kothari} and S. S. {Channappayya}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Lqaid: Localized Quality Aware Image Denoising Using Deep Convolutional Neural Networks},
year={2020},
volume={},
number={},
pages={2717-2721},
abstract={In this paper we propose the Localized Quality Aware Image Denoising (LQAID) technique for image denoising using deep convolutional neural networks (CNNs). LQAID relies on local quality estimates over global cues like noise standard deviation since the perceptual quality of a noisy image is typically spatially varying. Specifically, we use localized quality maps generated using DistNet, a spatial quality map estimation method. These quality maps are used to augment the noisy image and guide the denoising process. The augmented noisy image is denoised using a deep fully convolutional network (FCN) trained using mean square error (MSE) as the loss function. The proposed approach shows state-of-the-art performance both qualitatively and quantitatively on two vision datasets: TID 2008 and BSD500. We also show that the proposed approach possesses excellent generalization ability. Lastly, the proposed approach is completely blind since it neither requires information about the strength of the additive noise nor does it try to explicitly estimate it.},
keywords={Distortion map;denoising;fully convolutional network and perceptual quality},
doi={10.1109/ICASSP40776.2020.9053056},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054258,
author={X. {Kong} and M. {Zhao} and H. {Zhou} and C. {Zhang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Weakly Supervised Crowd-Wise Attention For Robust Crowd Counting},
year={2020},
volume={},
number={},
pages={2722-2726},
abstract={Due to a wide range of various application scenes, robust crowd counting is still quite difficult and the performance is far from being satisfied. In this paper, we propose a novel robust crowd counting method by introducing a weakly supervised crowd-wise attention network. The proposed work improves the counting accuracy and robustness by: i) Weakly-supervised crowd segmentation. With a generated segmentation label using motion-guided region-growth, both the appearance feature of one-labeled image and motion features abstracted from its adjacent unlabeled frames, are combined to implement weakly supervised crowd region segmentation, with which active crowd region can be finely perceived from different background disturbances. ii) More accurate spatial attention. We generate a spatial attention map based on the active crowd segmentation, which is used to reweigh the appearance feature to achieve attention-based density estimation. Evaluation of the widely used World Expo’ 10 dataset shows that the proposed work can achieve state-of-the-art performance on both accuracy and robustness.},
keywords={crowd counting;crowd segmentation;spatial attention},
doi={10.1109/ICASSP40776.2020.9054258},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054089,
author={C. R. {Helmrich} and M. {Siekmann} and S. {Becker} and S. {Bosse} and D. {Marpe} and T. {Wiegand}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Xpsnr: A Low-Complexity Extension of The Perceptually Weighted Peak Signal-To-Noise Ratio For High-Resolution Video Quality Assessment},
year={2020},
volume={},
number={},
pages={2727-2731},
abstract={The objective PSNR metric is known to correlate quite poorly with subjective assessments of video coding quality. Thus, a number of alternative VQA measures such as (MS-)SSIM and VMAF have been proposed. These, however, are often algorithmically complex and difficult to use for visually motivated encoder optimization tasks, especially subjectively optimized bit allocation. In this paper we show that, by way of low-complexity enhancements of our previous work on a perceptually weighted PSNR (WPSNR) metric, addressing shortcomings with video and ultra high-definition content, the prediction of human judgments of video coding quality by the WPSNR can be improved. In fact, the resulting XPSNR seems to match the performance of the aforementioned state-of-the-art methods.},
keywords={PSNR;SSIM;UHD;video coding;VQA},
doi={10.1109/ICASSP40776.2020.9054089},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052906,
author={Y. {Sugito} and M. {Bertalmío}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Non-Experts or Experts? Statistical Analyses of MOS using DSIS method},
year={2020},
volume={},
number={},
pages={2732-2736},
abstract={In image quality assessments, the results of subjective evaluation experiments that use the double-stimulus impairment scale (DSIS) method are often expressed in terms of the mean opinion score (MOS), which is the average score of all subjects for each test condition. Some MOS values are used to derive image quality criteria, and it has been assumed that it is preferable to perform tests with non-expert subjects rather than with experts. In this study, we analyze the results of several subjective evaluation experiments using the DSIS method. Our first contribution is to discuss the statistical meaning of the MOS values, which has not been previously addressed in the literature. Second, our results show that, contrary to the established belief, there are advantages when performing subjective tests with experts, in that they allow experiments to be performed with fewer subjects, and to better determine the lower threshold of image quality.},
keywords={The double-stimulus impairment scale (DSIS) method;mean opinion score (MOS);non-expert;expert;statistical analysis},
doi={10.1109/ICASSP40776.2020.9052906},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053739,
author={L. F. {Tiotsop} and A. {Servetti} and E. {Masala}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Full Reference Video Quality Measures Improvement Using Neural Networks},
year={2020},
volume={},
number={},
pages={2737-2741},
abstract={The accuracy of video quality metrics (VQMs) is an important issue for several applications. In this work, first we observe that the accuracy of several video quality metrics (VQMs) is strongly related to the spatial complexity index (SI) of the source. In particular, our investigation suggests that the VQMs are more likely to inaccurately predict the subjective quality of the processed video sequences derived from sources characterized by low SI. To address such a situation, we propose a machine learning based improvement for each of the VQMs considered in this work and a video quality metric fusion index (VQMFI) that jointly exploits all the VQMs considered in the study as well as spatiotemporal features to produce a better estimation of the subjective quality. Computational results demonstrate the superiority of our proposals on several datasets.},
keywords={Video quality;machine learning;spatial activity},
doi={10.1109/ICASSP40776.2020.9053739},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053731,
author={J. {Huang} and C. {Cui} and C. {Zhang} and Z. {Shen} and J. {Yu} and Y. {Yin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning Multi-Scale Attentive Features for Series Photo Selection},
year={2020},
volume={},
number={},
pages={2742-2746},
abstract={People used to take a series of nearly identical photos about the same subject, but it is usually a tedious chore to select the reversed ones from them. Despite the remarkable progress, most existing studies on image aesthetics assessment fail to fulfill the task of series photo selection. In this paper, we develop a novel deep CNN architecture that aggregates multi-scale features from different network layers, in order to capture the subtle differences between series photos. To reduce the risk of redundant or even interfering features, we introduce the spatial-channel self-attention mechanism to adaptively recalibrate the features at each layer, so that informative features can be selectively emphasized and less useful ones suppressed. Extensive experiments on a benchmark dataset well demonstrate the potential of our approach for series photo selection.},
keywords={Aesthetics assessment;series photo selection;multi-scale;self-attention mechanism},
doi={10.1109/ICASSP40776.2020.9053731},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052903,
author={H. {Liu} and P. {Wei} and W. {Huang} and G. {Hua} and F. {Meng}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Spatio-Temporal and Geometry Constrained Network for Automobile Visual Odometry},
year={2020},
volume={},
number={},
pages={2747-2751},
abstract={Visual odometry (VO) is an essence of vision-based localization and mapping system where existing learning-based approaches utilize CNN and RNN to model camera motion and gain promising results. However, these methods lack full use of the relationship between spatial characteristics and temporal clues, as well as geometry constraints in VO. To overcome these deficiencies, an end-to-end framework that leverages spatio-temporal relevance and geometrical knowledge is proposed. In particular, a spatial response module (SRM) is designed to extract the visual motion features by emphasizing the most interconnected regions while suppressing the irrelevant areas. A module named temporal response module (TRM) is used to regress the camera motion via adopting the optimal motion features. Moreover, a geometry constrained (GC) loss that minimizes the estimated inter-frame pose errors and the accumulated pose errors within a local period is introduced. Actually, the GC loss utilizes adaptive learnable balance factors for balancing losses. Experimental results on KITTI and Malaga datasets demonstrate that the proposed model outperforms state-of-the-art monocular methods.},
keywords={Visual Odometry;Spatio-Temporal Relevance;Geometry Constrained Loss},
doi={10.1109/ICASSP40776.2020.9052903},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053116,
author={S. {Jaballah} and A. {Bhavsar} and M. {Larabi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Comprehensive Framework for 2D-JND Extension to 360-DEG Images},
year={2020},
volume={},
number={},
pages={2752-2756},
abstract={Masking effect is one of the most important perceptual properties that could be modeled by estimating an adaptive threshold known as the just noticeable difference (JND) referring to the maximum difference not perceived by the human visual system (HVS). In this paper, a novel framework to extend 2D-JND models to estimate thresholds for 360-degree images is proposed. The JND is estimated by viewports instead of applying it to the projected format image. Then, the viewport-based JND maps are back-projected to obtain the 360-JND map. To reduce the visible boundaries between viewports an alpha blending process is applied. The validation of the proposed framework is made using subjective experiments. It demonstrates that when applied to 360-degree images, the proposed framework outperforms the 2D-JND models in terms of observers preference at the same noise level.},
keywords={360-degreeimages;Just Noticeable Difference (JND);Human visual system;Equirectangular projection},
doi={10.1109/ICASSP40776.2020.9053116},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054084,
author={R. {Singh} and S. {Yu} and J. C. {Principe}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Composite Dynamic Texture Synthesis Using Hierarchical Linear Dynamical System},
year={2020},
volume={},
number={},
pages={2757-2761},
abstract={We demonstrate that a systematic inclusion of prior structural constraints on the states of a linear dynamical system significantly improves its ability to model complex multidimensional sequences. This constrained LDS, typically termed as the hierarchical linear dynamical system (HLDS), is a Kalman filter based topology that extracts relevant self-segmenting information from the input signal in an unsupervised manner by hierarchically constraining its information representing state subspaces thereby slowing down the signal dynamics. We highlight some of its practical advantages over the existing methods in real-world video applications. As a concrete application, we show that the HLDS, despite being a linear model trained in an unsupervised setting, is able to capture the dynamics of complex texture sequences consisting of multiple co-occurring textures. We compare its performance with a similarly trained LDS model in the reconstruction and synthesis of such signals.},
keywords={LDS;hierarchical;unsupervised;texture;synthesis},
doi={10.1109/ICASSP40776.2020.9054084},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053696,
author={J. {Butora} and J. {Fridrich}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Steganography and its Detection in JPEG Images Obtained with the "TRUNC" Quantizer},
year={2020},
volume={},
number={},
pages={2762-2766},
abstract={Many portable imaging devices use the operation of "trunc" (rounding towards zero) instead of rounding as the final quantizer for computing DCT coefficients during JPEG compression. We show that this has rather profound consequences for steganography and its detection. In particular, side-informed steganography needs to be redesigned due to the different nature of the rounding error. The steganographic algorithm J-UNIWARD becomes vulnerable to steganalysis with the JPEG rich model and needs to be adjusted for this source. Steganalysis detectors need to be retrained since a steganalyst unaware of the existence of the trunc quantizer will experience 100% false alarm.},
keywords={Steganography;side-information;trunc quantizer;steganalysis;JPEG},
doi={10.1109/ICASSP40776.2020.9053696},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054486,
author={Q. {Giboulot} and R. {Cogranne} and P. {Bas}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={JPEG Steganography with Side Information from the Processing Pipeline},
year={2020},
volume={},
number={},
pages={2767-2771},
abstract={The current art in schemes using deflection criterion such as Mi-POD for JPEG steganography is either under-performing or on par with distortion-based schemes. We link this lack of performance to a poor estimation of the variance of the model of the noise on the cover image. In this paper, we propose a method to better estimate the variances of DCT coefficients by taking into account the dependencies between pixels that come from the development pipeline. Using this estimate, we are able to extend statistically-informed steganographic schemes to the JPEG domain while significantly outperforming the current state-of-the-art JPEG steganography. An extension of Gaussian Embedding in the JPEG domain using quantization error as side-information is also formulated and shown to attain state-of-the-art performances.},
keywords={Steganography;JPEG images;Statistical model;Side-information;Covariance estimation},
doi={10.1109/ICASSP40776.2020.9054486},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054061,
author={R. {Cogranne}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Selection-Channel-Aware Reverse JPEG Compatibility for Highly Reliable Steganalysis of JPEG Images},
year={2020},
volume={},
number={},
pages={2772-2776},
abstract={This paper deeply studies the principle of the recent reverse JPEG compatibility attack [1]. This analysis allows us to cast the problem of hidden data detection in DCT coefficients within hypothesis testing theory. The optimal LR test, thought efficient, is rather computationally expensive. Therefore, mild assumptions are used to simplify the detection problem dramatically and design a test that is simple yet extremely efficient and reliable. It is shown that the proposed detector is way more efficient than the original statistical test [1], and allows highly reliable detection of data hidden within JPEG images.},
keywords={Steganalysis;Hypothesis testing;JPEG compression;Statistical methods;Reliable Detection},
doi={10.1109/ICASSP40776.2020.9054061},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054673,
author={S. {Schwarz} and A. {Theóphilo} and A. {Rocha}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={EMET: Embeddings from Multilingual-Encoder Transformer for Fake News Detection},
year={2020},
volume={},
number={},
pages={2777-2781},
abstract={In the last few years, social media networks have changed human life experience and behavior as it has broken down communication barriers, allowing ordinary people to actively produce multimedia content on a massive scale. On this wise, the information dissemination in social media platforms becomes increasingly common. However, misinformation is propagated with the same facility and velocity as real news, though it can result in irreversible damage to an individual or society at large. Solving this problem is not a trivial task, considering the reduced size of the text messages usually posted on these communication vehicles. This paper proposes an end-to-end framework called EMET to classify the reliability of small messages posted on social media platforms. Our method leverages text-embeddings from multilingual-encoder transformers that take into consideration the semantic knowledge from preceding trustworthy news and the use of the reader’s reactions to detect misleading content. Our findings demonstrated the value of user interaction and prior information to check social media post’s credibility.},
keywords={Fake News Detection;Information Forensics;Deep Learning;Natural Language Processing;Social Media Data},
doi={10.1109/ICASSP40776.2020.9054673},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052979,
author={F. {Matern} and C. {Riess} and M. {Stamminger}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Depth Map Fingerprinting and Splicing Detection},
year={2020},
volume={},
number={},
pages={2782-2786},
abstract={With the ubiquity of social networks, images have become crucial in todays exchange of information. Most of these images are taken by smartphones. For forensic approaches relying on fixed image formation pipelines, the capabilities of smartphones using computational photography pose new challenges. But these new capabilities also offer opportunities for forensic analysis. A growing amount of commodity devices are able to capture 3-D information using various technologies such as stereo imaging or structured light. Modern smartphones commonly save such 3-D information as depth maps alongside regular images.In this work, we propose to use characteristic artifacts of depth reconstruction algorithms as trace for forensic analysis. The proposed method is able to infer the source algorithm of stereo reconstructions with an accuracy of up to 97%. We further demonstrate the applicability of the method to collected smartphone data. It is able to discriminate patches from different sources with an AUC of up to 0.88 and can be used for splicing localization in depth maps.},
keywords={image forensics;source identification;forgery detection;depth;smartphone},
doi={10.1109/ICASSP40776.2020.9052979},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053926,
author={X. {Liao} and Z. {Huang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Framework for Parameters Estimation of Image Operator Chain},
year={2020},
volume={},
number={},
pages={2787-2791},
abstract={Currently, many effective techniques have been proposed to estimate the parameters of tampering operations. Most of them consider the situation that an image is tampered by only one operation. However, multiple manipulation operations are always used to tamper an image in our daily life. Moreover, since the tampering traces of previous operations may be weakened or eliminated by later ones, the detecting accuracy of methods used for detecting single operations would be reduced. In this paper, we propose a new method to estimate the parameters of operations in different manipulation chains. Especially, we first investigate the correlation of operations and divide the degree of interactions between them into uncoupled and coupled. Furthermore, resizing and median filtering are adopted to reveal the assessment framework. Meanwhile, we propose well-directed features including energy density of difference-image to estimate operation parameters. The experiment proves the effectiveness of our method.},
keywords={Image forensics;operator chains;parameters estimation;coupled operations;uncoupled operations},
doi={10.1109/ICASSP40776.2020.9053926},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053729,
author={E. J. {Chou} and A. {Gururajan} and K. {Laine} and N. K. {Goel} and A. {Bertiger} and J. W. {Stokes}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Privacy-Preserving Phishing Web Page Classification Via Fully Homomorphic Encryption},
year={2020},
volume={},
number={},
pages={2792-2796},
abstract={This work introduces a fast and lightweight homomorphic-encryption pipeline that enables privacy-preserving machine learning for phishing web page recognition. The primary goals are to use visual features to train an accurate model and to implement an inference pipeline with practical runtime and communication costs. To do so, we deploy a variety of techniques that cover deep learning and optical character recognition to extract salient visual features, and optimize the inner mechanisms of state-of-the-art homomorphic encryption schemes to reduce the encryption-related costs. Our presented system is able to achieve over 90% on the visual classification task, while using less than 250 KB of communication bandwidth and around 0.7 seconds of computation time. We hope our work not only demonstrates a private visual phishing detection pipeline, but also outlines techniques to practically utilize homomorphic encryption in a variety of machine learning tasks.},
keywords={Phishing;Classification;Homomorphic Encryption},
doi={10.1109/ICASSP40776.2020.9053729},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054046,
author={S. {Ferdowsi} and B. {Razeghi} and T. {Holotyak} and F. P. {Calmon} and S. {Voloshynovskiy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Privacy-Preserving Image Sharing Via Sparsifying Layers on Convolutional Groups},
year={2020},
volume={},
number={},
pages={2797-2801},
abstract={We propose a practical framework to address the problem of privacy-aware image sharing in large-scale setups. We argue that, while compactness is always desired at scale, this need is more severe when trying to furthermore protect the privacy-sensitive content. We therefore encode images, such that, from one hand, representations are stored in the public domain without paying the huge cost of privacy protection, but ambiguated and hence leaking no discernible content from the images, unless a combinatorially-expensive guessing mechanism is available for the attacker. From the other hand, authorized users are provided with very compact keys that can easily be kept secure. This can be used to disambiguate and reconstruct faithfully the corresponding access-granted images. We achieve this with a convolutional autoencoder of our design, where feature maps are passed independently through sparsifying transformations, providing multiple compact codes, each responsible for reconstructing different attributes of the image. The framework is tested on a large-scale database of images with public implementation available.},
keywords={privacy-preserving image sharing;convolutional autoencoders;sparse representation;learned compression;image obfuscation},
doi={10.1109/ICASSP40776.2020.9054046},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053868,
author={B. M. {Lal Srivastava} and N. {Vauquier} and M. {Sahidullah} and A. {Bellet} and M. {Tommasi} and E. {Vincent}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Evaluating Voice Conversion-Based Privacy Protection against Informed Attackers},
year={2020},
volume={},
number={},
pages={2802-2806},
abstract={Speech data conveys sensitive speaker attributes like identity or accent. With a small amount of found data, such attributes can be inferred and exploited for malicious purposes: voice cloning, spoofing, etc. Anonymization aims to make the data unlinkable, i.e., ensure that no utterance can be linked to its original speaker. In this paper, we investigate anonymization methods based on voice conversion. In contrast to prior work, we argue that various linkage attacks can be designed depending on the attackers’ knowledge about the anonymization scheme. We compare two frequency warping-based conversion methods and a deep learning based method in three attack scenarios. The utility of converted speech is measured via the word error rate achieved by automatic speech recognition, while privacy protection is assessed by the increase in equal error rate achieved by state-of-the-art i-vector or x-vector based speaker verification. Our results show that voice conversion schemes are unable to effectively protect against an attacker that has extensive knowledge of the type of conversion and how it has been applied, but may provide some protection against less knowledgeable attackers.},
keywords={privacy;voice conversion;speech recognition;speaker verification;linkage attack},
doi={10.1109/ICASSP40776.2020.9053868},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053107,
author={O. {Günlü} and R. F. {Schaefer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low-Complexity and Reliable Transforms for Physical Unclonable Functions},
year={2020},
volume={},
number={},
pages={2807-2811},
abstract={Noisy measurements of a physical unclonable function (PUF) are used to store secret keys with reliability, security, privacy, and complexity constraints. A new set of low-complexity and orthogonal transforms with no multiplication is proposed to obtain bit-error probability results significantly better than all methods previously proposed for key binding with PUFs. The uniqueness and security performance of a transform selected from the proposed set is shown to be close to optimal. An error-correction code with a low-complexity decoder and a high code rate is shown to provide a block-error probability significantly smaller than provided by previously proposed codes with the same or smaller code rates.},
keywords={physical unclonable function (PUF);no multiplication transforms;secret key agreement;low complexity},
doi={10.1109/ICASSP40776.2020.9053107},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054604,
author={O. {Taran} and S. {Bonev} and T. {Holotyak} and S. {Voloshynovskiy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adversarial Detection of Counterfeited Printable Graphical Codes: Towards "Adversarial Games" In Physical World},
year={2020},
volume={},
number={},
pages={2812-2816},
abstract={This paper addresses a problem of anti-counterfeiting of physical objects and aims at investigating a possibility of counterfeited printable graphical code detection from a machine learning perspectives. We investigate a fake generation via two different deep regeneration models and study the authentication capacity of several discriminators on the data set of real printed graphical codes where different printing and scanning qualities are taken into account. The obtained experimental results provide a new insight on scenarios, where the printable graphical codes can be accurately cloned and could not be distinguished.},
keywords={Printable graphical codes;clonability attacks;adversarial discriminators;machine learning},
doi={10.1109/ICASSP40776.2020.9054604},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054389,
author={R. {Castelletto} and S. {Milani} and P. {Bestagini}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Phylogenetic Minimum Spanning Tree Reconstruction Using Autoencoders},
year={2020},
volume={},
number={},
pages={2817-2821},
abstract={The history of a shared and re-posted multimedia content can be reconstructed by analyzing the mutual relations between all of its near-duplicate copies and solving a minimum spanning tree (MST) problem, as shown by multimedia phylogeny research field, Unfortunately, MST estimation strategies are severely impaired by the noise affecting dissimilarity measures between pairs of near-duplicate contents, For this reason, researchers have recently been investigating robust dissimilarity metrics.This paper proposes a matrix denoising solution that both mitigates dissimilarity noise and reconstruct the desired phylogenetic tree at the same time, The proposed strategy is a first attempt to estimate a MST via a denoising autoencoder that returns an approximation of the adjacency matrix corresponding to the underlying tree, Experimental results prove that the proposed solution outperforms the previous approaches and easily adapts to different analysis scenarios.},
keywords={noisy minimum spanning tree;image phylogeny;autoencoder;UNET},
doi={10.1109/ICASSP40776.2020.9054389},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054361,
author={H. {Yang} and Z. {Yang} and Y. {Bao} and S. {Liu} and Y. {Huang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={FCEM: A Novel Fast Correlation Extract Model For Real Time Steganalysis Of VoIP Stream Via Multi-Head Attention},
year={2020},
volume={},
number={},
pages={2822-2826},
abstract={Extracting correlation features between codes-words with high computational efficiency is crucial to steganalysis of Voice over IP (VoIP) streams. In this paper, we utilized attention mechanisms, which have recently attracted enormous interests due to their highly parallelizable computation and flexibility in modeling correlation in sequence, to tackle steganalysis problem of Quantization Index Modulation (QIM) based steganography in compressed VoIP stream. We design a light-weight neural network named Fast Correlation Extract Model (FCEM) only based on a variant of attention called multi-head attention to extract correlation features from VoIP frames. Despite its simple form, FCEM outperforms complicated Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) models on both prediction accuracy and time efficiency. It significantly improves the best result in detecting both low embedded rates and short samples recently. Besides, the proposed model accelerates the detection speed as twice as before when the sample length is as short as 0.1s, making it a excellent method for online services.},
keywords={Attention mechanisms;QIM Based Steganography;Voice over IP;Steganalysis},
doi={10.1109/ICASSP40776.2020.9054361},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054397,
author={J. {Yang} and H. {Zheng} and X. {Kang} and Y. {Shi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Approaching Optimal Embedding In Audio Steganography With GAN},
year={2020},
volume={},
number={},
pages={2827-2831},
abstract={Audio steganography is a technology that embeds messages into audio without raising any suspicion from hearing it. Current steganography methods are based on heuristic cost designs. In this work, we proposed a framework based on Generative Adversarial Network (GAN) to approach optimal embedding for audio steganography in the temporal domain. This is the first attempt to approach optimal embedding with GAN and automatically learn the embedding probability/cost for audio steganography. The embedding framework consists of three parts: a U-Net based generator, an embedding simulator, and a discriminator. For practical applications, Syndrome-Trellis Coding (STC) is used to generate stego audio with the learned embedding probability. Experimental results on the UME-ERJ and WSJ speech datasets have shown that the proposed framework can automatically learn the adaptive embedding probabilities for audio steganogra- phy and has a considerable advantage in terms of resisting steganalyzers in comparison with the existing conventional method.},
keywords={Generators;Training;Gallium nitride;Generative adversarial networks;Security;Error analysis;Kernel;Audio;steganography;Generative Adversarial Network (GAN)},
doi={10.1109/ICASSP40776.2020.9054397},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054033,
author={W. {Cui} and S. {Liu} and F. {Jiang} and Y. {Liu} and D. {Zhao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multi-Stage Residual Hiding for Image-Into-Audio Steganography},
year={2020},
volume={},
number={},
pages={2832-2836},
abstract={The widespread application of audio communication technologies has speeded up audio data flowing across the Internet, which made it a popular carrier for covert communication. In this paper, we present a cross-modal steganography method for hiding image content into audio carriers while preserving the perceptual fidelity of the cover audio. In our framework, two multi-stage networks are designed: the first network encodes the decreasing multilevel residual errors inside different audio subsequences with the corresponding stage sub-networks, while the second network decodes the residual errors from the modified carrier with the corresponding stage sub-networks to produce the final revealed results. The multi-stage design of proposed framework not only make the controlling of payload capacity more flexible, but also make hiding easier because of the gradual sparse characteristic of residual errors. Qualitative experiments suggest that modifications to the carrier are unnoticeable by human listeners and that the decoded images are highly intelligible.},
keywords={Audio steganography;residual hiding;multi-stage network;convolutional neural networks},
doi={10.1109/ICASSP40776.2020.9054033},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054386,
author={H. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Patch-Level Selection and Breadth-First Prediction Strategy for Reversible Data Hiding},
year={2020},
volume={},
number={},
pages={2837-2841},
abstract={A core work in reversible data hiding is designing an embedding method enabling the hider to take advantages of smooth elements as many as possible while the detection procedure for marked elements is invertible to the receiver. It motivates us to introduce a novel patch-level selection and breadth-first prediction strategy for efficient reversible data hiding. However, different from conventional works, the proposed work allows us to preferentially and simultaneously use adjacent smooth elements as many as possible. Experiments show that it significantly outperforms a part of state-of-the-arts at relatively low embedding rates, demonstrating the superiority.},
keywords={reversible data hiding (RDH);watermarking;breadth-first search (BFS);patch;fragile},
doi={10.1109/ICASSP40776.2020.9054386},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053869,
author={W. {Kim} and K. {Lee}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Digital Watermarking For Protecting Audio Classification Datasets},
year={2020},
volume={},
number={},
pages={2842-2846},
abstract={In this study, we investigate the possibility of protecting audio classification datasets used in deep learning by embedding a pattern in the magnitude of the time-frequency representation of a subset of the dataset. Previous studies on audio watermarking technologies require the actual sound of the watermarked audio to extract the information embedded in it. In our study, we propose an audio watermarking framework aimed to identify whether a deep learning based audio classification model is trained with the watermarked audio classification dataset or not by using only the classification results. The experimental results show that our proposed method can identify the usage of an audio classification dataset while having minimal effect on the overall classification performance. The results are consistent with three different audio classification datasets. The proposed method is robust to different types and parameters of time-frequency representations and classification models.},
keywords={Deep Learning;Audio Watermark;Audio Classification;Dataset Protection;Time-Frequency Representation},
doi={10.1109/ICASSP40776.2020.9053869},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053103,
author={S. {Yang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Saliency-Based Image Contrast Enhancement with Reversible Data Hiding},
year={2020},
volume={},
number={},
pages={2847-2851},
abstract={Reversible data hiding (RDH) has become a hot research area in the recent years due to its wide applications such as authentication. Among all the RDH methods proposed, contrast enhancement based reversible data hiding is one that was recently proposed. However, most of the existing schemes proposed focus on image itself without taking human visual system (HVS) into account. Consequently, the full range of the redundancy from HVS cannot be fully exploited, and the resulting image may be visually unpleasing. In this paper, a new approach is proposed by introducing the visual saliency into the process of reversible data hiding. Specifically, a saliency fixation map is generated by using an existing saliency prediction model, followed by a saliency guided segmentation to better enhance the salient regions in the input image. The evaluation results show that the proposed method yields better visual results and outperforms the existing methods in statistical metrics.},
keywords={Reversible data hiding;Visual saliency;Contrast enhancement;Human visual system;Image quality},
doi={10.1109/ICASSP40776.2020.9053103},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054420,
author={Z. {Li} and H. {Li} and K. {Lam} and A. C. {Kot}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unseen Face Presentation Attack Detection with Hypersphere Loss},
year={2020},
volume={},
number={},
pages={2852-2856},
abstract={Presentation attack is one of the main threats to face verification systems and attracts great attention of research community. Recent methods achieve great success in intra-database test. However, the problem is more complex in practical scenario as the type of attack could be unseen to system designers. In this paper, we formulate the face presentation attack detection task under an open-set setting and address with our proposed deep anomaly detection based method. The training process is end-to-end supervised by a novel hypersphere loss function and the decision making is directly based on the learned feature representation. We conduct extensive experiments on multiple prevailing databases and evaluate our implemented models by using various metrics. The results show our proposed method is effective against unseen types of attacks and superior to latest state-of-the-art.},
keywords={unseen presentation attack detection;face anti-spoofing;anomaly detection;deep metric learning},
doi={10.1109/ICASSP40776.2020.9054420},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053670,
author={F. {Tajaddodianfar} and J. W. {Stokes} and A. {Gururajan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Texception: A Character/Word-Level Deep Learning Model for Phishing URL Detection},
year={2020},
volume={},
number={},
pages={2857-2861},
abstract={Phishing is the starting point for many cyberattacks that threaten the confidentiality, availability and integrity of enterprises’ and consumers’ data. The URL of a web page that hosts the attack provides a rich source of information to determine the maliciousness of the web server. In this work, we propose a novel deep learning architecture, Texception, that takes a URL as input and predicts whether it belongs to a phishing attack. Architecturally, Texception uses both character-level and word-level information from the incoming URL and does not depend on manually crafted features or feature engineering. This makes it different from classical approaches. In addition, Texception benefits from multiple parallel convolutional layers and can grow deeper or wider. We show that this flexibility enables Texception to generalize better for new URLs. Our results on production data show that Texception is able to significantly outperform a traditional text classification method by increasing the true positive rate by 126.7% at an extremely low false positive rate (0.01%) which is crucial for our model’s healthy operation at internet scale.},
keywords={Phishing;Detection;Character-Level;Deep Learning;Word Embedding},
doi={10.1109/ICASSP40776.2020.9053670},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054006,
author={B. B. {Yilmaz} and E. {Mert Ugurlu} and A. {Zajić} and M. {Prvulovic}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Cell-Phone Classification: A Convolutional Neural Network Approach Exploiting Electromagnetic Emanations},
year={2020},
volume={},
number={},
pages={2862-2866},
abstract={In this paper, we propose a methodology to identify both the brand of a cell-phone, and the status of its camera by exploiting electromagnetic (EM) emanations. The method is composed of two parts: Feature extraction and Convolutional Neural Network (CNN). We first extract features by averaging magnitudes of short-time Fourier transform (STFT) of the measured EM signal, which helps to reduce input dimension of the neural network, and to filter spurious emissions. The extracted features are fed into the proposed CNN, which contains two convolutional layers (followed by max-pooling layers), and four fully-connected layers. Finally, we provide experimental results which exhibit more than 99% classification accuracy for the test signals.},
keywords={Security;Classification;Convolutional Neural Network;Electromagnetic Emanations},
doi={10.1109/ICASSP40776.2020.9054006},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054043,
author={M. A. {Gutierrez-Estevez} and Z. {Utkovski} and P. {Agostini} and D. {Schäufele} and M. {Frey} and I. {Bjelakovic} and S. {Stańczak}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Quality-of-Service Prediction for Physical-layer Security via Secrecy Maps},
year={2020},
volume={},
number={},
pages={2867-2871},
abstract={While most of the theoretical aspects of physical layer security are well understood, practical applications lag substantially behind theoretical advances. As a step towards the integration of physical-layer security aspects in the radio access system design, the concept of secrecy maps has been recently introduced. Building upon this concept, in this work we focus on system design-related aspects which consider physical-layer security as a service, together with thereby associated secrecy-related Quality-of-Service (QoS) requirements. We adopt a statistical learning framework to characterize the wireless environment from the perspective of semantic security via QoS maps that indicate locations in the wireless environment that surmount a QoS-related performance threshold with a guaranteed level of confidence. To provide secrecy characterization of a particular wireless environment, we propose to learn any-to-any QoS maps by leveraging the tensor completion framework. We demonstrate the advantage of the proposed method over a baseline algorithm based on tensor rank minimization via numerical simulations.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054043},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054336,
author={W. {Labidi} and C. {Deppe} and H. {Boche}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Secure Identification for Gaussian Channels},
year={2020},
volume={},
number={},
pages={2872-2876},
abstract={New applications in modern communications are demanding robust and ultra-reliable low latency information exchange such as machine-to-machine and human-to-machine communications. For many of these applications, the identification approach of Ahlswede and Dueck is much more efficient than the classical transmission scheme proposed by Shannon. Previous studies concentrate mainly on identification over discrete channels. We focus on Gaussian channels for their known practical relevance. We deal with secure identification over Gaussian channels. In particular, we provide a suitable coding scheme for the Gaussian wiretap channel (GWC) and determine the corresponding secure identification capacity.},
keywords={Identification theory;Information theoretic security;Gaussian Channels},
doi={10.1109/ICASSP40776.2020.9054336},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052911,
author={C. {Han} and A. {Liu} and L. {Huo} and H. {Wang} and X. {Liang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Anti-Jamming Routing For Internet of Satellites: a Reinforcement Learning Approach},
year={2020},
volume={},
number={},
pages={2877-2881},
abstract={The anti-jamming routing for the Internet of Satellites (IoS) has drawn increasing attentions due to the unknown interrupts, unexpected congestion and smart jamming. This paper investigates anti-jamming routing scheme for heterogeneous IoS, with the aim of minimizing anti-jamming routing cost. Firstly, to tackle the smart jamming which can automatically change jamming strategies according to the jamming effect, we formulate the routing anti-jamming problem as a hierarchical anti-jamming Stackelberg game. Secondly, we propose a deep reinforcement learning based routing algorithm (DRLR) to obtain an available routing path subset. Furthermore, based on this set, a fast response anti-jamming algorithm (FRA) is proposed to achieve fast and reliable antijamming routing. Finally, the simulations have shown that the proposed algorithm have lower routing cost and better antijamming performance than existing approaches.},
keywords={Internet of Satellites;Anti-jamming communication;Reinforcement learning;Stackelberg game;Routing selection},
doi={10.1109/ICASSP40776.2020.9052911},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053913,
author={F. {Lemarchand} and C. {Marlin} and F. {Montreuil} and E. {Nogues} and M. {Pelcat}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Electro-Magnetic Side-Channel Attack Through Learned Denoising and Classification},
year={2020},
volume={},
number={},
pages={2882-2886},
abstract={This paper proposes an upgraded Electro Magnetic (EM) sidechannel attack that automatically reconstructs the intercepted data. A novel system is introduced, running in parallel with leakage signal interception and catching compromising data on the fly. Leveraging on deep learning and Character Recognition (CR) the proposed system retrieves more than 57% of characters present in intercepted signals regardless of signal type: analog or digital. The building of the learning database is detailed and the resulting data made publicly available. The solution is based on Software-Defined Radio (SDR) and Graphics Processing Unit (GPU) architectures. It can be easily deployed onto existing information systems to detect compromising data leakage that should be kept secret.},
keywords={Electro-Magnetic Side-Channel;Denoising;Automation},
doi={10.1109/ICASSP40776.2020.9053913},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054390,
author={J. W. {Stokest} and R. {Agrawal} and G. {McDonald}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Detection of Malicious Vbscript Using Static and Dynamic Analysis with Recurrent Deep Learning},
year={2020},
volume={},
number={},
pages={2887-2891},
abstract={Attackers have used malicious VBScripts as an important computer infection vector. In this study, we explore a system that employs both static and dynamic analysis to detect malicious VBScripts. For the static analysis, we investigate two deep recurrent models, LaMP (LSTM and Max Pooling) and CPoLS (Convoluted Partitioning of Long Sequences), which process a VBScript as a byte sequence. Lower layers capture the sequential nature of these byte sequences while higher layers classify the resulting embedding as malicious or benign. Our models are trained in an end-to-end fashion allowing discriminative training even for the sequential processing layers. Dynamic analysis allows us to investigate obfuscated VBScripts an additional files which may be dropped during execution. Evaluating these models on a large corpus of 240,504 VBScript files indicates that the best performing LaMP model has a 69.3% true positive rate (TPR) at a false positive rate (FPR) of 1.0%. Similarly, the best CPoLS model has a TPR of 67.9% at an FPR of 1.0%. Our system is general in nature and can be applied to other scripting languages (e.g., JavaScript) as well.},
keywords={VBScript;Detection;Recurrent Neural Network;Deep Learning},
doi={10.1109/ICASSP40776.2020.9054390},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054264,
author={Z. {Zohrevand} and U. {Glässer}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Dynamic Attack Scoring Using Distributed Local Detectors},
year={2020},
volume={},
number={},
pages={2892-2896},
abstract={Nowadays, continuously operating critical services increasingly rely on complex cyber-physical systems, which are also known as high-profile targets of cyberattacks, potentially resulting in security breaches that can cause severe damage.This paper presents a novel study on detecting cyberattacks against distributed supervisory control systems. AttackTracker, a scalable and unsupervised analytic framework for behavior-based online intrusion detection, is organized as a hierarchical network of cooperating attack detectors. Each local attack detector monitors and reports the status of a subsystem by labeling observations, assigning attack scores, and raising red flags by comparing actual versus predicted signal values from the observed input stream. While higher-level detectors utilize information aggregated from detectors at lower levels to assess the global security status of the supervisory control system.Our experiments show that AttackTracker outperforms leading methods for detecting complex attacks in a real-world operational context and it can be used for intrusion detection across a wide range of cyber-physical systems.},
keywords={Cybersecurity;Intrusion detection;Time series analysis and forecasting;False alarm mitigation;Cyber-physical system},
doi={10.1109/ICASSP40776.2020.9054264},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053574,
author={X. {Yan} and X. {Chen} and Y. {Jiang} and S. {Xia} and Y. {Zhao} and F. {Zheng}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Hijacking Tracker: A Powerful Adversarial Attack on Visual Tracking},
year={2020},
volume={},
number={},
pages={2897-2901},
abstract={Visual object tracking has made important breakthroughs with the assistance of deep learning models. Unfortunately, recent research has clearly proved that deep learning models are vulnerable to malicious adversarial attacks, which mislead the models making wrong decisions by perturbing the input image. The threat to the models alerts us to pay attention to the model security of deep learning- based tracking algorithms. Therefore, we study the adversarial attacks against advanced trackers based on deep learning to better identify the vulnerability of tracking algorithms. In this paper, we propose to add slight adversarial perturbations to the input image by an inconspicuous but powerful attack strategy—hijacking algorithm. Specifically, the hijacking strategy misleads trackers in two aspects: one is shape hijacking that changes the shape of the model output; the other is position hijacking that gradually pushes the output to any position in the image frame. Besides, we further propose an adaptive optimization approach to integrate two hijacking mechanisms efficiently. Eventually, the hijacking algorithm results in fooling the tracker to track the wrong target gradually. The experimental results demonstrate the powerful attack ability of our method—quickly hijacking state-of-the-art trackers and reducing the accuracy of these models by more than 90% on OTB2015.},
keywords={Hijacking;visual tracking;adversarial attack},
doi={10.1109/ICASSP40776.2020.9053574},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052967,
author={X. {Wang} and S. {Wang} and P. {Chen} and X. {Lin} and P. {Chin}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={AdvMS: A Multi-Source Multi-Cost Defense Against Adversarial Attacks},
year={2020},
volume={},
number={},
pages={2902-2906},
abstract={Designing effective defense against adversarial attacks is a crucial topic as deep neural networks have been proliferated rapidly in many security-critical domains such as malware detection and self-driving cars. Conventional defense methods, although shown to be promising, are largely limited by their single-source single-cost nature: The robustness promotion tends to plateau when the defenses are made increasingly stronger while the cost tends to amplify. In this paper, we study principles of designing multi-source and multi-cost schemes where defense performance is boosted from multiple defending components. Based on this motivation, we propose a multi-source and multi-cost defense scheme, Adversarially Trained Model Switching (AdvMS), that inherits advantages from two leading schemes: adversarial training and random model switching. We show that the multi-source nature of AdvMS mitigates the performance plateauing issue and the multi-cost nature enables improving robustness at a flexible and adjustable combination of costs over different factors which can better suit specific restrictions and needs in practice.},
keywords={Adversarial attack;adversarial robustness;stochastic defense;adversarial training},
doi={10.1109/ICASSP40776.2020.9052967},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053419,
author={E. H. {Do} and V. N. {Gadepally}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Classifying Anomalies for Network Security},
year={2020},
volume={},
number={},
pages={2907-2911},
abstract={Detecting and classifying anomalous behaviors in computer networks remains a formidable challenge. This work outlines a machine learning technique that uses deep neural networks to detect and classify a variety of network attacks. Our approach is based on that hypothesis that different network attacks generate a distinguishable change in entropy of certain network flow features. To generate a training and validation dataset, we inject synthetic attacks of different types and intensities into raw packet capture data collected from an internet backbone link by the MAWI group. Experimental results show that our machine learning classification model can achieve high accuracy for network attacks in which attack intensities are as low as 5% of overall traffic.},
keywords={Packet Capture;Cyber Security;Anomaly Detection},
doi={10.1109/ICASSP40776.2020.9053419},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053959,
author={A. {Garnaev} and A. {Petropulu} and W. {Trappe} and H. V. {Poor}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Switching Transmission Game with Latency as the User’s Communication Utility},
year={2020},
volume={},
number={},
pages={2912-2916},
abstract={We consider the communication between a source (user) and a destination in the presence of a jammer, and study resource assignment in a non-cooperative game theory framework using communication latency as the user’s utility. The user switches between two different modes, i.e., the (a) regular transmission mode, according to which both players follow a Nash equilibrium; and the (b) smart transmission mode, according to which the user always implements the best response strategy. First, we consider the case in which the switching between transmission modes occurs with a given frequency. For this case we find the optimal transmission power of the user by formulating and solving a Bayesian game problem. We show that an increase in the frequency of smart transmissions leads to a decrease in communication latency and to an increase in the total transmission cost. We determine the switching frequency that optimizes the latency-cost trade off using α-fairness criteria. We also discuss the implications of the proposed latency metric on the player strategies as compared to the previously well studied signal-to-interference-plus-noise ratio (SINR) metric.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053959},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053404,
author={B. E. {Gardner} and J. D. {Roth}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Efficient Methodology to De-Anonymize the 5G-New Radio Physical Downlink Control Channel},
year={2020},
volume={},
number={},
pages={2917-2921},
abstract={Data anonymization methods are currently implemented in 5G-New Radio technical standards that help facilitate performance requirements while affording users with a certain expectation of privacy. This paper presents an efficient approach to negating current anonymization methods in a computationally efficient manner. The approach is achieved through manipulation of the polar coding process, allowing for the identification of syndromes in the decoded bits. These syndromes allow for the determination of the message recipient’s address. The ideas presented here will help to inform the development of future technical standards in ensuring robust data privacy. Simulations were conducted that develop a relationship between successful decoding rate as a function of signal-to-noise ratio levels and number of receiver antennas, as well as techniques to further develop the efficiency of the methodology while limiting false positive results.},
keywords={De-anonymization;New Radio;5G;Physical Downlink Control Channel;Privacy},
doi={10.1109/ICASSP40776.2020.9053404},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053306,
author={M. {Gheisari} and T. {Furon} and L. {Amsaleg}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Joint Learning of Assignment and Representation for Biometric Group Membership},
year={2020},
volume={},
number={},
pages={2922-2926},
abstract={This paper proposes a framework for group membership protocols preventing the curious but honest server from reconstructing the enrolled biometric signatures and inferring the identity of querying clients. This framework learns the embedding parameters, group representations and assignments simultaneously. Experiments show the trade-off between security/privacy and verification/identification performances.},
keywords={Group Representation;Verification;Identification;Security;Data Privacy.},
doi={10.1109/ICASSP40776.2020.9053306},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054559,
author={B. {Xin} and W. {Yang} and Y. {Geng} and S. {Chen} and S. {Wang} and L. {Huang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Private FL-GAN: Differential Privacy Synthetic Data Generation Based on Federated Learning},
year={2020},
volume={},
number={},
pages={2927-2931},
abstract={Generative Adversarial Network (GAN) has already made a big splash in the field of generating realistic "fake" data. However, when data is distributed and data-holders are reluctant to share data for privacy reasons, GAN’s training is difficult. To address this issue, we propose private FL-GAN, a differential privacy generative adversarial network model based on federated learning. By strategically combining the Lipschitz limit with the differential privacy sensitivity, the model can generate high-quality synthetic data without sacrificing the privacy of the training data. We theoretically prove that private FL-GAN can provide strict privacy guarantee with differential privacy, and experimentally demonstrate our model can generate satisfactory data.},
keywords={information security;federated learning;differential privacy;data generation},
doi={10.1109/ICASSP40776.2020.9054559},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053801,
author={Y. {Xu} and Y. {Wang} and J. {Liang} and Y. {Jiang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Augmentation Data Synthesis Via Gans: Boosting Latent Fingerprint Reconstruction},
year={2020},
volume={},
number={},
pages={2932-2936},
abstract={Latent fingerprint reconstruction is a vital preprocessing step for its identification. This task is very challenging due to not only existing complicated degradation patterns but also its scarcity of paired training data. To address these challenges, we propose a novel generative adversarial network (GAN) based data augmentation scheme to improve such reconstruction. It translates the abundant clean fingerprints to their corresponding latent ones, only exploiting a small-scale latent dataset and an unpaired large-scale clean dataset, from which a large-scale paired clean-latent augmentation set is built for the reconstruction task. Specifically, our method models the distribution of the latent degradation patterns into a Gaussian one and generates latent fingerprints based on the sampled degradation patterns and clean fingerprints. Besides, we develop an auxiliary training procedure to stabilize training and further disentangle ridge structures and degradation patterns by regressing a latent fingerprint from its latent representation and its corresponding binarized fingerprint. Boosted by the proposed data augmentation, our reconstruction shows significant improvements in visual evaluation and fingerprint identification performance.},
keywords={latent fingerprint;image reconstruction;data augmentation;generative adversarial network},
doi={10.1109/ICASSP40776.2020.9053801},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053058,
author={J. {Li} and X. {Zhang} and J. {Xu} and L. {Zhang} and Y. {Wang} and S. {Ma} and W. {Gao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning to Fool the Speaker Recognition},
year={2020},
volume={},
number={},
pages={2937-2941},
abstract={Due to the widespread deployment of fingerprint/face/speaker recognition systems, attacking deep learning based biometric systems has drawn more and more attention. Previous research mainly studied the attack to the vision-based system, such as fingerprint and face recognition. While the attack for speaker recognition has not been investigated yet, although it has been widely used in our daily life. In this paper, we attempt to fool the state-of-the-art speaker recognition model and present speaker recognition attacker, a lightweight model to fool the deep speaker recognition model by adding imperceptible perturbations onto the raw speech waveform. We find that the speaker recognition system is also vulnerable to the attack, and we achieve a high success rate on the non-targeted attack. Besides, we also present an effective method to optimize the speaker recognition attacker to obtain a trade-off between the attack success rate with the perceptual quality. Experiments on the TIMIT dataset show that we can achieve a sentence error rate of 99.2% with an average SNR 57.2dB and PESQ 4.2 with speed rather faster than real-time.},
keywords={deep neural network attack;speaker recognition;convolution neural networks;adversarial examples generation},
doi={10.1109/ICASSP40776.2020.9053058},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054115,
author={D. {Peng} and J. {Xiao} and R. {Zhu} and G. {Gao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Ts-Fen: Probing Feature Selection Strategy for Face Anti-Spoofing},
year={2020},
volume={},
number={},
pages={2942-2946},
abstract={Deep features extracted from different domains have shown great advantages in the face anti-spoofing task. Previous extraction strategies consider less of the extent variation in distinction among feature properties. Many of them straightly make classification using the extracted information but generalize weakly. In this paper, we propose a novel Two-Stream Feature Extraction Network (TS-FEN) based on depth and chrominance cues, guiding both sparsity and density of the feature distribution. We specifically design a Feature Enhancement (FE) Structure in the depth stream to strengthen the discrimination capacity, as well as a Feature Selection (FS) Module in the chroma stream to keep feature diversity and distinction. Besides, the specially designed bias arcface loss aims to enlarge the central distribution dispersion of opposite categories. Extensive experiments on three benchmark datasets validate that our proposed approach achieves explicit improvement on both intra-testing and cross-testing.},
keywords={face anti-spoofing;feature enhancement;feature selection;category discrepancy},
doi={10.1109/ICASSP40776.2020.9054115},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053922,
author={A. {Mohammadi} and S. {Bhattacharjee} and S. {Marcel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Cross-Dataset Performance of Face Presentation Attack Detection Systems Using Face Recognition Datasets},
year={2020},
volume={},
number={},
pages={2947-2951},
abstract={Presentation attack detection (PAD) is now considered critically important for any face-recognition (FR) based access-control system. Current deep-learning based PAD systems show excellent performance when they are tested in intra-dataset scenarios. Under cross-dataset evaluation the performance of these PAD systems drops significantly. This lack of generalization is attributed to domain-shift. Here, we propose a novel PAD method that leverages the large variability present in FR datasets to induce invariance to factors that cause domain-shift. Evaluation of the proposed method on several datasets, including datasets collected using mobile devices, shows performance improvements in cross-dataset evaluations.1},
keywords={mobile biometrics;presentation attack detection;cross-dataset evaluation;domain generalization},
doi={10.1109/ICASSP40776.2020.9053922},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053969,
author={X. {Wu} and Z. {Xie} and Y. {Gao} and Y. {Xiao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={SSTNet: Detecting Manipulated Faces Through Spatial, Steganalysis and Temporal Features},
year={2020},
volume={},
number={},
pages={2952-2956},
abstract={Compared to conventional object detection which focuses on high-level image content, face manipulation detection pays more attention to low-level artifacts and temporal discrepancies. However, there are few methods considering both of these two characteristics. In this work, we propose a novel manipulation detection framework, named SSTNet, which detects tampered faces through Spatial, Steganalysis and Temporal features. Spatial features are extracted by a deep neural network for finding visible tampering traces like unnatural color, shape and texture. We propose a constraint on convolutional filters to extract steganalysis features for detecting hidden tampering artifacts like abnormal statistical characteristics of image pixels. Temporal features are extracted by a recurrent network for discovering inconsistency between consecutive frames. Experimental results on Face-Forensics++ dataset demonstrate that SSTNet outperforms other methods and achieves state-of-the-art performance on accuracy and robustness to compression. Furthermore, the generalization capability of SSTNet is verified on the GAN-based DeepFakes dataset.},
keywords={Face manipulation detection;Fake video detection;Image forensics},
doi={10.1109/ICASSP40776.2020.9053969},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054018,
author={B. {Peixoto} and B. {Lavi} and P. {Bestagini} and Z. {Dias} and A. {Rocha}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multimodal Violence Detection in Videos},
year={2020},
volume={},
number={},
pages={2957-2961},
abstract={Effective tools for detection of violence are highly demanded, specially when dealing with video streams. Such tools have a wide range of applications, from forensics and law enforcement to parental control over the ever increasing amount of videos available online. Prior studies showed that deep learning has great potential in detecting violence, but focuses on detecting violence in general, or only specific cases of violent behavior. While the concept of violence is broad and highly subjective, simpler concepts such as fights, explosions, and gunshots, convey the idea of violence while being more objective. Even though different concepts relate to this same broader idea of violence, they differ widely in relation to whether or not they convey the idea of movement, the presence of a specific object, or even if they generate distinctive sounds. In this study, we propose to analyze different concepts related to violence and how to better describe these concepts exploring visual and auditory cues in order to reach a robust method to detect violence.},
keywords={computer vision;violence classification;deeplearning;multimodal classification;forensic computing},
doi={10.1109/ICASSP40776.2020.9054018},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054261,
author={O. {Mayer} and B. {Hosler} and M. C. {Stamm}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Open Set Video Camera Model Verification},
year={2020},
volume={},
number={},
pages={2962-2966},
abstract={We introduce a new open set video forensics problem called video camera model verification. The video camera model verification task is to determine if two query videos were captured by the same camera model. Importantly, verification must be reliable on videos from camera models unknown to the investigator, referred to the as the open set scenario. While researchers have considered other open set problems for digital images, video forensics introduces unique challenges. In this work we propose a new, video-specific system for open set verification of camera models. To do this, we design a deep-learning based system that 1) uses a CNN to extract expressive deep features from video patches, 2) compares pairs of these features using a similarity network, and 3) fuses multiple comparisons to produce a video-level verification decision. We experimentally show that this technique accurately verifies the source camera model of videos in open set scenarios.},
keywords={Multimedia Forensics;Video Forensics;Open Set;Verification;Deep Learning},
doi={10.1109/ICASSP40776.2020.9054261},
ISSN={2379-190X},
month={May},}

('deep learning', 65)
('', 24)
('convolutional neural network', 16)
('convolutional neural networks', 14)
('speech enhancement', 14)
('sound event detection', 13)
('attention mechanism', 11)
('cnn', 10)
('multitask learning', 10)
('data augmentation', 9)
('machine learning', 9)
('deep neural network', 9)
('superresolution', 8)
('deep neural networks', 8)
('classification', 7)
('object detection', 7)
('eeg', 7)
('multiscale', 7)
('music information retrieval', 7)
('segmentation', 7)
('acoustic scene classification', 7)
('depth estimation', 7)
('crowd counting', 7)
('semisupervised learning', 6)
('denoising', 6)
('autoencoder', 6)
('image reconstruction', 6)
('neural networks', 6)
('unet', 6)
('audio tagging', 6)
('generative adversarial networks', 6)
('domain adaptation', 5)
('fewshot learning', 5)
('feature extraction', 5)
('residual learning', 5)
('transfer learning', 5)
('signal processing', 5)
('blind source separation', 5)
('adversarial learning', 5)
('dataset', 5)
('speech separation', 4)
('image restoration', 4)
('recurrent neural networks', 4)
('security', 4)
('video coding', 4)
('source separation', 4)
('reverberation', 4)
('speech', 4)
('spatial attention', 4)
('visual tracking', 4)
('independent vector analysis', 4)
('beamforming', 4)
('adversarial training', 4)
('audio source separation', 4)
('dereverberation', 4)
('metric learning', 4)
('polar codes', 4)
('generative models', 4)
('steganography', 4)
('neural network', 4)
('electroencephalogram (eeg)', 4)
('jpeg', 3)
('sound classification', 3)
('triplet loss', 3)
('lstm', 3)
('sparse representation', 3)
('detection', 3)
('hearing aids', 3)
('5g', 3)
('audio coding', 3)
('nonconvex optimization', 3)
('hashing', 3)
('cover song identification', 3)
('light field', 3)
('epilepsy', 3)
('maximum likelihood', 3)
('selfattention', 3)
('fpga', 3)
('3d convolution', 3)
('hyperspectral imaging', 3)
('unsupervised domain adaptation', 3)
('noise reduction', 3)
('feature selection', 3)
('lightweight network', 3)
('anomaly detection', 3)
('nonnegative matrix factorization', 3)
('action recognition', 3)
('dilated convolution', 3)
('multichannel', 3)
('image classification', 3)
('quantization', 3)
('deep convolutional neural network', 3)
('steganalysis', 3)
('compressive sensing', 3)
('selfsupervised learning', 3)
('optical flow', 3)
('sound event classification', 3)
('image forensics', 3)
('privacy', 3)
('seizure detection', 3)
('retinal vessel segmentation', 3)
('face antispoofing', 3)
('sparsity', 3)
('acoustic event detection', 3)
('unsupervised learning', 2)
('face hallucination', 2)
('asic', 2)
('audio classification', 2)
('dnn', 2)
('singing voice separation', 2)
('recurrent neural network', 2)
('face verification', 2)
('domain generalization', 2)
('convolutional neural network (cnn)', 2)
('realtime', 2)
('fmri', 2)
('adaptation models', 2)
('adversarial examples', 2)
('braincomputer interface (bci)', 2)
('video frame interpolation', 2)
('automatic music transcription', 2)
('fast algorithm', 2)
('discrete wavelet transform', 2)
('human visual system', 2)
('convolution neural networks', 2)
('virtual reality', 2)
('synthetic data', 2)
('audioset', 2)
('parkinson\xe2\x80\x99s disease', 2)
('deep residual network', 2)
('vvc', 2)
('deep convolutional neural networks', 2)
('dictionary learning', 2)
('transformer', 2)
('microphone arrays', 2)
('representation learning', 2)
('sideinformation', 2)
('endtoend', 2)
('person reidentification', 2)
('video saliency', 2)
('attention networks', 2)
('sound localization', 2)
('microphone array', 2)
('crowdsourcing', 2)
('atrial fibrillation', 2)
('feature learning', 2)
('cycleconsistency', 2)
('weakly labelled data', 2)
('data clustering', 2)
('pansharpening', 2)
('realtime speech enhancement', 2)
('receptive field', 2)
('training', 2)
('versatile video coding', 2)
('convolution neural networks (cnn)', 2)
('mri', 2)
('biometrics', 2)
('motion compensation', 2)
('volume reconstruction', 2)
('embedding', 2)
('siamese network', 2)
('hidden markov model', 2)
('brain connectivity', 2)
('independent vector extraction', 2)
('singing voice', 2)
('biomedical imaging', 2)
('speaker verification', 2)
('phase retrieval', 2)
('robustness', 2)
('realtime tracking', 2)
('microphone array processing', 2)
('genetic algorithm', 2)
('image enhancement', 2)
('multiview', 2)
('mapping', 2)
('retrieval', 2)
('3d reconstruction', 2)
('generative adversarial network', 2)
('evaluation', 2)
('reverberation time', 2)
('biomedical image analysis', 2)
('oneshot learning', 2)
('presentation attack detection', 2)
('convex optimization', 2)
('artificial reverberation', 2)
('fundamental frequency', 2)
('sed', 2)
('phishing', 2)
('weakly labeled', 2)
('reinforcement learning', 2)
('computational imaging', 2)
('auxiliary function', 2)
('time domain', 2)
('hardware implementation', 2)
('simd', 2)
('wearable', 2)
('dynamic time warping', 2)
('computational complexity', 2)
('clustering', 2)
('waveunet', 2)
('acoustic impulse response', 2)
('kspace trajectories', 2)
('verification', 2)
('weaklylabeled', 2)
('hevc', 2)
('speech quality', 2)
('photoplethysmogram', 2)
('inverse problem', 2)
('keyword spotting', 2)
('voice activity detection', 2)
('graph signal processing', 2)
('single image super resolution', 2)
('image quality', 2)
('face recognition', 2)
('structurefrommotion', 2)
('singular value decomposition', 2)
('graph convolutional networks', 2)
('hypothesis testing', 2)
('local gaussian modeling', 2)
('adversarial attack', 2)
('compressed sensing', 2)
('semantic segmentation', 2)
('ecg', 2)
('lyrics transcription', 2)
('channel attention', 2)
('acceleration', 2)
('directtoreverberant ratio', 2)
('painting', 2)
('random unitary transform', 2)
('computational modeling', 2)
('image deraining', 2)
('xray imaging', 2)
('multiview learning', 2)
('estimation', 2)
('image segmentation', 2)
('alternating direction method of multipliers', 2)
('endtoend learning', 2)
('contrast enhancement', 2)
('variational inference', 2)
('melody extraction', 2)
('deep supervision', 2)
('hierarchical', 2)
('channel selection', 2)
('bioacoustics', 2)
('sensitivity (se(%))', 1)
('objectspecific change detection', 1)
('tensor factorizations', 1)
('dynamic', 1)
('adversarial discriminators', 1)
('portfolio optimisation', 1)
('local laplacian prior', 1)
('computer audition', 1)
('attack detection', 1)
('meta learning', 1)
('blind image fusion', 1)
('point sets', 1)
('gaussian channels', 1)
('interframe speech correlation', 1)
('subspace detection', 1)
('computer vision', 1)
('temporal sampling', 1)
('spectral modeling synthesis', 1)
('channel state information', 1)
('rare sound event', 1)
('video forensics', 1)
('network quantization', 1)
('realtime signal processing', 1)
('gabor wavelet', 1)
('decoding', 1)
('residual', 1)
('arrhythmia detection', 1)
('statistical modeling', 1)
('noreference', 1)
('weakly supervised sound event detection', 1)
('mobile biometrics', 1)
('pooling', 1)
('structural correlations between images textures', 1)
('artificial ieeg data', 1)
('attractor', 1)
('unscented kalman filter', 1)
('nonlocal', 1)
('active contour model', 1)
('genomics', 1)
('kalman filter', 1)
('jpeg images', 1)
('music', 1)
('factor graph', 1)
('height and weight prediction', 1)
('graph regularization', 1)
('deepbrain stimulation', 1)
('factorized convolution', 1)
('intra prediction', 1)
('fully convolutional network and perceptual quality', 1)
('routine analysis', 1)
('group representation', 1)
('imagebased 3d reconstruction', 1)
('document image shadow removal', 1)
('reweighted \xe2\x84\x931minimization', 1)
('sdrs', 1)
('acoustic matching', 1)
('block coordinate descent method', 1)
('label consistent ksvd', 1)
('low rank approximation', 1)
('surface reconstruction', 1)
('light fields', 1)
('relational reasoning', 1)
('light field microscopy', 1)
('regularity extraction', 1)
('rain mask', 1)
('design space exploration', 1)
('selfattention models', 1)
('saliency prediction', 1)
('spatial features', 1)
('recurrent variational autoencoders', 1)
('audio music similarity', 1)
('cosine similarity', 1)
('light field imaging', 1)
('riemannian manifold', 1)
('number theoretic transform', 1)
('bit flipping', 1)
('adversarial domain adaptation', 1)
('xvector', 1)
('asymmetric autoencoder', 1)
('carnatic music', 1)
('lossless coding', 1)
('sar imaging', 1)
('spectrogram consistency', 1)
('comparative study', 1)
('least squares', 1)
('conditional vae', 1)
('opening range breakout', 1)
('demographic information', 1)
('atherosclerosis', 1)
('fastica', 1)
('interactive video streaming', 1)
('source coding', 1)
('singlechannel audio separation', 1)
('onlinetobatch conversion', 1)
('proteinprotein interactions', 1)
('sparse coding', 1)
('acoustic', 1)
('interpolation', 1)
('lie groups', 1)
('image superresolution', 1)
('acoustic signal detection', 1)
('energy control', 1)
('long shortterm memory (lstm)', 1)
('homography estimation', 1)
('recurrent neural networks (rnn)', 1)
('super resolution', 1)
('temporal convolutions', 1)
('clotho', 1)
('key action extraction', 1)
('gradient', 1)
('descriptive transcription', 1)
('social media data', 1)
('fake news detection', 1)
('density regression', 1)
('proposal generation', 1)
('online semisupervised unmixing', 1)
('noisy image deblurring', 1)
('sensory', 1)
('hsi denoising', 1)
('timefrequency representation', 1)
('deep clustering', 1)
('classspecific', 1)
('generalized coherence', 1)
('lcmv filter', 1)
('transferability', 1)
('puremic', 1)
('face parsing map', 1)
('stochastic optimization', 1)
('multiresolution', 1)
('active contour', 1)
('autonomous vehicles', 1)
('perceptual video coding', 1)
('vqa', 1)
('ri spectrograms', 1)
('basketball', 1)
('timeevolving data', 1)
('weaklysupervised learning', 1)
('pose estimation', 1)
('complexity reduction', 1)
('image fusion', 1)
('affective computing', 1)
('blind acoustic parameter estimation', 1)
('subspace learning', 1)
('transient oscillation', 1)
('privacypreserving image sharing', 1)
('siamese neural network', 1)
('blind denoising network', 1)
('distance', 1)
('sparse prior', 1)
('line spectral process', 1)
('alternating minimization', 1)
('multiorientation', 1)
('unsupervised segmentation', 1)
('expectation propagation algorithm (epa)', 1)
('multiarmed bandit', 1)
('audio systems', 1)
('modulation layer', 1)
('adhd', 1)
('embedded approaches', 1)
('graph convolutional network', 1)
('polyphonic sound detection score', 1)
('hyperspectral image', 1)
('doubleweighted factor', 1)
('channelwise interdependencies', 1)
('adaptive elastic loss', 1)
('natural language processing', 1)
('multiscale feature aggregation', 1)
('individualization', 1)
('feature aggregation', 1)
('conditional generative adversarial networks', 1)
('image retrieval', 1)
('multichannel sound reproduction', 1)
('visual odometry', 1)
('driving behaviors reasoning', 1)
('metrics', 1)
('topological data analysis', 1)
('auditory salience', 1)
('finegrained recognition', 1)
('image decomposition', 1)
('doa', 1)
('audio databases', 1)
('determined speech source separation', 1)
('medical biometrics', 1)
('central projection', 1)
('functional connectivity', 1)
('parametric style transfer', 1)
('spatial aliasing', 1)
('missingannotation scenarios', 1)
('matching pursuit', 1)
('empirical mode decomposition', 1)
('gesture recognition', 1)
('saliency', 1)
('skeletonlike relation', 1)
('dodcnn', 1)
('compare learning', 1)
('variablerate dataflow', 1)
('dnn accelerator', 1)
('image separation with side information', 1)
('soundscape analysis', 1)
('diffuse field measurements', 1)
('hyperspectral image compression', 1)
('graph theory', 1)
('total variation regularization', 1)
('common spatial patterns (csp)', 1)
('pulmonary textures classification', 1)
('orthogonal frequencydivision multiplexing (ofdm)', 1)
('patch', 1)
('unseen presentation attack detection', 1)
('spectral unmixing', 1)
('classification algorithms', 1)
('retinal images', 1)
('auditory filterbank', 1)
('multimodal analysis', 1)
('microphone calibration', 1)
('pixellevel', 1)
('scattering', 1)
('video deduplication', 1)
('image clustering', 1)
('unet network', 1)
('image timeseries', 1)
('dynamic stream weights', 1)
('residue refinement', 1)
('multiple sclerosis lesion', 1)
('rgbd dataset', 1)
('image registration', 1)
('wind speed', 1)
('stacked sparse autoencoder', 1)
('generative adversarial network (gan)', 1)
('urine sediment examination', 1)
('onsetoffset detection', 1)
('flexible tunability', 1)
('convergent cross mapping', 1)
('old master paintings', 1)
('nearcache processing', 1)
('multitask cnn', 1)
('fall detection', 1)
('video representation', 1)
('scattering transform', 1)
('banding artifact', 1)
('pseudolabelling', 1)
('quality scaling factor', 1)
('object monitoring', 1)
('multilabel learning', 1)
('linear estimation', 1)
('swallow sound signal', 1)
('orthogonal features', 1)
('sound field recording', 1)
('magnitude parametric models', 1)
('kronecker product', 1)
('sound event detection (sed)', 1)
('location regression', 1)
('recurrent network', 1)
('violence classification', 1)
('drift detection', 1)
('encoding scheme', 1)
('ambisonics', 1)
('reliable detection', 1)
('presentation attack', 1)
('crowd segmentation', 1)
('video stabilization', 1)
('sequence matching', 1)
('feature maps', 1)
('ynet', 1)
('image reranking', 1)
('latticebased cryptography', 1)
('musical source separation', 1)
('mutual information', 1)
('covariate shift adaptation.', 1)
('stochastic aggregation', 1)
('cooperative adaptive line enhancer', 1)
('wall clutter mitigation', 1)
('shape from bandwidth', 1)
('space filling curves', 1)
('online secondary path modeling', 1)
('system identification', 1)
('broadband signal processing.', 1)
('ctc and lstm', 1)
('multilayer neural network', 1)
('signal representation', 1)
('lasso', 1)
('visualization', 1)
('loss function', 1)
('open set', 1)
('temperature scaling', 1)
('speech coding', 1)
('selfdriving vehicles', 1)
('rgbd multimodality', 1)
('rnn', 1)
('local key estimation', 1)
('triplet mining', 1)
('sound event localization and detection', 1)
('instantaneous frequency spectrogram (ifgram)', 1)
('eye tracking', 1)
('abnormality detection', 1)
('person search', 1)
('occlusion', 1)
('median filtering', 1)
('multiframe', 1)
('ssim', 1)
('sparse', 1)
('prewhitening', 1)
('detail enhancement', 1)
('real time.', 1)
('cybersecurity', 1)
('lifting scheme', 1)
('gradient boosting', 1)
('image phylogeny', 1)
('affine transformation', 1)
('inverse discrete wavelet transformation', 1)
('depression detection', 1)
('ecg delineation', 1)
('linearfrequencymodulated continuouswave (lfmcw) radar', 1)
('transposed cnn (tcnn)', 1)
('nonexpert', 1)
('sliding window', 1)
('burst image denoising', 1)
('manufacturing', 1)
('music synthesis', 1)
('crossdataset evaluation', 1)
('least mean square', 1)
('temporal prediction', 1)
('alpha wave', 1)
('dropblock', 1)
('filter pruning', 1)
('runtime', 1)
('monocular video', 1)
('ppg', 1)
('and multihead selfattention', 1)
('room impulse response', 1)
('distance learning', 1)
('vessel network extraction', 1)
('active visual 3d reconstruction', 1)
('visual saliency', 1)
('bayer cfa', 1)
('s2pt', 1)
('spreadspectrum', 1)
('c50', 1)
('low complexity', 1)
('balance control', 1)
('new radio', 1)
('malicious event classification', 1)
('linkage attack', 1)
('synthesis', 1)
('psychoacoustics', 1)
('estimate resensing', 1)
('restingstate eeg', 1)
('objective metrics', 1)
('parameters estimation', 1)
('differential privacy', 1)
('license plate recognition', 1)
('watermarking', 1)
('bandwidth extension', 1)
('lossless image compression', 1)
('cpnsta model', 1)
('multisite transfer', 1)
('hyperspectral unmixing', 1)
('sparse code multiple access (scma)', 1)
('mean opinion score (mos)', 1)
('laplacerician distribution', 1)
('design automation', 1)
('survival prediction', 1)
('robust pca', 1)
('griffin\xe2\x80\x93lim algorithm', 1)
('adaptive processing', 1)
('raht', 1)
('outofdistribution', 1)
('timereversal (tr)', 1)
('creative interfaces', 1)
('pattern classifier', 1)
('group delay', 1)
('deep metric learning', 1)
('distributed sensor networks', 1)
('statistical surprisal', 1)
('xvectors', 1)
('generalized eigenvalue problem', 1)
('cdr estimation', 1)
('false alarm mitigation', 1)
('dcase2019 challenge', 1)
('surface electromyogram (semg)', 1)
('online adaption', 1)
('designgan', 1)
('mammogram classification', 1)
('impact noise', 1)
('compressive analysis', 1)
('variable bitrates', 1)
('multilabel classification', 1)
('macro xray fluorescence scanning', 1)
('training strategy', 1)
('network compression', 1)
('drug discovery', 1)
('adversarial attacks', 1)
('modified zff', 1)
('hierarchical hourglass network', 1)
('spherical arrays', 1)
('distance geometry', 1)
('artifactrelated perceptual score', 1)
('object recognition', 1)
('data assimilation', 1)
('ghost imaging', 1)
('vector autoregressive model', 1)
('prediction', 1)
('scanning laser ophthalmoscopy (slo)', 1)
('color stabilization', 1)
('autoencoders', 1)
('hotspot segmentation', 1)
('multimodal retrieval', 1)
('multilevel deep sequences', 1)
('automatic target recognition', 1)
('clarinet tone quality', 1)
('amplitude modulation', 1)
('reproducing kernel', 1)
('plugandplay', 1)
('reducedrank', 1)
('multichannel naec', 1)
('fast fourier transform (fft)', 1)
('active occlusion recognition', 1)
('learned image compression', 1)
('medical diagnostic imaging', 1)
('and spatial correlation', 1)
('no multiplication transforms', 1)
('knearest neighbors', 1)
('prototypical network', 1)
('fmri data and \xce\xb1\xe2\x88\x92divergence', 1)
('nonconvex penalty', 1)
('adaptive convolution', 1)
('voice conversion', 1)
('continuous wavelet transforms', 1)
('packet capture', 1)
('rotationinvariant features', 1)
('subjective quality estimation of handsfree terminals', 1)
('coordinate descent algorithm', 1)
('phonocardiogram', 1)
('deep matrix completion', 1)
('monaural', 1)
('residual networks', 1)
('fetal heart rate', 1)
('computational auditory scene analysis', 1)
('lung tumor.', 1)
('shiftinvariant harmonic phase', 1)
('conditional gans', 1)
('parallel gans', 1)
('nearest kronecker product', 1)
('narrowband noise', 1)
('fairness in computer vision', 1)
('binaural rendering', 1)
('mobile iris biometrics', 1)
('privacy aware audio synthesis', 1)
('instrument labelling', 1)
('cocktail party problem', 1)
('rank tracking.', 1)
('dynamic scene deblurring', 1)
('tf loss', 1)
('lowrank', 1)
('early olfactory system', 1)
('qim based steganography', 1)
('distillation', 1)
('image dehazing', 1)
('soundfield navigation', 1)
('automatic speech recognition', 1)
('viewangle invariant', 1)
('regularization', 1)
('deep neural network (dnn)driven feature learning method', 1)
('blood pressure', 1)
('weighted least squares', 1)
('coded audio enhancement', 1)
('inception nucleus', 1)
('ultrasound neuromodulation', 1)
('multichannel speech enhancement.', 1)
('affordance', 1)
('downsampling', 1)
('airlight estimation', 1)
('fewshot', 1)
('spatial temporal transformer', 1)
('just noticeable difference (jnd)', 1)
('level set', 1)
('neural architecture search', 1)
('audio event detection', 1)
('pitch', 1)
('multipath', 1)
('exposure fusion', 1)
('inverse problems', 1)
('constantq transform', 1)
('phase amplitude coupling', 1)
('annotation fusion', 1)
('bitplane', 1)
('electromagnetic emanations', 1)
('target tracking', 1)
('sound zone control', 1)
('timedomain network', 1)
('object segmentation', 1)
('biattention', 1)
('nighttime parking lots', 1)
('parallel processing', 1)
('transformer model', 1)
('od and oc segmentation', 1)
('hrtf', 1)
('and multihead attention', 1)
('source counting', 1)
('bidirectional propagation', 1)
('environmental audio processing', 1)
('paroxysmal', 1)
('blockbased inference', 1)
('multivariate empirical mode decomposition', 1)
('modality translation', 1)
('scflip decoding', 1)
('divergence', 1)
('eventbased vision', 1)
('selfsupervised', 1)
('state space reconstruction', 1)
('forgery detection', 1)
('cough', 1)
('orb', 1)
('mimo', 1)
('psnr', 1)
('rotated filter', 1)
('programming', 1)
('frequencydomain blind source separation', 1)
('convolutional sparse coding', 1)
('wind noise', 1)
('reweighting technique', 1)
('coloured noise', 1)
('perceptual encryption', 1)
('adaptive motion vector resolution', 1)
('content disentangling', 1)
('selfattention mechanism', 1)
('image processing', 1)
('heterogeneous computing', 1)
('nonnegative autoencoder', 1)
('multiscale dilated convolutions', 1)
('singular spectrum analysis', 1)
('common spatial pattern (csp)', 1)
('mixing matrix', 1)
('keypoint detection', 1)
('adaptive resolution change', 1)
('depth from defocus', 1)
('dataset collection', 1)
('pixelwise loss', 1)
('autovc', 1)
('deanonymization', 1)
('hardware oriented dcnn', 1)
('interpretable learning', 1)
('multidimensional music similarity', 1)
('hearing aid signal processing', 1)
('anc', 1)
('sound texture', 1)
('cascaded classifier', 1)
('data segmentation', 1)
('colour coding', 1)
('selective control', 1)
('discontinuity preservation', 1)
('wiener filter', 1)
('3d tomography', 1)
('sleepdisordered breathing', 1)
('drugtarget interaction prediction', 1)
('fundus image', 1)
('sure', 1)
('persistent entropy', 1)
('missing values', 1)
('binary hypothesis', 1)
('gallium nitride', 1)
('particle filtering', 1)
('overdetermined', 1)
('underdetermined speech source separation', 1)
('student\xe2\x80\x99s t distribution', 1)
('scene text detection', 1)
('audiovisual correspondence', 1)
('multispectral', 1)
('multisubject fmri data', 1)
('multispectral image compression', 1)
('low resolution', 1)
('sketch recognition', 1)
('convolutional networks', 1)
('adhd classification', 1)
('cartoontexture decomposition', 1)
('texture', 1)
('physical downlink control channel', 1)
('marked point process', 1)
('multiscale fusion', 1)
('model complexity', 1)
('vehicle reidentification', 1)
('qdct domain.', 1)
('aesthetics assessment', 1)
('center of pressure', 1)
('field programmable gate array (fpga)', 1)
('tone quality evaluation', 1)
('qcldpc', 1)
('timevarying channel', 1)
('acoustic system identification', 1)
('frame theory', 1)
('scenebased virtual reality', 1)
('distortion map', 1)
('kernel', 1)
('deconvolution', 1)
('crossdomain adaptation', 1)
('scenedependent acoustic event detection', 1)
('printable graphical codes', 1)
('acoustic model', 1)
('wideband', 1)
('throughthewall radar imaging', 1)
('electroencephalogram.', 1)
('geometry calibration', 1)
('speaker identification', 1)
('spatial covariance model', 1)
('block toeplitz', 1)
('crnns', 1)
('twostage segmentation', 1)
('energyefficiency', 1)
('iodcnn', 1)
('nilm', 1)
('equiribriated recurrent neural network', 1)
('crosssimilarity matrices', 1)
('endtoend speech source separation', 1)
('adversarial perturbation', 1)
('sampling', 1)
('and timefrequency mask', 1)
('performance analysis', 1)
('compact convolutional neural network', 1)
('sign language recognition', 1)
('matrix lie group', 1)
('keyframe scheduling', 1)
('poseinvariant features', 1)
('feature fusion', 1)
('higher order ambisonics', 1)
('playing technique recognition', 1)
('latent fingerprint', 1)
('cgan', 1)
('topology inference', 1)
('auxiliary information', 1)
('timefrequency analysis', 1)
('operator chains', 1)
('index tracking', 1)
('localancestry inference', 1)
('multitarget tracking', 1)
('geometry constrained loss', 1)
('nonstationary', 1)
('binaural', 1)
('reflectance', 1)
('speech distortion', 1)
('long shortterm memory network', 1)
('dynamic vision sensor', 1)
('temporal context', 1)
('pruning', 1)
('parametric decomposition', 1)
('data privacy.', 1)
('ultrahighdefinition', 1)
('video classification', 1)
('video quality predictor', 1)
('multiframe filtering', 1)
('drowsiness', 1)
('360 video coding', 1)
('panda identification', 1)
('multichannel linear prediction', 1)
('weakly labeled data', 1)
('expression recognition', 1)
('crossmodal', 1)
('adaptive detection', 1)
('direction of arrival', 1)
('attention unet', 1)
('region proposal network (rpn)', 1)
('bitmap', 1)
('network encoder', 1)
('instancelevel', 1)
('anchor box', 1)
('backprop kalman filter', 1)
('singleshot', 1)
('routing selection', 1)
('binary hash', 1)
('humanobject interaction', 1)
('residual hiding', 1)
('image colorization', 1)
('decomposed', 1)
('dialogue enhancement', 1)
('complex spectral mapping', 1)
('global structure graph', 1)
('mixup data augmentation', 1)
('learned compression', 1)
('joint training', 1)
('data hiding', 1)
('analysisbysynthesis', 1)
('speechtosinging transformation', 1)
('multiaperture', 1)
('hourglass networks', 1)
('jnd transform', 1)
('legendre polynomial', 1)
('crossmodal attention', 1)
('butterfly network', 1)
('genetics', 1)
('online portfolio selection', 1)
('edge detection and classification', 1)
('sequence representation', 1)
('hierarchical structure', 1)
('weak labels', 1)
('conversational tests', 1)
('iris segmentation', 1)
('egocentric', 1)
('texture editing', 1)
('deblurring implementation', 1)
('multipolarization imaging', 1)
('terahertz', 1)
('dataflow processing', 1)
('compact representations', 1)
('acoustic condition monitoring', 1)
('image compression', 1)
('maximum likelihood and \xce\xb1\xe2\x88\x92divergence', 1)
('feature refinement', 1)
('video codec', 1)
('graph embedding', 1)
('context aggregation', 1)
('communication', 1)
('audio analysis software', 1)
('subspace estimation', 1)
('matched filter', 1)
('empirical wavelet transform', 1)
('soundscape', 1)
('kernel prediction networks', 1)
('brain computer interface (bci)', 1)
('finegrained action recognition', 1)
('uplink mimo', 1)
('astronomical imaging', 1)
('music structure', 1)
('endtoend asr', 1)
('deepfilter', 1)
('acoustic feedback', 1)
('manycore architecture', 1)
('modulation perchannel energy normalization', 1)
('3d object reconstruction', 1)
('visual attention', 1)
('demosaicing', 1)
('crossmodal retrieval', 1)
('face aging', 1)
('video deblurring', 1)
('multiregion', 1)
('diffraction', 1)
('perceptual evaluation of speech quality (pesq)', 1)
('underwater acoustic signal', 1)
('energy disaggregation', 1)
('deep distance metric learning', 1)
('differential beamforming', 1)
('sheet music', 1)
('detail transfer', 1)
('bitflipping', 1)
('object localisation', 1)
('wave simulation', 1)
('percussive sound synthesis', 1)
('enhanced interference rejection combining (eirc)', 1)
('drift', 1)
('adaptive average pooling', 1)
('music embedding', 1)
('phasebased processing', 1)
('secret key agreement', 1)
('sound source localization', 1)
('parallel programming', 1)
('classical music', 1)
('variational autoencoder', 1)
('wifi', 1)
('realtime hand gesture recognition', 1)
('supervised learning', 1)
('deep structure', 1)
('hypergraph learning', 1)
('analytical models', 1)
('crossmodal distillation', 1)
('network evolution', 1)
('semisupervised duration estimation', 1)
('fashion landmark estimation', 1)
('complex image noises', 1)
('low power graphics processing units', 1)
('temporal correlation', 1)
('multiview visualization', 1)
('category discrepancy', 1)
('connected tube model', 1)
('audio recognition', 1)
('parafac2', 1)
('semisupervised classification', 1)
('cnnlstm', 1)
('data fusion', 1)
('l1norm scaleinvariant loss function', 1)
('artificial audio bandwidth extension', 1)
('directional constraint', 1)
('lidar', 1)
('intelligibility', 1)
('periodicity', 1)
('speech quality and intelligibility', 1)
('deghosting', 1)
('thirdparty tests', 1)
('photoplethysmography (ppg)', 1)
('video analysis', 1)
('probability distribution modelling', 1)
('3d reconstruction and rendering', 1)
('phaseless inverse scattering', 1)
('image derainning', 1)
('subspaces', 1)
('weight pruning', 1)
('medical image segmentation', 1)
('orthogonal regression', 1)
('successive cancellation list decoding', 1)
('point clouds', 1)
('search', 1)
('temporal squeeze pooling', 1)
('underwater image analysis', 1)
('siamese', 1)
('singlesource frame/bin detector.', 1)
('permutation', 1)
('audiovisual speaker tracking', 1)
('inpaintingbased compression', 1)
('visnir translation', 1)
('parameterized structured pruning', 1)
('scale invariance', 1)
('multispectral filter array', 1)
('crossview image generation', 1)
('spectrogram', 1)
('reversible data hiding (rdh)', 1)
('superpixels', 1)
('style transfer mapping', 1)
('total generalized variation', 1)
('exceptional motion detection', 1)
('braincomputer interfaces', 1)
('internet of satellites', 1)
('finite rate of innovation', 1)
('imaging through scattering media', 1)
('largescale', 1)
('screen content coding', 1)
('synchronous dataflow', 1)
('multistage network', 1)
('distant speech recognition', 1)
('hrtf selection', 1)
('auxiliary function approach', 1)
('digital filters', 1)
('spectrum correction', 1)
('parametric equalization', 1)
('smartphone', 1)
('dimension estimation', 1)
('earpiece', 1)
('csi', 1)
('information theoretic security', 1)
('music autotagging', 1)
('heading direction', 1)
('channelattention', 1)
('noninvasive', 1)
('mutual dependency analysis', 1)
('linear timeinvariant systems', 1)
('multiple instance learning', 1)
('impulse response', 1)
('voice analysis', 1)
('connectivity', 1)
('spatialspectral', 1)
('video restoration', 1)
('sparse fir', 1)
('side information', 1)
('remote sensing', 1)
('tasnet', 1)
('unitary constraint', 1)
('correlated residual blocks', 1)
('head detection', 1)
('diabetes mellitus', 1)
('recurrent learning', 1)
('temporal muscle activation maps', 1)
('essential proteins', 1)
('action classification', 1)
('datadependent', 1)
('hyperspectral image deconvolution', 1)
('machine health monitoring', 1)
('financial nlp', 1)
('multichannel surface electromyography', 1)
('directionofarrival estimation', 1)
('point cloud superresolution', 1)
('rare event detection', 1)
('autism spectrum disorder', 1)
('uhd', 1)
('visual search', 1)
('speaker recognition', 1)
('speaker recognition systems', 1)
('image coding', 1)
('fourier transform spectroscopy', 1)
('music perception', 1)
('noisy label', 1)
('fragile', 1)
('similarity learning', 1)
('systolic array', 1)
('personalization', 1)
('common carotid artery', 1)
('image deblocking', 1)
('image obfuscation', 1)
('spca', 1)
('asr', 1)
('positron emission tomography', 1)
('gaussian background', 1)
('localization', 1)
('vehicle engine noise classification', 1)
('talking face from speech', 1)
('musculoskeletal radiographs', 1)
('audio effects modeling', 1)
('sea clutter modelling', 1)
('context guided', 1)
('edge detection', 1)
('fisher vector', 1)
('multiple orthogonal least squares', 1)
('human action recognition', 1)
('lowlight image enhancement', 1)
('complex steerable pyramid', 1)
('speech decoding', 1)
('single microphone', 1)
('highquality density map', 1)
('detection algorithms', 1)
('adversarial images', 1)
('multiactivation structure', 1)
('audio captioning', 1)
('macropixel', 1)
('diffusion', 1)
('mm algorithm', 1)
('missing labels', 1)
('spatiotemporal relevance', 1)
('expressive performance', 1)
('trunc quantizer', 1)
('sound duration', 1)
('edge and cloud', 1)
('discriminative feature', 1)
('timeofflight', 1)
('fully convolutional neural network', 1)
('melfilterbank', 1)
('audio embeddings', 1)
('linear predictive coding', 1)
('lip reading', 1)
('audio embedding.', 1)
('modulation format (mf)', 1)
('acoustic event classification and detection', 1)
('adaptive algorithm', 1)
('hvs response model', 1)
('unsupervised', 1)
('mobile', 1)
('face superresolution', 1)
('enhanced nonlocal cascading network', 1)
('adaboost', 1)
('computational sound scene analysis', 1)
('equirectangular projection', 1)
('zeroshot learning', 1)
('brain connectivity estimation', 1)
('channelwise attention', 1)
('computability', 1)
('hybrid structural sparse error', 1)
('music emotion recognition', 1)
('generators', 1)
('leastsquares', 1)
('active noise cancellation', 1)
('voice over ip', 1)
('pointsource model', 1)
('face manipulation detection', 1)
('hiv', 1)
('model compression', 1)
('spatiotemporal attention', 1)
('uterine activity', 1)
('finite element analysis', 1)
('constrained kalman filter', 1)
('document image processing', 1)
('nuclei segmentation', 1)
('harmonic filters', 1)
('timefrequency.', 1)
('multimodal classification', 1)
('staged training strategy', 1)
('art', 1)
('salient object window', 1)
('admm', 1)
('selfpaced learning', 1)
('siamese networks', 1)
('ctc training', 1)
('multimedia forensics', 1)
('audio representation learning', 1)
('expert', 1)
('cervical auscultation', 1)
('feature representation', 1)
('hardware architecture', 1)
('detection and classification of acoustic scenes and events (dcase)', 1)
('spherical microphone arrays', 1)
('spoofing', 1)
('multibranch learning', 1)
('attention', 1)
('speech technology', 1)
('mvdr', 1)
('acoustic beamforming', 1)
('lung cancer diagnosis', 1)
('characterlevel', 1)
('rgbd recognition', 1)
('epi neighborhood distribution', 1)
('homogeneous diffusion', 1)
('point', 1)
('convolution neural network', 1)
('signal recovery', 1)
('encoderdecoder', 1)
('wavelet shrinkage', 1)
('kernel density estimate', 1)
('geometric constraints', 1)
('object tracking', 1)
('3dof+', 1)
('sparse bayesian learning', 1)
('spectralspatial prior', 1)
('histology image', 1)
('synthetic aperture radar (sar)', 1)
('deblurring', 1)
('nonlinear filtering', 1)
('iris localization', 1)
('intrusive', 1)
('meeting', 1)
('sequence model', 1)
('error analysis', 1)
('headrelated transfer function', 1)
('gait analysis', 1)
('just noticeable distortion', 1)
('3d dynamic face recognition', 1)
('joint approximate diagonalization (jad)', 1)
('auditory model', 1)
('uncertainty quantification', 1)
('multichannel signal', 1)
('keyword detection', 1)
('visual masking', 1)
('dcgan', 1)
('dilated convolutional recurrent neural network', 1)
('local correlation block', 1)
('acoustic modeling', 1)
('clonability attacks', 1)
('humanmachine collaboration', 1)
('eating environment', 1)
('feature pyramid network', 1)
('temporal action detection', 1)
('active learning', 1)
('rate adaptation', 1)
('sourcefilter model', 1)
('knn', 1)
('bmode ultrasound', 1)
('ivectors', 1)
('pattern recognition', 1)
('homogeneous motion', 1)
('depthwise separable convolution', 1)
('consistency', 1)
('holographic phase retrieval', 1)
('coronary artery disease', 1)
('equalization matching', 1)
('spectrogram inversion', 1)
('lda', 1)
('cuboid', 1)
('granger causality', 1)
('lds', 1)
('shape', 1)
('compressed alignment', 1)
('ontology layers', 1)
('lyrics alignment', 1)
('array processing', 1)
('sound recognition', 1)
('temporal reasoning', 1)
('structural sparse representation', 1)
('spike overlap', 1)
('aggregation', 1)
('alternating optimization', 1)
('phylogeny', 1)
('reinke\xe2\x80\x99s edema', 1)
('polynomial matrix eigenvalue decomposition', 1)
('mitbih arrhythmia database', 1)
('musical audio processing', 1)
('interrater agreement', 1)
('music genre', 1)
('load sequences', 1)
('signer identification', 1)
('cost function', 1)
('semantic audio representations', 1)
('sinkhorn distance', 1)
('biomechanical measures', 1)
('timepredictability', 1)
('speaker extraction', 1)
('stochastic defense', 1)
('adaptive filter', 1)
('weaklysupervised segmentation', 1)
('independent lowrank matrix analysis', 1)
('saliency map', 1)
('electroencephalogram', 1)
('feature autoweighting', 1)
('deep unrolling', 1)
('convolutional neural networks (cnn)', 1)
('measurements', 1)
('overdetermined source separation', 1)
('dual aperture camera', 1)
('sequential learning', 1)
('transparency.', 1)
('eye fundus', 1)
('online learning', 1)
('instrument classification', 1)
('visual music transcription', 1)
('dcase 2017 challenge', 1)
('image sharpening', 1)
('glaucoma', 1)
('anchor information', 1)
('sift points', 1)
('snorer diarisation', 1)
('material segmentation', 1)
('target speech extraction', 1)
('progression rate', 1)
('feature disentanglement', 1)
('drnet', 1)
('whole slide images', 1)
('speech synthesis', 1)
('turing machines', 1)
('vbscript', 1)
('explicit content detection', 1)
('fast', 1)
('transmission matrix calibration', 1)
('health sensing', 1)
('spatiotemporal filter.', 1)
('fisheye camera', 1)
('time series analysis and forecasting', 1)
('mri reconstruction', 1)
('multilevel', 1)
('background noise class', 1)
('joint analysis', 1)
('movement imagination and execution', 1)
('convolutional neural net', 1)
('color matching', 1)
('vanishing/exploding gradient problem', 1)
('dynamic programming', 1)
('different ct scanners', 1)
('auditory system', 1)
('camera postprocessing', 1)
('facial image', 1)
('sensor placement', 1)
('popularity', 1)
('forwardbackward algorithm', 1)
('colour image compression', 1)
('auditory attention', 1)
('switched halfsample interpolation filter', 1)
('geometric posetransition feature.', 1)
('probability map', 1)
('cyber security', 1)
('optical aberration', 1)
('intrusion detection', 1)
('autotuning', 1)
('pushbroom imager', 1)
('robust features', 1)
('auralization', 1)
('multitask loss', 1)
('s1\xe2\x80\x93s2 detection', 1)
('neural vocoders', 1)
('lie algebra', 1)
('twostream model', 1)
('single view', 1)
('fluorescence', 1)
('pooling functions', 1)
('svd', 1)
('multiobjective optimization', 1)
('3d shape reconstruction', 1)
('3d fully convolutional neural network', 1)
('depth', 1)
('panoramic videos', 1)
('lung cancer', 1)
('feedback path', 1)
('gans', 1)
('active noise control', 1)
('shvc', 1)
('belief propagation', 1)
('sentiment analysis', 1)
('graph learning', 1)
('symbollevel interference cancellation (slic)', 1)
('adversarial robustness', 1)
('uncoupled operations', 1)
('jpeg compression', 1)
('feature decomposition', 1)
('sleep spindles', 1)
('occlusion detection', 1)
('neonatal sepsis', 1)
('flow fields', 1)
('brain decoding', 1)
('psds', 1)
('information security', 1)
('temporal attention', 1)
('manifold optimization', 1)
('alternating guidance', 1)
('anthropometric parameters', 1)
('auditory object recognition', 1)
('soft attention', 1)
('video compression', 1)
('cnns', 1)
('qrs complex detection', 1)
('video', 1)
('kullbackleibler divergence', 1)
('realtime implementation', 1)
('sleep quality evaluation', 1)
('hyperspectral images', 1)
('domaininvariant feature', 1)
('closing strategy', 1)
('proximal averaged', 1)
('binary tree', 1)
('user context', 1)
('softbayes', 1)
('hyperprior', 1)
('semantic information preserved loss', 1)
('cyberphysical system', 1)
('separation', 1)
('querybyexample', 1)
('zero frequency filter', 1)
('physiology', 1)
('homomorphic encryption', 1)
('knowledge distillation', 1)
('driver monitoring system', 1)
('skeletal joint prediction', 1)
('universal adversarial attack', 1)
('electromagnetic sidechannel', 1)
('the doublestimulus impairment scale (dsis) method', 1)
('biometric identification', 1)
('stabilization', 1)
('speech recognition', 1)
('disentangled representation', 1)
('multichannel signal enhancement', 1)
('multitask', 1)
('least squares regression', 1)
('logmel spectrograms', 1)
('physical principle', 1)
('early termination', 1)
('video question generation', 1)
('audio steganography', 1)
('motor imagery', 1)
('temporal pooling', 1)
('finegrained vehicle recognition', 1)
('ecog', 1)
('eventdriven', 1)
('stagewise learning', 1)
('spherical video coding', 1)
('deep neural network embeddings', 1)
('gaussian processes', 1)
('acoustic segment model', 1)
('singlemicrophone speech enhancement', 1)
('word embedding', 1)
('tucker decomposition', 1)
('biomimetic', 1)
('data generation', 1)
('generative model', 1)
('nonconvex stochastic optimization', 1)
('3d deformation signature', 1)
('progressive interclass association', 1)
('audio event classification', 1)
('iterative deblurring framework', 1)
('prognosis', 1)
('ontology structure', 1)
('video quality', 1)
('saliency analysis', 1)
('fourier accumulation', 1)
('series photo selection', 1)
('binaural doa estimation', 1)
('positive predictivity (+pr)', 1)
('covariance matrix', 1)
('speech dereverberation', 1)
('soft labels', 1)
('patch detector', 1)
('noisy minimum spanning tree', 1)
('nystr\xc3\xb6m approximation', 1)
('fashion translation', 1)
('3d hand pose estimation', 1)
('speech animation', 1)
('rgbd object detection', 1)
('machine learning.', 1)
('nonparametric spectral granger causality', 1)
('adaptive bilateral filter', 1)
('crosstalk', 1)
('public speaking anxiety', 1)
('deep learningbased radiomics', 1)
('corcos model', 1)
('background recalibration loss', 1)
('lossy image compression', 1)
('single image dehazing', 1)
('directionofarrival information', 1)
('distraction', 1)
('pixel', 1)
('3d localization', 1)
('graph neural network', 1)
('unpaired data', 1)
('scattering delay network', 1)
('shortest path problem', 1)
('multiple supervisions', 1)
('steadystate visually evoked potentials', 1)
('speech presence probability', 1)
('augmented reality', 1)
('music source separation', 1)
('pairwise activity', 1)
('audio watermark', 1)
('finite impulse response filter', 1)
('ordinal regression', 1)
('dataset protection', 1)
('registration', 1)
('heatedup softmax', 1)
('atrous spatial pyramid pooling', 1)
('surface electromyography (semg)', 1)
('heart sounds', 1)
('preconditioning', 1)
('contour display', 1)
('landmark attention', 1)
('privacy and security', 1)
('automatic diagnosis', 1)
('stereo image superresolution', 1)
('8k', 1)
('lossless compression', 1)
('stackelberg game', 1)
('timedomain audio source separation', 1)
('realtime ecg telemonitoring', 1)
('microlens array camera', 1)
('recursive least mestimate', 1)
('multilevel adversarial network', 1)
('fully convolutional network', 1)
('midi', 1)
('silicon nose', 1)
('scene text recognition', 1)
('federated learning', 1)
('image denoising', 1)
('transmission similarity', 1)
('exocentric', 1)
('semantic information', 1)
('weakly supervision', 1)
('crosssupervised learning', 1)
('frequencydomain adaptive filter', 1)
('blind channel identification', 1)
('independent positive semidefinite tensor analysis', 1)
('3d head pose estimation', 1)
('harmony analysis', 1)
('vae', 1)
('vlsi architecture', 1)
('cell detection', 1)
('hardware accelerators', 1)
('breadthfirst search (bfs)', 1)
('antijamming communication', 1)
('entropy coding', 1)
('stacked hourglass network', 1)
('bayes detection', 1)
('multiple transform selection', 1)
('mean opinion score', 1)
('crnn', 1)
('conditional information', 1)
('false contour', 1)
('backtoback butterfly network', 1)
('viterbi algorithm', 1)
('defense', 1)
('deeplearning', 1)
('fake video detection', 1)
('deepneuralnetwork (dnn)', 1)
('music tagging', 1)
('lead reversal', 1)
('forensic computing', 1)
('daytonight image translation.', 1)
('mil', 1)
('unassigned distance geometry', 1)
('ecoacoustics', 1)
('variational algorithm', 1)
('sliceaware attention model', 1)
('invehicle', 1)
('position constraint loss', 1)
('crossstandard implementation', 1)
('cayley transform', 1)
('telewide camera', 1)
('image rectification', 1)
('unsupervised multiple source localization', 1)
('room acoustics', 1)
('panoramic images', 1)
('harmonic features', 1)
('teacherstudent learning', 1)
('metric loss', 1)
('ondevice speech recognition', 1)
('face', 1)
('physical unclonable function (puf)', 1)
('region of interest (roi) pooling', 1)
('hybrid loss function', 1)
('attention mechanisms', 1)
('atrial fibrillation detection', 1)
('compression artifact', 1)
('variable span tradeoff filter', 1)
('noncontact vital signs detection', 1)
('ecg analysis', 1)
('vigilance task', 1)
('sensor calibration', 1)
('weaklylabeled data', 1)
('group sparse learning', 1)
('video deraining', 1)
('multiperson tracking', 1)
('motor imagery (mi)', 1)
('driver attention', 1)
('speaker interference reduction', 1)
('polyphonic acoustic event detection (aed)', 1)
('hijacking', 1)
('curve text', 1)
('nonlocal selfsimilarity', 1)
('conjugate gradient', 1)
('synthetic aperture radar', 1)
('drift correction', 1)
('spatially varying blur', 1)
('360degreeimages', 1)
('event camera', 1)
('spatial audio', 1)
('embedded', 1)
('persistent homology', 1)
('aed', 1)
('multimodality', 1)
('reversible data hiding', 1)
('redundant information', 1)
('adaptive structure', 1)
('intensity', 1)
('millimeterwave', 1)
('coclustering', 1)
('fourier imaging', 1)
('concentric circular arrays', 1)
('approximate message passing', 1)
('video object detection', 1)
('tree of shapes', 1)
('identification', 1)
('wavelet', 1)
('cervical cancer', 1)
('spatial activity', 1)
('histogram equalization', 1)
('single stream network', 1)
('intra coding', 1)
('adaptive beamforming', 1)
('tensors', 1)
('successive cancellation list', 1)
('optimal camera configuration', 1)
('time series', 1)
('deep neural networks (dnn)', 1)
('residual structure', 1)
('auxiliary function optimization', 1)
('speech intelligibility', 1)
('multiscale aggregation', 1)
('jamesstein estimation', 1)
('boundary cue', 1)
('cosine transform', 1)
('mutlichannel nonlinear system identification', 1)
('optical flow estimation', 1)
('source identification', 1)
('style transfer', 1)
('deep reinforcement learning', 1)
('random body movement (rbm)', 1)
('information fusion', 1)
('diversity', 1)
('public key cryptography', 1)
('instantaneous frequency', 1)
('fractional order', 1)
('statistical model', 1)
('symmetric adaptation consistency', 1)
('key area localization mechanism', 1)
('wax figures', 1)
('cube map', 1)
('biomedical image segmentation', 1)
('mobile health', 1)
('activity recognition', 1)
('action detection', 1)
('dilated depthwise convolution', 1)
('hand object interaction', 1)
('citizen science', 1)
('statistical analysis', 1)
('lightweight', 1)
('relative harmonic coefficients', 1)
('identification theory', 1)
('semantic consistency', 1)
('imagetoimage translation', 1)
('cyclegan', 1)
('information forensics', 1)
('spectral recovery', 1)
('soft edge detector', 1)
('human computer interaction', 1)
('adversarial examples generation', 1)
('automation', 1)
('onset detection', 1)
('adaptive thresholding', 1)
('diarization', 1)
('covariance estimation', 1)
('quadratic polynomial', 1)
('l1 pca', 1)
('malicious crowd dataset', 1)
('spike sorting', 1)
('singleparticle tracking', 1)
('audio', 1)
('deltas', 1)
('metalearning', 1)
('sensor selfnoise', 1)
('hyperlapse', 1)
('calibration of confidence prediction', 1)
('music signal analysis', 1)
('statistical methods', 1)
('gmm', 1)
('confocal imaging', 1)
('urban scene semantic segmentation', 1)
('lte/5g baseband processing', 1)
('anomaly detection in sounds', 1)
('deltadeltas', 1)
('locality sensitive hashing', 1)
('total variation', 1)
('evaluation metrics', 1)
('deep network', 1)
('fisher information', 1)
('hand pose estimation', 1)
('structure similarity', 1)
('multiscaled densenet', 1)
('interpretability', 1)
('human visual model', 1)
('full field of view', 1)
('gaussian mixture model', 1)
('stable systems', 1)
('adaptive weight map', 1)
('attention learning', 1)
('models of computation', 1)
('multimodal models', 1)
('speech bandwidth extension', 1)
('fullscale skip connection', 1)
('nonlinear distortion', 1)
('acoustic analysis', 1)
('triplet embeddings', 1)
('openmic2018', 1)
('music transcription', 1)
('semantic aggregation', 1)
('dynamic networks', 1)
('noise', 1)
('convolutional autoencoders', 1)
('electroencephalograph (eeg)', 1)
('hybrid dilated convolution', 1)
('3d model reconstruction', 1)
('complex ratio masking', 1)
('longrange dependencies', 1)
('text images', 1)
('mixed structural modeling', 1)
('discretization', 1)
('rain streak removal', 1)
('information theoretic (it)', 1)
('frequencyinvariant beamforming', 1)
('speech breathing', 1)
('spf function', 1)
('maximum sinr beamforming', 1)
('multichannel wiener filtering', 1)
('artificial intelligence', 1)
('binaural synthesis', 1)
('pitch control (audio)', 1)
('deep architecture', 1)
('classification.', 1)
('sequencetosequence', 1)
('deep flow collaboration', 1)
('joint diagonalization', 1)
('graph convolution networks', 1)
('timevarying spatial filtering', 1)
('image denoiser benchmarking', 1)
('random scattering media', 1)
('multipathmultirate feature extractor', 1)
('teacher student models', 1)
('saec', 1)
('t60', 1)
('spectral feature inversion', 1)
('multimodal', 1)
('multitaper spectral estimation', 1)
('autism', 1)
('design simulator', 1)
('nonparametric regression', 1)
('coupled operations', 1)
('feature enhancement', 1)
('cnn accerleration', 1)
('deep neural network attack', 1)
('branch', 1)
('mental fatigue', 1)
('eeg feature selection', 1)
('multi task learning', 1)
('molecular featurization', 1)
('enhancementbysynthesis', 1)
('bci', 1)
('photonics', 1)
('reweighted procedure', 1)
('nonlinear forward model', 1)
('automatic pitch correction', 1)
('encoderdecoder network', 1)
('deep neural networks (dnns)', 1)