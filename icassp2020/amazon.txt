@INPROCEEDINGS{9053336,
author={B. {Shi} and M. {Sun} and K. C. {Puvvada} and C. {Kao} and S. {Matsoukas} and C. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Few-Shot Acoustic Event Detection Via Meta Learning},
year={2020},
volume={},
number={},
pages={76-80},
abstract={We study few-shot acoustic event detection (AED) in this paper. Few-shot learning enables detection of new events with very limited labeled data. Compared to other research areas like computer vision, few-shot learning for audio recognition has been under-studied. We formulate few-shot AED problem and explore different ways of utilizing traditional supervised methods for this setting as well as a variety of meta-learning approaches, which are conventionally used to solve few-shot classification problem. Compared to supervised baselines, meta-learning models achieve superior performance, thus showing its effectiveness on generalization to new audio events. Our analysis including impact of initialization and domain discrepancy further validate the advantage of meta-learning approaches in few-shot AED.},
keywords={Acoustic event detection;few-shot learning;meta learning},
doi={10.1109/ICASSP40776.2020.9053336},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053150,
author={C. {Kao} and M. {Sun} and W. {Wang} and C. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Comparison of Pooling Methods on LSTM Models for Rare Acoustic Event Classification},
year={2020},
volume={},
number={},
pages={316-320},
abstract={Acoustic event classification (AEC) and acoustic event detection (AED) refer to the task of detecting whether specific target events occur in audios. As long short-term memory (LSTM) leads to state-of-the-art results in various speech related tasks, it is employed as a popular solution for AEC as well. This paper focuses on investigating the dynamics of LSTM model on AEC tasks. It includes a detailed analysis on LSTM memory retaining, and a benchmarking of nine different pooling methods on LSTM models using 1.7M generated mixture clips of multiple events with different signal-to-noise ratios. This paper focuses on understanding: 1) utterance-level classification accuracy; 2) sensitivity to event position within an utterance. The analysis is done on the dataset for the detection of rare sound events from DCASE 2017 Challenge. We find max pooling on the prediction level to perform the best among the nine pooling approaches in terms of classification accuracy and insensitivity to event position within an utterance. To authors’ best knowledge, this is the first kind of such work focused on LSTM dynamics for AEC tasks.},
keywords={Long short-term memory (LSTM);acoustic event classification and detection;pooling functions},
doi={10.1109/ICASSP40776.2020.9053150},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052990,
author={K. {Drossos} and S. {Lipping} and T. {Virtanen}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Clotho: an Audio Captioning Dataset},
year={2020},
volume={},
number={},
pages={736-740},
abstract={Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online 1.},
keywords={audio captioning;dataset;Clotho},
doi={10.1109/ICASSP40776.2020.9052990},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053989,
author={B. {Tolooshams} and R. {Giri} and A. H. {Song} and U. {Isik} and A. {Krishnaswamy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Channel-Attention Dense U-Net for Multichannel Speech Enhancement},
year={2020},
volume={},
number={},
pages={836-840},
abstract={Supervised deep learning has gained significant attention for speech enhancement recently. The state-of-the-art deep learning methods perform the task by learning a ratio/binary mask that is applied to the mixture in the time-frequency domain to produce the clean speech. Despite the great performance in the single-channel setting, these frameworks lag in performance in the multichannel setting as the majority of these methods a) fail to exploit the available spatial information fully, and b) still treat the deep architecture as a black box which may not be well-suited for multichannel audio processing. This paper addresses these drawbacks, a) by utilizing complex ratio masking instead of masking on the magnitude of the spectrogram, and more importantly, b) by introducing a channel-attention mechanism inside the deep architecture to mimic beamforming. We propose Channel-Attention Dense U-Net, in which we apply the channel-attention unit recursively on feature maps at every layer of the network, enabling the network to perform non-linear beamforming. We demonstrate the superior performance of the network against the state-of-the-art approaches on the CHiME-3 dataset.},
keywords={Channel-Attention;U-Net;Complex Ratio Masking;Multichannel Speech Enhancement.},
doi={10.1109/ICASSP40776.2020.9053989},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053673,
author={D. {Manandhar} and M. {Bastan} and K. {Yap}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Dynamically Modulated Deep Metric Learning for Visual Search},
year={2020},
volume={},
number={},
pages={2408-2412},
abstract={This paper proposes dynamically modulated metric learning (DMML) for learning a tiered similarity space to perform visual search. Existing methods often treat the training samples having different degree of information with equal importance which hinders in capturing the underlying granularities in visual similarity. Proposed DMML automatically exploits the informativeness of samples during training by leveraging correlation between image attributes and embedding that are learned jointly. The two tasks are interlinked by supervising signals where the predicted attribute vectors are used to dynamically learn the loss function. To this end, we propose a new soft-binomial deviance loss that helps to capture the feature similarity space at multiple granularities. Compared to recent ensemble and attention based methods, our DMML framework is conceptually simple yet effective, and achieves state-of-the-art performances on standard benchmark datasets; e.g. an improvement of 4% Recall@1 over the SOTA [1] on DeepFashion dataset.},
keywords={Visual Search;Metric Loss;Embedding},
doi={10.1109/ICASSP40776.2020.9053673},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054111,
author={R. {Li} and J. {Jiang} and X. {Wu} and H. {Mao} and C. {Hsieh} and W. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Bridging Mixture Density Networks with Meta-Learning for Automatic Speaker Identification},
year={2020},
volume={},
number={},
pages={3522-3526},
abstract={Speaker identification answers the fundamental question "Who is speakingƒ" The identification technology enables various downstream applications to provide a personalized experience. Both the prevalent i-vector based solutions and the state-of-the-art deep learning solutions usually treat all users equally, with no distinctions between new users and existing users, during the training process. We notice that a good many new users start with limited labeled training data, which often results in inferior predicting performance of recognizing users’ voices. To alleviate the disadvantage caused by training data deficiency, we propose a Mixture Density Network- based Meta-Learning method (MDNML) for speaker identification. MDNML emphasizes the expeditious process of learning to recognize new users where each has only a few seconds of labeled data.We conduct experiments on the LibriSpeech dataset and compare MDNML with four state-of-the-art baseline methods. The results conclude that MDNML achieves higher accuracy in recognizing new users with limited labeled utterances than all baseline methods. Our proposed solution significantly expedites the learning by transferring the knowledge learned from the existing user base through gradient-based meta-learning. We consider our work to be a steppingstone for more sophisticated meta-learning frameworks for accelerating voice recognition. Furthermore, we discuss a strategy for enhancing the accuracy by incorporating the notion of household-based acoustic profiles with MDNML.},
keywords={mixture density networks;meta-learning;new users;speaker identification},
doi={10.1109/ICASSP40776.2020.9054111},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053091,
author={A. {Mantha} and Y. {Arora} and S. {Gupta} and P. {Kanumala} and Z. {Liu} and S. {Guo} and K. {Achan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Large-Scale Deep Architecture for Personalized Grocery Basket Recommendations},
year={2020},
volume={},
number={},
pages={3807-3811},
abstract={With growing consumer adoption of online grocery shopping through platforms such as Amazon Fresh, Instacart, and Walmart Grocery, there is a pressing business need to provide relevant recommendations throughout the customer journey. In this paper, we introduce a production within-basket grocery recommendation system, RTT2Vec, which generates real-time personalized product recommendations to supplement the user’s current grocery basket. We conduct extensive offline evaluation of our system and demonstrate a 9.4% uplift in prediction metrics over baseline stateof-the-art within-basket recommendation models. We also propose an approximate inference technique 11.6x times faster than exact inference approaches. In production, our system has resulted in an increase in average basket size, improved product discovery, and enabled faster user check-out.},
keywords={Recommender System;Personalization;Representation Learning},
doi={10.1109/ICASSP40776.2020.9053091},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054090,
author={H. {Sundar} and W. {Wang} and M. {Sun} and C. {Wang}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Raw Waveform Based End-to-end Deep Convolutional Network for Spatial Localization of Multiple Acoustic Sources},
year={2020},
volume={},
number={},
pages={4642-4646},
abstract={In this paper, we present an end-to-end deep convolutional neural network operating on multi-channel raw audio data to localize multiple simultaneously active acoustic sources in space. Previously reported deep learning based approaches work well in localizing a single source directly from multi-channel raw-audio, but are not easily extendable to localize multiple sources due to the well known permutation problem. We propose a novel encoding scheme to represent the spatial coordinates of multiple sources, which facilitates 2D localization of multiple sources in an end-to-end fashion, avoiding the permutation problem and achieving arbitrary spatial resolution. Experiments on a simulated data set and real recordings from the AV16.3 Corpus demonstrate that the proposed method generalizes well to unseen test conditions, and outperforms a recent time difference of arrival (TDOA) based multiple source localization approach reported in the literature.},
keywords={Acoustic Source Localization;Deep Learning;Convolutional Neural Networks;Raw Waveform},
doi={10.1109/ICASSP40776.2020.9054090},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053565,
author={A. H. {Li} and A. {Sethy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Semi-Supervised Learning for Text Classification by Layer Partitioning},
year={2020},
volume={},
number={},
pages={6164-6168},
abstract={Most recent neural semi-supervised learning (SSL) algorithms rely on adding small perturbation to either the input vectors or their representations. These methods have been successful on computer vision tasks as the images form a continuous manifold, but are not appropriate for discrete input such as sentence. To adapt these methods to text input, we propose to decompose a neural network M into two components F and U so that M = U ◦ F . The layers in F are then frozen and only the layers in U will be updated during most time of the training. In this way, F serves as a feature extractor that maps the input to high-level representation and adds systematical noise using dropout. We can then train U using any state-of-the-art SSL algorithms such as Π-model, temporal ensembling, mean teacher, etc. Furthermore, this gradually unfreezing schedule also prevents a pretrained model from catastrophic forgetting. The experimental results demonstrate that our approach provides improvements when compared to state of the art methods especially on short texts.},
keywords={Semi-supervised learning;transfer learning;text classification;neural network},
doi={10.1109/ICASSP40776.2020.9053565},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053678,
author={V. {Aggarwal} and M. {Cotescu} and N. {Prateek} and J. {Lorenzo-Trueba} and R. {Barra-Chicote}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Using Vaes and Normalizing Flows for One-Shot Text-To-Speech Synthesis of Expressive Speech},
year={2020},
volume={},
number={},
pages={6179-6183},
abstract={We propose a Text-to-Speech method to create an unseen expressive style using one utterance of expressive speech of around one second. Specifically, we enhance the disentanglement capabilities of a state-of-the-art sequence-to-sequence based system with a Variational AutoEncoder (VAE) and a Householder Flow. The proposed system provides a 22% KL-divergence reduction while jointly improving perceptual metrics over state-of-the-art. At synthesis time we use one example of expressive style as a reference input to the encoder for generating any text in the desired style. Perceptual MUSHRA evaluations show that we can create a voice with a 9% relative naturalness improvement over standard Neural Text-to-Speech, while also improving the perceived emotional intensity (59 compared to the 55 of neutral speech).},
keywords={Text-to-speech;data efficiency;and semi-supervised learning},
doi={10.1109/ICASSP40776.2020.9053678},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053176,
author={S. {Ling} and Y. {Liu} and J. {Salazar} and K. {Kirchhoff}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition},
year={2020},
volume={},
number={},
pages={6429-6433},
abstract={We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly.},
keywords={speech recognition;acoustic representation learning;semi-supervised learning},
doi={10.1109/ICASSP40776.2020.9053176},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054401,
author={J. {Casebeer} and U. {Isik} and S. {Venkataramani} and A. {Krishnaswamy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Efficient Trainable Front-Ends for Neural Speech Enhancement},
year={2020},
volume={},
number={},
pages={6639-6643},
abstract={Many neural speech enhancement and source separation systems operate in the time-frequency domain. Such models often benefit from making their Short-Time Fourier Transform (STFT) front-ends trainable. In current literature, these are implemented as large Discrete Fourier Transform matrices; which are prohibitively inefficient for low-compute systems. We present an efficient, trainable front-end based on the butterfly mechanism to compute the Fast Fourier Transform, and show its accuracy and efficiency benefits for low-compute neural speech enhancement models. We also explore the effects of making the STFT window trainable.},
keywords={speech enhancement;source separation;deep learning;hearables;low computation},
doi={10.1109/ICASSP40776.2020.9054401},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053940,
author={T. {Park} and K. {Kumatani} and M. {Wu} and S. {Sundaram}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Multi-Channel Speech Recognition Using Frequency Aligned Network},
year={2020},
volume={},
number={},
pages={6859-6863},
abstract={Conventional speech enhancement technique such as beamforming has known benefits for far-field speech recognition. Our own work in frequency-domain multi-channel acoustic modeling has shown additional improvements by training a spatial filtering layer jointly within an acoustic model. In this paper, we further develop this idea and use frequency aligned network for robust multi-channel automatic speech recognition (ASR). Unlike an affine layer in the frequency domain, the proposed frequency aligned component prevents one frequency bin influencing other frequency bins. We show that this modification not only reduces the number of parameters in the model but also significantly and improves the ASR performance. We investigate effects of frequency aligned network through ASR experiments on the real-world far-field data where users are interacting with an ASR system in uncontrolled acoustic environments. We show that our multi-channel acoustic model with a frequency aligned network shows up to 18% relative reduction in word error rate.},
keywords={multi-channel acoustic modeling;beamforming;microphone arrays;automatic speech recognition},
doi={10.1109/ICASSP40776.2020.9053940},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053367,
author={S. {Wager} and A. {Khare} and M. {Wu} and K. {Kumatani} and S. {Sundaram}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fully Learnable Front-End for Multi-Channel Acoustic Modeling Using Semi-Supervised Learning},
year={2020},
volume={},
number={},
pages={6864-6868},
abstract={In this work, we investigated the teacher-student training paradigm to train a fully learnable multi-channel acoustic model for far-field automatic speech recognition (ASR). Using a large offline teacher model trained on beamformed audio, we trained a simpler multi-channel student acoustic model used in the speech recognition system. For the student, both multi-channel feature extraction layers and the higher classification layers were jointly trained using the logits from the teacher model. In our experiments, compared to a baseline model trained on about 600 hours of transcribed data, a relative word-error rate (WER) reduction of about 27.3% was achieved when using an additional 1800 hours of untran-scribed data. We also investigated the benefit of pre-training the multi-channel front end to output the beamformed log-mel filter bank energies (LFBE) using L2 loss. We find that pre-training improves the word error rate by 10.7% when compared to a multi-channel model directly initialized with a beamformer and mel-filter bank coefficients for the front end. Finally, combining pre-training and teacher-student training produces a WER reduction of 31% compared to our baseline.},
keywords={far-field automatic speech recognition;acoustic modeling;knowledge distillation;teacher-student training},
doi={10.1109/ICASSP40776.2020.9053367},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053541,
author={W. {Wang} and Q. {Tang} and K. {Livescu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Unsupervised Pre-Training of Bidirectional Speech Encoders via Masked Reconstruction},
year={2020},
volume={},
number={},
pages={6889-6893},
abstract={We propose an approach for pre-training speech representations via a masked reconstruction loss. Our pre-trained encoder networks are bidirectional and can therefore be used directly in typical bidirectional speech recognition models. The pre-trained networks can then be fine-tuned on a smaller amount of supervised data for speech recognition. Experiments with this approach on the LibriSpeech and Wall Street Journal corpora show promising results. We find that the main factors that lead to speech recognition improvements are: masking segments of sufficient width in both time and frequency, pre-training on a much larger amount of unlabeled data than the labeled data, and domain adaptation when the unlabeled and labeled data come from different domains. The gain from pre-training is additive to that of supervised data augmentation.},
keywords={Unsupervised representation learning;Pre-training;Masked reconstruction},
doi={10.1109/ICASSP40776.2020.9053541},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053121,
author={J. {Yuan} and H. {Lin} and Y. {Liu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Detection and Analysis of T/D Deletion in Librispeech},
year={2020},
volume={},
number={},
pages={7324-7328},
abstract={In this study we developed a new method for automatic identification of t/d deletion. Our method achieved 94% accuracy on TIMIT and 87% on human-annotated data from Librispeech. We then conducted an analysis of t/d deletion on more than 500k tokens in Librispeech. The following results were found: (1) /d/ is more likely to be deleted than /t/; (2) t/d is more likely to be deleted when preceded by a nasal or a coronal obstruent; (3) In terms of the following phone, the rate of t/d deletion from low to high was: vowels and pause < glides and liquids < other phones; (4) In terms of the morphological class, the rate of t/d deletion from high to low was: stem > semi-weak past tense > regular past tense; (5) t/d is less likely to be deleted when the phonological neighborhood density (PND) is higher.},
keywords={t/d deletion;large-scale analysis;automatic detection of t/d deletion;phonological neighborhood density},
doi={10.1109/ICASSP40776.2020.9053121},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053313,
author={Y. {Gao} and Y. {Mishchenko} and A. {Shah} and S. {Matsoukas} and S. {Vitaladevuni}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Towards Data-Efficient Modeling for Wake Word Spotting},
year={2020},
volume={},
number={},
pages={7479-7483},
abstract={Wake word (WW) spotting is challenging in far-field not only because of the interference in signal transmission but also the complexity in acoustic environment. Traditional WW model training requires large amount of in-domain WW-specific data with substantial human annotations. This prevents the model building in the situation of lacking such data. In this paper we present data-efficient solutions to address the challenges in WW modeling, such as domain-mismatch, noisy conditions, limited annotation, etc. Our proposed system is composed of a multi-condition training pipeline with stratified data augmentation, which improves the model robustness to a variety of predefined acoustic conditions, together with a semi-supervised learning pipeline to extract the WW and adversarial examples from untranscribed speech corpus. Starting from only 10 hours of domain-mismatched WW audio, we are able to enlarge and enrich the training dataset by 20-100 times to capture the complexity in acoustic environments. Our experiments on real user data show that the proposed solutions can achieve comparable performance of a production-grade model by saving 97% of the amount of WW-specific data to collect and 86% of the bandwidth for annotation.},
keywords={wake word spotting;far-field;multi-condition training;semi-supervised learning},
doi={10.1109/ICASSP40776.2020.9053313},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054301,
author={H. B. {Moss} and V. {Aggarwal} and N. {Prateek} and J. {González} and R. {Barra-Chicote}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={BOFFIN TTS: Few-Shot Speaker Adaptation by Bayesian Optimization},
year={2020},
volume={},
number={},
pages={7639-7643},
abstract={We present BOFFIN TTS (Bayesian Optimization For FIne-tuning Neural Text To Speech), a novel approach for few-shot speaker adaptation. Here, the task is to fine-tune a pre-trained TTS model to mimic a new speaker using a small corpus of target utterances. We demonstrate that there does not exist a one-size-fits-all adaptation strategy, with convincing synthesis requiring a corpus-specific configuration of the hyper-parameters that control fine-tuning. By using Bayesian optimization to efficiently optimize these hyper-parameter values for a target speaker, we are able to perform adaptation with an average 30% improvement in speaker similarity over standard techniques. Results indicate, across multiple corpora, that BOFFIN TTS can learn to synthesize new speakers using less than ten minutes of audio, achieving the same naturalness as produced for the speakers used to train the base model.},
keywords={text-to-speech;speaker adaptation;Bayesian optimization;transfer learning},
doi={10.1109/ICASSP40776.2020.9054301},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054304,
author={K. {Gillespie} and I. C. {Konstantakopoulos} and X. {Guo} and V. T. {Vasudevan} and A. {Sethy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Device Directedness Classification of Utterances With Semantic Lexical Features},
year={2020},
volume={},
number={},
pages={7859-7863},
abstract={User interactions with personal assistants like Alexa, Google Home and Siri are typically initiated by a wake term or wake-word. Several personal assistants feature "follow-up" modes that allow users to make additional interactions without the need of a wakeword. For the system to only respond when appropriate, and to ignore speech not intended for it, utterances must be classified as device-directed or non-device-directed. State of the art systems have largely used acoustic features for this task, while others have used only lexical features or have added LM-based lexical features. We propose a directedness classifier that combines semantic lexical features with a lightweight acoustic feature and show it is effective in classifying directedness. The mixed-domain lexical and acoustic feature model is able to achieve 14% relative reduction of EER over a state of the art acoustic-only baseline model. Finally, we successfully apply transfer learning and semi-supervised learning to the model to improve accuracy even further.},
keywords={Directedness;Semantic Classification;LSTM;Semi-supervised Learning;Word Embeddings},
doi={10.1109/ICASSP40776.2020.9054304},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053728,
author={J. {Kim} and Y. {Kim}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Pseudo Labeling and Negative Feedback Learning for Large-Scale Multi-Label Domain Classification},
year={2020},
volume={},
number={},
pages={7964-7968},
abstract={In large-scale domain classification, an utterance can be handled by multiple domains with overlapped capabilities. However, only a limited number of ground-truth domains are provided for each training utterance in practice while knowing as many as correct target labels is helpful for improving the model performance. In this paper, given one ground-truth domain for each training utterance, we regard domains consistently predicted with the highest confidences as additional pseudo labels for the training. In order to reduce prediction errors due to incorrect pseudo labels, we leverage utterances with negative system responses to decrease the confidences of the incorrectly predicted domains. Evaluating on user utterances from an intelligent conversational system, we show that the proposed approach significantly improves the performance of domain classification with hypothesis reranking.},
keywords={Domain classification;multi-label classification;pseudo labeling;negative feedback learning},
doi={10.1109/ICASSP40776.2020.9053728},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053531,
author={Z. {Chen} and X. {Fan} and Y. {Ling}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Pre-Training for Query Rewriting in a Spoken Language Understanding System},
year={2020},
volume={},
number={},
pages={7969-7973},
abstract={Query rewriting (QR) is an increasingly important technique to reduce customer friction caused by errors in a spoken language understanding pipeline, where the errors originate from various sources such as speech recognition errors, language understanding errors or entity resolution errors. In this work, we first propose a neural-retrieval based approach for query rewriting. Then, inspired by the wide success of pre-trained contextual language embeddings, and also as a way to compensate for insufficient QR training data, we propose a language-modeling (LM) based approach to pre-train query embeddings on historical user conversation data with a voice assistant. In addition, we propose to use the NLU hypotheses generated by the language understanding system to augment the pre-training. Our experiments show pre-training provides rich prior information and help the QR task achieve strong performance. We also show joint pre-training with NLU hypotheses has further benefit. Finally, after pre-training, we find a small set of rewrite pairs is enough to fine-tune the QR model to outperform a strong baseline by full training on all QR training data.},
keywords={query rewrite;neural retrieval;contextual embeddings;pre-training;user friction},
doi={10.1109/ICASSP40776.2020.9053531},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053042,
author={A. {Alok} and R. {Gupta} and S. {Ananthakrishnan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Design Considerations for Hypothesis Rejection Modules in Spoken Language Understanding Systems},
year={2020},
volume={},
number={},
pages={8049-8053},
abstract={Spoken Language Understanding (SLU) systems typically consist of a set of machine learning models that operate in conjunction to produce an SLU hypothesis. The generated hypothesis is then sent to downstream components for further action. However, it is desirable to discard an incorrect hypothesis before sending it downstream. In this work, we present two designs for SLU hypothesis rejection modules: (i) scheme R1 that performs rejection on domain specific SLU hypothesis and, (ii) scheme R2 that performs rejection on hypothesis generated from the overall SLU system. Hypothesis rejection modules in both schemes reject/accept a hypothesis based on features drawn from the utterance directed to the SLU system, the associated SLU hypothesis and SLU confidence score. Our experiments suggest that both the schemes yield similar results (scheme R1: 2.5% FRR @ 4.5% FAR, scheme R2: 2.5% FRR @ 4.6% FAR), with the best performing systems using all the available features. We argue that while either of the rejection schemes can be chosen over the other, they carry some inherent differences which need to be considered while making this choice. Additionally, we incorporate ASR features in the rejection module (obtaining an 1.9% FRR @ 3.8% FAR) and analyze the improvements.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053042},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053167,
author={D. {Makhervaks} and W. {Hinthorn} and D. {Dimitriadis} and A. {Stolcke}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Combining Acoustics, Content and Interaction Features to Find Hot Spots in Meetings},
year={2020},
volume={},
number={},
pages={8054-8058},
abstract={Involvement hot spots have been proposed as a useful concept for meeting analysis and studied off and on for over 15 years. These are regions of meetings that are marked by high participant involvement, as judged by human annotators. However, prior work was either not conducted in a formal machine learning setting, or focused on only a subset of possible meeting features or downstream applications (such as summarization). In this paper we investigate to what extent various acoustic, linguistic and pragmatic aspects of the meetings, both in isolation and jointly, can help detect hot spots. In this context, the openSMILE toolkit [1] is to used to extract features based on acoustic-prosodic cues, BERT word embeddings [2] are used for encoding the lexical content, and a variety of statistics based on speech activity are used to describe the verbal interaction among participants. In experiments on the annotated ICSI meeting corpus, we find that the lexical model is the most informative, with incremental contributions from interaction and acoustic-prosodic model components.},
keywords={Hot spots;involvement;meeting understanding;feature fusion},
doi={10.1109/ICASSP40776.2020.9053167},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054240,
author={A. {Tyagi} and V. {Sharma} and R. {Gupta} and L. {Samson} and N. {Zhuang} and Z. {Wang} and B. {Campbell}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast Intent Classification for Spoken Language Understanding Systems},
year={2020},
volume={},
number={},
pages={8119-8123},
abstract={Spoken Language Understanding (SLU) systems consist of several machine learning components operating together (e.g. intent classification, named entity recognition and resolution). Deep learning models have obtained state of the art results on several of these tasks, largely attributed to their better modeling capacity. However, an increase in modeling capacity comes with added costs of higher latency and energy usage, particularly when operating on low complexity devices. To address the latency and computational complexity issues, we explore a BranchyNet scheme on an intent classification scheme within SLU systems. The BranchyNet scheme when applied to a high complexity model, adds exit points at various stages in the model allowing early decision making for a set of queries to the SLU model. We conduct experiments on the Facebook Semantic Parsing dataset with two candidate model architectures for intent classification. Our experiments show that the BranchyNet scheme provides gains in terms of computational complexity without compromising model accuracy. We also conduct analytical studies regarding the improvements in the computational cost, distribution of utterances that egress from various exit points and the impact of adding more complexity to models with the BranchyNet scheme.},
keywords={Spoken Language Understanding;BranchyNet;Intent Classification},
doi={10.1109/ICASSP40776.2020.9054240},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054696,
author={M. {Yu} and H. D. {Nguyen} and A. {Sokolov} and J. {Lepird} and K. M. {Sathyendra} and S. {Choudhary} and A. {Mouchtaris} and S. {Kunzmann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multilingual Grapheme-To-Phoneme Conversion with Byte Representation},
year={2020},
volume={},
number={},
pages={8234-8238},
abstract={Grapheme-to-phoneme (G2P) models convert a written word into its corresponding pronunciation and are essential components in automatic-speech-recognition and text-to-speech systems. Recently, the use of neural encoder-decoder architectures has substantially improved G2P accuracy for mono- and multi-lingual cases. However, most multilingual G2P studies focus on sets of languages that share similar graphemes, such as European languages. Multilingual G2P for languages from different writing systems, e.g. European and East Asian, remains an understudied area. In this work, we propose a multilingual G2P model with byte-level input representation to accommodate different grapheme systems, along with an attention-based Transformer architecture. We evaluate the performance of both character-level and byte-level G2P using data from multiple European and East Asian locales. Models using byte representation yield 16.2%– 50.2% relative word error rate improvement over character-based counterparts for mono- and multi-lingual use cases. In addition, byte-level models are 15.0%–20.1% smaller in size. Our results show that byte is an efficient representation for multilingual G2P with languages having large grapheme vocabularies.},
keywords={Grapheme-to-phoneme (G2P);multilingual;end-to-end models;byte representation;pronunciation generation},
doi={10.1109/ICASSP40776.2020.9054696},
ISSN={2379-190X},
month={May},}
('semisupervised learning', 4)
('texttospeech', 2)
('deep learning', 2)
('pretraining', 2)
('transfer learning', 2)
('', 1)
('speech recognition', 1)
('teacherstudent training', 1)
('acoustic source localization', 1)
('metalearning', 1)
('long shortterm memory (lstm)', 1)
('multicondition training', 1)
('feature fusion', 1)
('pseudo labeling', 1)
('dataset', 1)
('mixture density networks', 1)
('unet', 1)
('semantic classification', 1)
('source separation', 1)
('clotho', 1)
('personalization', 1)
('acoustic event detection', 1)
('lstm', 1)
('low computation', 1)
('neural network', 1)
('wake word spotting', 1)
('negative feedback learning', 1)
('spoken language understanding', 1)
('multilabel classification', 1)
('acoustic modeling', 1)
('representation learning', 1)
('microphone arrays', 1)
('contextual embeddings', 1)
('meeting understanding', 1)
('byte representation', 1)
('beamforming', 1)
('channelattention', 1)
('endtoend models', 1)
('text classification', 1)
('speaker identification', 1)
('multichannel speech enhancement.', 1)
('directedness', 1)
('word embeddings', 1)
('hot spots', 1)
('speaker adaptation', 1)
('visual search', 1)
('and semisupervised learning', 1)
('new users', 1)
('user friction', 1)
('multilingual', 1)
('audio captioning', 1)
('phonological neighborhood density', 1)
('acoustic event classification and detection', 1)
('branchynet', 1)
('hearables', 1)
('involvement', 1)
('domain classification', 1)
('query rewrite', 1)
('bayesian optimization', 1)
('acoustic representation learning', 1)
('convolutional neural networks', 1)
('intent classification', 1)
('complex ratio masking', 1)
('metric loss', 1)
('fewshot learning', 1)
('multichannel acoustic modeling', 1)
('farfield automatic speech recognition', 1)
('pronunciation generation', 1)
('pooling functions', 1)
('speech enhancement', 1)
('graphemetophoneme (g2p)', 1)
('data efficiency', 1)
('meta learning', 1)
('recommender system', 1)
('farfield', 1)
('automatic detection of t/d deletion', 1)
('t/d deletion', 1)
('embedding', 1)
('raw waveform', 1)
('masked reconstruction', 1)
('automatic speech recognition', 1)
('largescale analysis', 1)
('knowledge distillation', 1)
('unsupervised representation learning', 1)
('neural retrieval', 1)