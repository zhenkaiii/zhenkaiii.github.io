@INPROCEEDINGS{9053921,
author={E. {Tzinis} and S. {Wisdom} and J. R. {Hershey} and A. {Jansen} and D. P. W. {Ellis}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Universal Sound Separation Using Sound Classification},
year={2020},
volume={},
number={},
pages={96-100},
abstract={Deep learning approaches have recently achieved impressive performance on both audio source separation and sound classification. Most audio source separation approaches focus only on separating sources belonging to a restricted domain of source classes, such as speech and music. However, recent work has demonstrated the possibility of "universal sound separation", which aims to separate acoustic sources from an open domain, regardless of their class. In this paper, we utilize the semantic information learned by sound classifier networks trained on a vast amount of diverse sounds to improve universal sound separation. In particular, we show that semantic embeddings extracted from a sound classifier can be used to condition a separation network, providing it with useful additional information. This approach is especially useful in an iterative setup, where source estimates from an initial separation stage and their corresponding classifier-derived embeddings are fed to a second separation network. By performing a thorough hyperparameter search consisting of over a thousand experiments, we find that classifier embeddings from oracle clean sources provide nearly one dB of SNR gain, and our best iterative models achieve a significant fraction of this oracle performance, establishing a new state-of-the-art for universal sound separation.},
keywords={Audio source separation;deep learning;semantic audio representations;sound classification},
doi={10.1109/ICASSP40776.2020.9053921},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054137,
author={A. {Jansen} and D. P. W. {Ellis} and S. {Hershey} and R. C. {Moore} and M. {Plakal} and A. C. {Popat} and R. A. {Saurous}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Coincidence, Categorization, and Consolidation: Learning to Recognize Sounds with Minimal Supervision},
year={2020},
volume={},
number={},
pages={121-125},
abstract={Humans do not acquire perceptual abilities in the way we train machines. While machine learning algorithms typically operate on large collections of randomly-chosen, explicitly-labeled examples, human acquisition relies more heavily on multimodal unsupervised learning (as infants) and active learning (as children). With this motivation, we present a learning framework for sound representation and recognition that combines (i) a self-supervised objective based on a general notion of unimodal and cross-modal coincidence, (ii) a clustering objective that reflects our need to impose categorical structure on our experiences, and (iii) a cluster-based active learning procedure that solicits targeted weak supervision to consolidate categories into relevant semantic classes. By training a combined sound embedding/clustering/classification network according to these criteria, we achieve a new state-of-the-art unsupervised audio representation and demonstrate up to a 20-fold reduction in the number of labels required to reach a desired classification performance.},
keywords={Sound classification;self-supervised learning;multimodal models;clustering;active learning},
doi={10.1109/ICASSP40776.2020.9054137},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053065,
author={H. {Shrivaslava} and Y. {Yin} and R. R. {Shah} and R. {Zimmermann}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Mt-Gcn For Multi-Label Audio Tagging With Noisy Labels},
year={2020},
volume={},
number={},
pages={136-140},
abstract={Multi-label audio tagging is the task of predicting the types of sounds occurring in an audio clip. Recently, large-scale audio datasets such as Google’s AudioSet, have allowed researchers to use deep learning techniques for this task but this comes at the cost of label noise in the datasets. Audio datasets such as the AudioSet are usually built following a hierarchical structure known as ontology which captures the relationships between different sound events with domain knowledge. However, existing methods for audio tagging failed to utilize this domain knowledge about label relationships in their models, resulting in models being sensitive to label noise. We therefore present MT-GCN, a Multi-task Learning based Graph Convolutional Network that learns domain knowledge from ontology. The relationships between sound events in our proposed method are described by a graph. We propose two ontology-based graph construction methods, and conduct extensive experiments on the FSDKaggle2019 dataset. The experimental results show that our approach outperforms the baseline methods by a significant margin.},
keywords={Audio Tagging;Graph Convolutional Networks;Multi-task Learning},
doi={10.1109/ICASSP40776.2020.9053065},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053846,
author={S. {Sonning} and C. {Schüldt} and H. {Erdogan} and S. {Wisdom}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Performance Study of a Convolutional Time-Domain Audio Separation Network for Real-Time Speech Denoising},
year={2020},
volume={},
number={},
pages={831-835},
abstract={Time-domain audio separation networks based on dilated temporal convolutions have recently been shown to perform very well compared to methods that are based on a time-frequency representation in speech separation tasks, even outperforming an oracle binary time-frequency mask of the speakers. This paper investigates the performance of such a time-domain network (Conv-TasNet) for speech denoising in a real-time setting, comparing various parameter settings. Most importantly, different amounts of lookahead are evaluated and compared to the baseline of a fully causal model. We show that a large part of the increase in performance between a causal and non-causal model is achieved with a lookahead of only 20 milliseconds, demonstrating the usefulness of even small lookaheads for many real-time applications.},
keywords={Speech enhancement;noise reduction;deep learning;convolutional neural networks;time domain},
doi={10.1109/ICASSP40776.2020.9053846},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053268,
author={M. {Whitehill} and J. {Garrison} and S. {Patel}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Whosecough: In-the-Wild Cougher Verification Using Multitask Learning},
year={2020},
volume={},
number={},
pages={896-900},
abstract={Current automatic cough counting systems can determine how many coughs are present in an audio recording. However, they cannot determine who produced the cough. This limits their usefulness as most systems are deployed in locations with multiple people (i.e., a smart home device in a four-person home). Previous models trained solely on speech performed reasonably well on forced coughs [1]. By incorporating coughs into the training data, the model performance should improve. However, since limited natural cough data exists, training on coughs can lead to model overfitting. In this work, we overcome this problem by using multitask learning, where the second task is speaker verification. Our model achieves 82.15% classification accuracy amongst four users on a natural, in-the-wild cough dataset, outperforming human evaluators on average by 9.82%.},
keywords={Cough;Health Sensing;Multitask Learning;Speaker Verification;Deep Neural Networks},
doi={10.1109/ICASSP40776.2020.9053268},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054067,
author={B. {Li} and J. {Han} and K. {Rose}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Adaptive Linear Estimator Based Approach to Bi-Directional Motion Compensated Prediction},
year={2020},
volume={},
number={},
pages={2038-2042},
abstract={Bi-directional motion compensated prediction is widely utilized in video coding. Conventionally, the encoder searches for two motion vectors pointing to reference frames in both directions, and transmits these motion vectors to the decoder. Recognizing that the two reference frames are already available to the decoder, prior work proposed decoder-side motion estimation to extract motion information or optical flow, at the cost of dramatic increase in decoder complexity. This paper proposes a novel bi-directional motion compensation mode that efficiently utilizes the motion information that is already available to the decoder, without recourse to extensive search. An estimation theory based approach is proposed and utilized to provide a high quality prediction, which adaptively combines contributions from multiple motion-compensated references. Experimental results show that the proposed method, while yielding a greatly reduced decoder side complexity, introduces a significant coding gain for a diverse set of video sequences.},
keywords={Video coding;motion compensation;hierarchical structure;linear estimation},
doi={10.1109/ICASSP40776.2020.9054067},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053634,
author={Z. {Tu} and J. {Lin} and Y. {Wang} and B. {Adsumilli} and A. C. {Bovik}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Bband Index: a No-Reference Banding Artifact Predictor},
year={2020},
volume={},
number={},
pages={2712-2716},
abstract={Banding artifact, or false contouring, is a common video compression impairment that tends to appear on large flat regions in encoded videos. These staircase-shaped color bands can be very noticeable in high-definition videos. Here we study this artifact, and propose a new distortion-specific no-reference video quality model for predicting banding artifacts, called the Blind BANding Detector (BBAND index). BBAND is inspired by human visual models. The proposed detector can generate a pixel-wise banding visibility map and output a banding severity score at both the frame and video levels. Experimental results show that our proposed method outperforms state-of-the-art banding detection algorithms and delivers better consistency with subjective evaluations.},
keywords={Video quality predictor;compression artifact;banding artifact;false contour;human visual model},
doi={10.1109/ICASSP40776.2020.9053634},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053036,
author={M. {Gooneratne} and K. C. {Sim} and P. {Zadrazil} and A. {Kabel} and F. {Beaufays} and G. {Motta}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low-Rank Gradient Approximation for Memory-Efficient on-Device Training of Deep Neural Network},
year={2020},
volume={},
number={},
pages={3017-3021},
abstract={Training machine learning models on mobile devices has the potential of improving both privacy and accuracy of the models. However, one of the major obstacles to achieving this goal is the memory limitation of mobile devices. Reducing training memory enables models with high-dimensional weight matrices, like automatic speech recognition (ASR) models, to be trained on-device. In this paper, we propose approximating the gradient matrices of deep neural networks using a low-rank parameterization as an avenue to save training memory. The low-rank gradient approximation enables more advanced, memory-intensive optimization techniques to be run on device. Our experimental results show that we can reduce the training memory by about 33.0% for Adam optimization. It uses comparable memory to momentum optimization and achieves a 4.5% relative lower word error rate on an ASR personalization task.},
keywords={on-device learning;low-rank gradient;memory reduction},
doi={10.1109/ICASSP40776.2020.9053036},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054750,
author={S. {Samizade} and Z. {Tan} and C. {Shen} and X. {Guan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Adversarial Example Detection by Classification for Deep Speech Recognition},
year={2020},
volume={},
number={},
pages={3102-3106},
abstract={Machine Learning systems are vulnerable to adversarial attacks and will highly likely produce incorrect outputs under these attacks. There are white-box and black-box attacks regarding to adversary’s access level to the victim learning algorithm. To defend the learning systems from these attacks, existing methods in the speech domain focus on modifying input signals and testing the behaviours of speech recognizers. We, however, formulate the defense as a classification problem and present a strategy for systematically generating adversarial example datasets: one for white-box attacks and one for black-box attacks, containing both adversarial and normal examples. The white-box attack is a gradient-based method on Baidu DeepSpeech with the Mozilla Common Voice database while the black-box attack is a gradient-free method on a deep model-based keyword spotting system with the Google Speech Command dataset. The generated datasets are used to train a proposed Convolutional Neural Network (CNN), together with cepstral features, to detect adversarial examples. Experimental results show that, it is possible to accurately distinct between adversarial and normal examples for known attacks, in both single-condition and multi-condition training settings, while the performance degrades dramatically for unknown attacks. The adversarial datasets and the source code are made publicly available.},
keywords={Speech recognition;adversarial attack;convo-lutional neural network;cepstral feature},
doi={10.1109/ICASSP40776.2020.9054750},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053498,
author={P. {Gonnet} and T. {Deselaers}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Indylstms: Independently Recurrent LSTMS},
year={2020},
volume={},
number={},
pages={3352-3356},
abstract={We introduce Independently Recurrent Long Short-term Memory cells: IndyLSTMs. These differ from regular LSTM cells in that the recurrent weights are not modeled as a full matrix, but as a diagonal matrix, i.e. the output and state of each LSTM cell depends on the inputs and its own output/state, as opposed to the input and the outputs/states of all the cells in the layer. The number of parameters per Indy-LSTM layer, and thus the number of FLOPS per evaluation, is linear in the number of nodes in the layer, as opposed to quadratic for regular LSTM layers, resulting in potentially both smaller and faster models.We evaluate their performance experimentally by training several models on the popular IAM-OnDB and CASIA online handwriting datasets, as well as on several of our in-house datasets. We show that IndyLSTMs, despite their smaller size, consistently outperform regular LSTMs both in terms of accuracy per parameter, and in best accuracy overall. We attribute this improved performance to the IndyLSTMs being less prone to overfitting.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053498},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053798,
author={B. {Gfeller} and C. {Frank} and D. {Roblek} and M. {Sharifi} and M. {Tagliasacchi} and M. {Velimirović}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Pitch Estimation Via Self-Supervision},
year={2020},
volume={},
number={},
pages={3527-3531},
abstract={We present a method to estimate the fundamental frequency in monophonic audio, often referred to as pitch estimation. In contrast to existing methods, our neural network can be fully trained only on unlabeled data, using self-supervision. A tiny amount of labeled data is needed solely for mapping the network outputs to absolute pitch values. The key to this is the observation that if one creates two examples from one original audio clip by pitch shifting both, the difference between the correct outputs is known, without even knowing the actual pitch value in the original clip. Somewhat surprisingly, this idea combined with an auxiliary reconstruction loss allows training a pitch estimation model. Our results show that our pitch estimation method obtains an accuracy comparable to fully supervised models on monophonic audio, without the need for large labeled datasets. In addition, we are able to train a voicing detection output in the same model, again without using any labels.1},
keywords={audio pitch estimation;unsupervised learning;convolutional neural networks},
doi={10.1109/ICASSP40776.2020.9053798},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054748,
author={F. S. {Fard} and A. {Rad} and V. S. {Tomar}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Nasil: Neural Architecture Search with Imitation Learning},
year={2020},
volume={},
number={},
pages={3972-3976},
abstract={Automated machine learning (AML) refers to a class of techniques that, given a problem, can find an optimal set of model architectures, properties, and parameters. In recent years, AML has shown great success in finding neural network structures that are as good as the hand-designed structures and even better. AML is usually a time consuming process that needs to search a big search space. All available AML solutions start the search from scratch. Here we introduce neural architecture search with imitation learning (NASIL) method that starts the search by learning from hand designed structures by experts. NASIL is an actor-critic structure that uses hindsight experience replay(HER) and starts by training on these hand-designed structures stored in memory. We tested NASIL found structures on publicly available Google speech command dataset as well as in-house dataset. Our results show that NASIL is able to find a better structure than handdesigned structures in terms of accuracy and number of parameters. More importantly it is faster that similar approaches.},
keywords={AML;reinforcement learning;speech recognition;keyword spotting},
doi={10.1109/ICASSP40776.2020.9054748},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053900,
author={J. {Roth} and S. {Chaudhuri} and O. {Klejch} and R. {Marvin} and A. {Gallagher} and L. {Kaver} and S. {Ramaswamy} and A. {Stopczynski} and C. {Schmid} and Z. {Xi} and C. {Pantofaru}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Ava Active Speaker: An Audio-Visual Dataset for Active Speaker Detection},
year={2020},
volume={},
number={},
pages={4492-4496},
abstract={Active speaker detection is an important component in video analysis algorithms for applications such as speaker diarization, video re-targeting for meetings, speech enhancement, and human-robot interaction. The absence of a large, carefully labeled audio-visual active speaker dataset has limited evaluation in terms of data diversity, environments, and accuracy. In this paper, we present the AVA Active Speaker detection dataset (AVA-ActiveSpeaker) which has been publicly released to facilitate algorithm development and comparison. It contains temporally labeled face tracks in videos, where each face instance is labeled as speaking or not, and whether the speech is audible. The dataset contains about 3.65 million human labeled frames spanning 38.5 hours. We also introduce a state-of-the-art, jointly trained audio-visual model for real-time active speaker detection and compare several variants. The evaluation clearly demonstrates a significant gain due to audio-visual modeling and temporal integration over multiple frames.},
keywords={multimodal;audio-visual;active speaker detection;dataset},
doi={10.1109/ICASSP40776.2020.9053900},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054188,
author={T. N. {Sainath} and Y. {He} and B. {Li} and A. {Narayanan} and R. {Pang} and A. {Bruguier} and S. {Chang} and W. {Li} and R. {Alvarez} and Z. {Chen} and C. {Chiu} and D. {Garcia} and A. {Gruenstein} and K. {Hu} and A. {Kannan} and Q. {Liang} and I. {McGraw} and C. {Peyser} and R. {Prabhavalkar} and G. {Pundak} and D. {Rybach} and Y. {Shangguan} and Y. {Sheth} and T. {Strohman} and M. {Visontai} and Y. {Wu} and Y. {Zhang} and D. {Zhao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A Streaming On-Device End-To-End Model Surpassing Server-Side Conventional Model Quality and Latency},
year={2020},
volume={},
number={},
pages={6059-6063},
abstract={Thus far, end-to-end (E2E) models have not been shown to outperform state-of-the-art conventional models with respect to both quality, i.e., word error rate (WER), and latency, i.e., the time the hypothesis is finalized after the user stops speaking. In this paper, we develop a first-pass Recurrent Neural Network Transducer (RNN-T) model and a second-pass Listen, Attend, Spell (LAS) rescorer that surpasses a conventional model in both quality and latency. On the quality side, we incorporate a large number of utterances across varied domains [1] to increase acoustic diversity and the vocabulary seen by the model. We also train with accented English speech to make the model more robust to different pronunciations. In addition, given the increased amount of training data, we explore a varied learning rate schedule. On the latency front, we explore using the end-of-sentence decision emitted by the RNN-T model to close the microphone, and also introduce various optimizations to improve the speed of LAS rescoring. Overall, we find that RNN-T+LAS offers a better WER and latency tradeoff compared to a conventional model. For example, for the same latency, RNN-T+LAS obtains a 8% relative improvement in WER, while being more than 400-times smaller in model size.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054188},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054715,
author={B. {Li} and S. {Chang} and T. N. {Sainath} and R. {Pang} and Y. {He} and T. {Strohman} and Y. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Towards Fast and Accurate Streaming End-To-End ASR},
year={2020},
volume={},
number={},
pages={6069-6073},
abstract={End-to-end (E2E) models fold the acoustic, pronunciation and language models of a conventional speech recognition model into one neural network with a much smaller number of parameters than a conventional ASR system, thus making it suitable for on-device applications. For example, recurrent neural network transducer (RNN-T) as a streaming E2E model has shown promising potential for on-device ASR [1]. For such applications, quality and latency are two critical factors. We propose to reduce E2E model’s latency by extending the RNN-T endpointer (RNN-T EP) model [2] with additional early and late penalties. By further applying the minimum word error rate (MWER) training technique [3], we achieved 8.0% relative word error rate (WER) reduction and 130ms 90-percentile latency reduction over [2] on a Voice Search test set. We also experimented with a second-pass Listen, Attend and Spell (LAS) rescorer [4]. Although it did not directly improve the first pass latency, the large WER reduction provides extra room to trade WER for latency. RNN-T EP+LAS, together with MWER training brings in 18.7% relative WER reduction and 160ms 90-percentile latency reductions compared to the original proposed RNN-T EP [2] model.},
keywords={Training;Recurrent neural networks;Decoding;Error analysis;Transducers;Acoustics;Speech recognition;RNN-T;Endpointer;Latency},
doi={10.1109/ICASSP40776.2020.9054715},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054557,
author={Y. {Kubo} and M. {Bacchiani}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Joint Phoneme-Grapheme Model for End-To-End Speech Recognition},
year={2020},
volume={},
number={},
pages={6119-6123},
abstract={This paper proposes methods to improve a commonly used end-to-end speech recognition model, Listen-Attend-Spell (LAS). The methods we propose use multi-task learning to improve generalization of the model by leveraging information from multiple labels. The focus in this paper is on multi-task models for simultaneous signal-to-grapheme and signal-to-phoneme conversions while sharing the encoder parameters. Since phonemes are designed to be a precise description of the linguistic aspects of the speech signal, using phoneme recognition as an auxiliary task can help guiding the early stages of training to be more stable. In addition to conventional multi-task learning, we obtain further improvements by introducing a method that can exploit dependencies between labels in different tasks. Specifically, the dependencies between phonemes and grapheme sequences are considered. In conventional multi-task learning these sequences are assumed to be independent. Instead, in this paper, a joint model is proposed based on "iterative refinement" where dependency modeling is achieved by a multi-pass strategy. The proposed method is evaluated on a 28000h corpus of Japanese speech data. Performance of a conventional multi-task approach is contrasted with that of the joint model with iterative refinement.},
keywords={Automatic speech recognition;Listen-Attend-Spell;multi-task learning;iterative refinement},
doi={10.1109/ICASSP40776.2020.9054557},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054328,
author={A. {Tripathi} and H. {Lu} and H. {Sak}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={End-To-End Multi-Talker Overlapping Speech Recognition},
year={2020},
volume={},
number={},
pages={6129-6133},
abstract={In this paper we present an end-to-end speech recognition system that can recognize single-channel speech where multiple talkers can speak at the same time (overlapping speech) by using a neural network model based on Recurrent Neural Network Transducer (RNN-T) architecture. We augment the conventional RNN-T architecture by including a masking model for separation of encoded audio features, and multiple label encoders to encode transcripts from different speakers. We use a masking L2 loss to prevent transcripts to align to wrong speakers’ audio, and a speaker embedding loss to facilitate speaker tracking. We show that by using these additional training objectives, the proposed augmented RNN-T model can be trained with simulated overlapping speech data and can achieve a WER of 32% on words in overlapping speech segments from real-life telephone conversations. Our analysis of manual transcription task on the same test set shows that transcribing overlapping speech is hard even for humans who can get a WER of 37% compared to ground-truth.},
keywords={multi-talker;multi-speaker;overlapping speech;end-to-end},
doi={10.1109/ICASSP40776.2020.9054328},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052929,
author={S. {Gao} and Z. {Ou} and W. {Yang} and H. {Xu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Integrating Discrete and Neural Features Via Mixed-Feature Trans-Dimensional Random Field Language Models},
year={2020},
volume={},
number={},
pages={6169-6173},
abstract={There has been a long recognition that discrete features (n-gram features) and neural network based features have complementary strengths for language models (LMs). Improved performance can be obtained by model interpolation, which is, however, a sub-optimal two-step integration of discrete and neural features. The trans-dimensional random field (TRF) framework has the potential advantage of being able to flexibly integrate a richer set of features. However, either discrete or neural features are used alone in previous TRF LMs. This paper develops a mixed-feature TRF LM and demonstrates its advantage in integrating discrete and neural features. Various LMs are trained over PTB and Google one-billion-word datasets, and evaluated in N-best list rescoring experiments for speech recognition. Among all single LMs (i.e. without model interpolation), the mixed-feature TRF LMs perform the best, improving over both discrete TRF LMs and neural TRF LMs alone, and also being significantly better than LSTM LMs. Compared to interpolating two separately trained models with discrete and neural features respectively, the performance of mixed-feature TRF LMs matches the best interpolated model, and with simplified one-step training process and reduced training time.},
keywords={Language models;Trans-dimensional random fields;Dynamic noise-contrastive estimation;Speech recognition},
doi={10.1109/ICASSP40776.2020.9052929},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054106,
author={E. {Battenberg} and R. {Skerry-Ryan} and S. {Mariooryad} and D. {Stanton} and D. {Kao} and M. {Shannon} and T. {Bagby}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Location-Relative Attention Mechanisms for Robust Long-Form Speech Synthesis},
year={2020},
volume={},
number={},
pages={6194-6198},
abstract={Despite the ability to produce human-level speech for in-domain text, attention-based end-to-end text-to-speech (TTS) systems suffer from text alignment failures that increase in frequency for out-of-domain text. We show that these failures can be addressed using simple location-relative attention mechanisms that do away with content-based query/key comparisons. We compare two families of attention mechanisms: location-relative GMM-based mechanisms and additive energy-based mechanisms. We suggest simple modifications to GMM-based attention that allow it to align quickly and consistently during training, and introduce a new location-relative attention mechanism to the additive energy-based family, called Dynamic Convolution Attention (DCA). We compare the various mechanisms in terms of alignment speed and consistency during training, naturalness, and ability to generalize to long utterances, and conclude that GMM attention and DCA can generalize to very long utterances, while preserving naturalness for shorter, in-domain utterances.},
keywords={Speech synthesis;attention;sequence-to-sequence models},
doi={10.1109/ICASSP40776.2020.9054106},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053520,
author={G. {Sun} and Y. {Zhang} and R. J. {Weiss} and Y. {Cao} and H. {Zen} and Y. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fully-Hierarchical Fine-Grained Prosody Modeling For Interpretable Speech Synthesis},
year={2020},
volume={},
number={},
pages={6264-6268},
abstract={This paper proposes a hierarchical, fine-grained and interpretable latent variable model for prosody based on the Tacotron 2 text-to-speech model. It achieves multi-resolution modeling of prosody by conditioning finer level representations on coarser level ones. Additionally, it imposes hierarchical conditioning across all latent dimensions using a conditional variational auto-encoder (VAE) with an auto-regressive structure. Evaluation of reconstruction performance illustrates that the new structure does not degrade the model while allowing better interpretability. Interpretations of prosody attributes are provided together with the comparison between word-level and phone-level prosody representations. Moreover, both qualitative and quantitative evaluations are used to demonstrate the improvement in the disentanglement of the latent dimensions.},
keywords={text-to-speech;Tacotron 2;fine-grained VAE;hierarchical},
doi={10.1109/ICASSP40776.2020.9053520},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053126,
author={A. {Mani} and S. {Palaskar} and N. V. {Meripo} and S. {Konam} and F. {Metze}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={ASR Error Correction and Domain Adaptation Using Machine Translation},
year={2020},
volume={},
number={},
pages={6344-6348},
abstract={Off-the-shelf pre-trained Automatic Speech Recognition (ASR) systems are an increasingly viable service for companies of any size building speech-based products. While these ASR systems are trained on large amounts of data, domain mismatch is still an issue for many such parties that want to use this service as-is leading to not so optimal results for their task. We propose a simple technique to perform domain adaptation for ASR error correction via machine translation. The machine translation model is a strong candidate to learn a mapping from out-of-domain ASR errors to in-domain terms in the corresponding reference files. We use two off-the-shelf ASR systems in this work: Google ASR (commercial) and the ASPIRE model (open-source). We observe 7% absolute improvement in word error rate and 4 point absolute improvement in BLEU score in Google ASR output via our proposed method. We also evaluate ASR error correction via a downstream task of Speaker Diarization that captures speaker style, syntax, structure and semantic improvements we obtain via ASR correction.},
keywords={Domain adaptation;ASR error correction;machine translation;diarization;medical transcription},
doi={10.1109/ICASSP40776.2020.9053126},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053436,
author={G. {Sun} and Y. {Zhang} and R. J. {Weiss} and Y. {Cao} and H. {Zen} and A. {Rosenberg} and B. {Ramabhadran} and Y. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Generating Diverse and Natural Text-to-Speech Samples Using a Quantized Fine-Grained VAE and Autoregressive Prosody Prior},
year={2020},
volume={},
number={},
pages={6699-6703},
abstract={Recent neural text-to-speech (TTS) models with fine-grained latent features enable precise control of the prosody of synthesized speech. Such models typically incorporate a fine-grained variational autoencoder (VAE) structure, extracting latent features at each input token (e.g., phonemes). However, generating samples with the standard VAE prior often results in unnatural and discontinuous speech, with dramatic prosodic variation between tokens. This paper proposes a sequential prior in a discrete latent space which can generate more naturally sounding samples. This is accomplished by discretizing the latent features using vector quantization (VQ), and separately training an autoregressive (AR) prior model over the result. We evaluate the approach using listening tests, objective metrics of automatic speech recognition (ASR) performance, and measurements of prosody attributes. Experimental results show that the proposed model significantly improves the naturalness in random sample generation. Furthermore, initial experiments demonstrate that randomly sampling from the proposed model can be used as data augmentation to improve the ASR performance.},
keywords={text-to-speech;Tacotron 2;fine-grained VAE},
doi={10.1109/ICASSP40776.2020.9053436},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053657,
author={F. S. C. {Lim} and W. {Bastiaan Kleijn} and M. {Chinen} and J. {Skoglund}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Robust Low Rate Speech Coding Based on Cloned Networks and Wavenet},
year={2020},
volume={},
number={},
pages={6769-6773},
abstract={Rapid advances in machine-learning based generative modeling of speech make its use in speech coding attractive. However, the current performance of such models drops rapidly with noise contamination of the input, preventing use in practical applications. We present a new speech-coding scheme that is based on features that are robust to the distortions occurring in speech-coder input signals. To this purpose, we encourage the feature encoder to provide the same independent features for each of a set of linguistically equivalent signals, obtained by adding various noises to a common clean signal. The independent features, subjected to scalar quantization, are used as a conditioning vector sequence for WaveNet. Our experiments show that a 1.8 kb/s implementation of the resulting coder provides state-of-the-art performance for clean signals, and is additionally robust to noisy input.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053657},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053205,
author={D. S. {Park} and Y. {Zhang} and C. {Chiu} and Y. {Chen} and B. {Li} and W. {Chan} and Q. V. {Le} and Y. {Wu}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Specaugment on Large Scale Datasets},
year={2020},
volume={},
number={},
pages={6879-6883},
abstract={Recently, SpecAugment, an augmentation scheme for automatic speech recognition that acts directly on the spectrogram of input utterances, has shown to be highly effective in enhancing the performance of end-to-end networks on public datasets. In this paper, we demonstrate its effectiveness on tasks with large scale datasets by investigating its application to the Google Multidomain Dataset (Narayanan et al., 2018). We achieve improvement across all test domains by mixing raw training data augmented with SpecAugment and noise-perturbed training data when training the acoustic model. We also introduce a modification of SpecAugment that adapts the time mask size and/or multiplicity depending on the length of the utterance, which can potentially benefit large scale tasks. By using adaptive masking, we are able to further improve the performance of the Listen, Attend and Spell model on LibriSpeech to 2.2% WER on test-clean and 5.2% WER on test-other.},
keywords={End-to-end speech recognition;data augmentation;multi-domain training},
doi={10.1109/ICASSP40776.2020.9053205},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053974,
author={O. {Braga} and T. {Makino} and O. {Siohan} and H. {Liao}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={End-to-End Multi-Person Audio/Visual Automatic Speech Recognition},
year={2020},
volume={},
number={},
pages={6994-6998},
abstract={Traditionally, audio-visual automatic speech recognition has been studied under the assumption that the speaking face on the visual signal is the face matching the audio. However, in a more realistic setting, when multiple faces are potentially on screen one needs to decide which face to feed to the A/V ASR system. The present work takes the recent progress of A/V ASR one step further and considers the scenario where multiple people are simultaneously on screen (multi-person A/V ASR). We propose a fully differentiable A/V ASR model that is able to handle multiple face tracks in a video. Instead of relying on two separate models for speaker face selection and audiovisual ASR on a single face track, we introduce an attention layer to the ASR encoder that is able to soft-select the appropriate face video track. Experiments carried out on an A/V system trained on over 30k hours of YouTube videos illustrate that the proposed approach can automatically select the proper face tracks with minor WER degradation compared to an oracle selection of the speaking face while still showing benefits of employing the visual signal instead of the audio alone.},
keywords={Audio-visual speech recognition;Audio-visual speaker diarization},
doi={10.1109/ICASSP40776.2020.9053974},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053831,
author={G. {Wang} and A. {Rosenberg} and Z. {Chen} and Y. {Zhang} and B. {Ramabhadran} and Y. {Wu} and P. {Moreno}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Speech Recognition Using Consistent Predictions on Synthesized Speech},
year={2020},
volume={},
number={},
pages={7029-7033},
abstract={Speech synthesis has advanced to the point of being close to indistinguishable from human speech. However, efforts to train speech recognition systems on synthesized utterances have not been able to show that synthesized data can be effectively used to augment or replace human speech. In this work, we demonstrate that promoting consistent predictions in response to real and synthesized speech enables significantly improved speech recognition performance. We also find that training on 460 hours of LibriSpeech augmented with 500 hours of transcripts (without audio) performance is within 0.2% WER of a system trained on 960 hours of transcribed audio. This suggests that with this approach, when there is sufficient text available, reliance on transcribed audio can be cut nearly in half.},
keywords={Speech Recognition;Speech Synthesis;Data Augmentation},
doi={10.1109/ICASSP40776.2020.9053831},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053510,
author={T. N. {Sainath} and R. {Pang} and R. J. {Weiss} and Y. {He} and C. {Chiu} and T. {Strohman}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={An Attention-Based Joint Acoustic and Text on-Device End-To-End Model},
year={2020},
volume={},
number={},
pages={7039-7043},
abstract={Recently, we introduced a two-pass on-device end-to-end (E2E) speech recognition model, which runs RNN-T in the first-pass and then rescores/redecodes the result using a noncausal Listen, Attend and Spell (LAS) decoder. This on-device model obtained similar performance to a state-of-the-art conventional model. However, like many E2E models, it suffers from being trained only on supervised audio-text pairs and thus performs poorly on rare words compared to a conventional model which incorporates a language model trained on a much larger text corpus. In this work, we introduce a joint acoustic and text decoder (JATD) into the LAS decoder, which makes it possible to incorporate a much larger text corpus into training. We find that the JATD model obtains in a 3-10% relative improvement in WER compared to a LAS decoder trained only on supervised audio-text pairs across a variety of proper noun test sets.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053510},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054419,
author={M. {Ghodsi} and X. {Liu} and J. {Apfel} and R. {Cabrera} and E. {Weinstein}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Rnn-Transducer with Stateless Prediction Network},
year={2020},
volume={},
number={},
pages={7049-7053},
abstract={The RNN-Transducer (RNNT) outperforms classic Automatic Speech Recognition (ASR) systems when a large amount of supervised training data is available. For low-resource languages, the RNNT models overfit, and can not directly take advantage of additional large text corpora as in classic ASR systems.We focus on the prediction network of the RNNT, since it is believed to be analogous to the Language Model (LM) in the classic ASR systems. We pre-train the prediction network with text-only data, which is not helpful. Moreover, removing the recurrent layers from the prediction network, which makes the prediction network stateless, performs virtually as well as the original RNNT model, when using wordpieces. The stateless prediction network does not depend on the previous output symbols, except the last one. Therefore it simplifies the RNNT architectures and the inference.Our results suggest that the RNNT prediction network does not function as the LM in classical ASR. Instead, it merely helps the model align to the input audio, while the RNNT encoder and joint networks capture both the acoustic and the linguistic information.},
keywords={ASR;RNN-Transducer;RNNT;Prediction Network;Stateless},
doi={10.1109/ICASSP40776.2020.9054419},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9052937,
author={Z. {Lu} and L. {Cao} and Y. {Zhang} and C. {Chiu} and J. {Fan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Speech Sentiment Analysis via Pre-Trained Features from End-to-End ASR Models},
year={2020},
volume={},
number={},
pages={7149-7153},
abstract={In this paper, we propose to use pre-trained features from end-to-end ASR models to solve speech sentiment analysis as a down-stream task. We show that end-to-end ASR features, which integrate both acoustic and text information from speech, achieve promising results. We use RNN with self-attention as the sentiment classifier, which also provides an easy visualization through attention weights to help interpret model predictions. We use well benchmarked IEMOCAP dataset and a new large-scale speech sentiment dataset SWBD-sentiment for evaluation. Our approach improves the-state-of-the-art accuracy on IEMOCAP from 66.6% to 71.7%, and achieves an accuracy of 70.10% on SWBD-sentiment with more than 49,500 utterances.},
keywords={Speech sentiment analysis;ASR pretraining;End-to-end ASR model},
doi={10.1109/ICASSP40776.2020.9052937},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053751,
author={A. {Aksënova} and A. {Bruguier} and A. {Ritchart-Scott} and U. {Mendlovic}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Algorithmic Exploration of American English Dialects},
year={2020},
volume={},
number={},
pages={7374-7378},
abstract={In this paper, we use a novel algorithmic approach to explore dialectal variation in American English speech. Without the need for human phonemic annotations, we are able to use an existing corpus transcribed in text form only. Our results show that, in general, American English dialects can be divided into two larger groups: dialects of the South (Texas to North Carolina except for peninsular Florida), and the rest of the country. Our results confirm some well-known results from dialectology, such as the pin-pen merger, but show that some other ones, such as the cot-caught merger, may be losing their isogloss boundaries. Moreover, we demonstrate that our algorithm can extend to dialectal features in other languages.},
keywords={dialectology;isogloss;phonology;pronunciation learning},
doi={10.1109/ICASSP40776.2020.9053751},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053395,
author={S. {Mittermaier} and L. {Kürzinger} and B. {Waschneck} and G. {Rigoll}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Small-Footprint Keyword Spotting on Raw Audio Data with Sinc-Convolutions},
year={2020},
volume={},
number={},
pages={7454-7458},
abstract={Keyword Spotting (KWS) enables speech-based user interaction on smart devices. Always-on and battery-powered application scenarios for smart devices put constraints on hardware resources and power consumption, while also demanding high accuracy as well as real-time capability. Previous architectures first extracted acoustic features and then applied a neural network to classify keyword probabilities, optimizing towards memory footprint and execution time.Compared to previous publications, we took additional steps to reduce power and memory consumption without reducing classification accuracy. Power-consuming audio preprocessing and data transfer steps are eliminated by directly classifying from raw audio. For this, our end-to-end architecture extracts spectral features using parametrized Sinc-convolutions. Its memory footprint is further reduced by grouping depthwise separable convolutions. Our network achieves the competitive accuracy of 96.4% on Google’s Speech Commands test set with only 62k parameters.},
keywords={Keyword Spotting;Wake-Word Detection;Speech Commands;Sinc-Convolutions;Raw Audio},
doi={10.1109/ICASSP40776.2020.9053395},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053193,
author={J. {Lin} and K. {Kilgour} and D. {Roblek} and M. {Sharifi}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Training Keyword Spotters with Limited and Synthesized Speech Data},
year={2020},
volume={},
number={},
pages={7474-7478},
abstract={With the rise of low power speech-enabled devices, there is a growing demand to quickly produce models for recognizing arbitrary sets of keywords. As with many machine learning tasks, one of the most challenging parts in the model creation process is obtaining a sufficient amount of training data. In this paper, we explore the effectiveness of synthesized speech data in training small, spoken term detection models of around 400k parameters. Instead of training such models directly on the audio or low level features such as MFCCs, we use a pre-trained speech embedding model trained to extract useful features for keyword spotting models. Using this speech embedding, we show that a model which detects 10 keywords when trained on only synthetic speech is equivalent to a model trained on over 500 real examples. We also show that a model without our speech embeddings would need to be trained on over 4000 real examples to reach the same accuracy.},
keywords={keyword spotting;spoken term detection;limited data;speech synthesis},
doi={10.1109/ICASSP40776.2020.9053193},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053512,
author={T. {Hayashi} and R. {Yamamoto} and K. {Inoue} and T. {Yoshimura} and S. {Watanabe} and T. {Toda} and K. {Takeda} and Y. {Zhang} and X. {Tan}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Espnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit},
year={2020},
volume={},
number={},
pages={7654-7658},
abstract={This paper introduces a new end-to-end text-to-speech (E2E-TTS) toolkit named ESPnet-TTS, which is an extension of the open-source speech processing toolkit ESPnet. The toolkit supports state-of- the-art E2E-TTS models, including Tacotron 2, Transformer TTS, and FastSpeech, and also provides recipes inspired by the Kaldi automatic speech recognition (ASR) toolkit. The recipes are based on the design unified with the ESPnet ASR recipe, providing high reproducibility. The toolkit also provides pre-trained models and samples of all of the recipes so that users can use it as a baseline. Furthermore, the unified design enables the integration of ASR functions with TTS, e.g., ASR-based objective evaluation and semi- supervised learning with both ASR and TTS models. This paper describes the design of the toolkit and experimental evaluation in comparison with other toolkits. The experimental results show that our models can achieve state-of-the-art performance comparable to the other latest toolkits, resulting in a mean opinion score (MOS) of 4.25 on the LJSpeech dataset. The toolkit is publicly available at https://github.com/espnet/espnet.},
keywords={Open-source;end-to-end;text-to-speech},
doi={10.1109/ICASSP40776.2020.9053512},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054235,
author={C. {Peyser} and T. N. {Sainath} and G. {Pundak}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Proper Noun Recognition in End-To-End Asr by Customization of the Mwer Loss Criterion},
year={2020},
volume={},
number={},
pages={7789-7793},
abstract={Proper nouns present a challenge for end-to-end (E2E) automatic speech recognition (ASR) systems in that a particular name may appear only rarely during training, and may have a pronunciation similar to that of a more common word. Unlike conventional ASR models, E2E systems lack an explicit pronounciation model that can be specifically trained with proper noun pronounciations and a language model that can be trained on a large text-only corpus. Past work has addressed this issue by incorporating additional training data or additional models. In this paper, we instead build on recent advances in minimum word error rate (MWER) training to develop two new loss criteria that specifically emphasize proper noun recognition. Unlike past work on this problem, this method requires no new data during training or external models during inference. We see improvements ranging from 2% to 7% relative on several relevant benchmarks.},
keywords={},
doi={10.1109/ICASSP40776.2020.9054235},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053606,
author={K. {Hu} and T. N. {Sainath} and R. {Pang} and R. {Prabhavalkar}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Deliberation Model Based Two-Pass End-To-End Speech Recognition},
year={2020},
volume={},
number={},
pages={7799-7803},
abstract={End-to-end (E2E) models have made rapid progress in automatic speech recognition (ASR) and perform competitively relative to conventional models. To further improve the quality, a two-pass model has been proposed to rescore streamed hypotheses using the non-streaming Listen, Attend and Spell (LAS) model while maintaining a reasonable latency. The model attends to acoustics to rescore hypotheses, as opposed to a class of neural correction models that use only first-pass text hypotheses. In this work, we propose to attend to both acoustics and first-pass hypotheses using a deliberation network. A bidirectional encoder is used to extract context information from first-pass hypotheses. The proposed deliberation model achieves 12% relative WER reduction compared to LAS rescoring in Google Voice Search (VS) tasks, and 23% reduction on a proper noun test set. Compared to a large conventional model, our best model performs 21% relatively better for VS. In terms of computational complexity, the deliberation decoder has a larger size than the LAS decoder, and hence requires more computations in second-pass decoding.},
keywords={},
doi={10.1109/ICASSP40776.2020.9053606},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054259,
author={B. {Haynor} and P. S. {Aleksic}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Incorporating Written Domain Numeric Grammars into End-To-End Contextual Speech Recognition Systems for Improved Recognition of Numeric Sequences},
year={2020},
volume={},
number={},
pages={7809-7813},
abstract={Accurate recognition of numeric sequences is crucial for many contextual speech recognition applications. For example, a user might create a calendar event and be prompted by a virtual assistant for the time, date, and duration of the event. We propose a modular and scalable solution for improved recognition of numeric sequences. We use finite state transducers built from written domain numeric grammars to increase the likelihood of hypotheses containing matching numeric entities during beam search in an end-to-end speech recognition system. Using our technique results in relative reduction in word error rate of up to 59% on a variety of numeric sequence recognition tasks (times, percentages, digit sequences, …).},
keywords={Speech recognition;RNN-T;end-to-end;contextual ASR;FSTs},
doi={10.1109/ICASSP40776.2020.9054259},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054287,
author={Z. {Wu} and B. {Li} and Y. {Zhang} and P. S. {Aleksic} and T. N. {Sainath}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Multistate Encoding with End-To-End Speech RNN Transducer Network},
year={2020},
volume={},
number={},
pages={7819-7823},
abstract={Recurrent Neural Network Transducer (RNN-T) models [1] for automatic speech recognition (ASR) provide high accuracy speech recognition. Such end-to-end (E2E) models combine acoustic, pronunciation and language models (AM, PM, LM) of a conventional ASR system into a single neural network, dramatically reducing complexity and model size.In this paper, we propose a technique for incorporating contextual signals, such as intelligent assistant device state or dialog state, directly into RNN-T models. We explore different encoding methods and demonstrate that RNN-T models can effectively utilize such context. Our technique results in reduction in Word Error Rate (WER) of up to 10.4% relative on a variety of contextual recognition tasks. We also demonstrate that proper regularization can be used to model context independently for improved overall quality.},
keywords={E2E ASR;contextual ASR;sequence to sequence},
doi={10.1109/ICASSP40776.2020.9054287},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054745,
author={E. {Variani} and T. {Chen} and J. {Apfel} and B. {Ramabhadran} and S. {Lee} and P. {Moreno}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Neural Oracle Search on N-BEST Hypotheses},
year={2020},
volume={},
number={},
pages={7824-7828},
abstract={In this paper, we propose a neural search algorithm to select the most likely hypothesis using a sequence of acoustic representations and multiple hypotheses as input. The algorithm provides a sequence level score for each audio-hypothesis pair that is obtained by integrating information from multiple sources, such as the input acoustic representations, N-best hypotheses, additional 1st-pass statistics, and unpaired textual information through an external language model. These scores are then used to map the search problem of identifying the most likely hypothesis to a sequence classification problem. The definition of the proposed algorithm is broad enough to allow its use as an alternative to beam search in the 1st-pass or as a 2nd-pass, rescoring step. This algorithm achieves up to 12% relative reductions in Word Error Rate (WER) across several languages over state-of-the-art baselines with relatively few additional parameters. We also propose the use of a binary classifier gating function that can learn to trigger the 2nd-pass neural search model when the 1-best hypothesis is not the oracle hypothesis, thereby avoiding extra computation.},
keywords={Speech recognition;Encoder-decoder;N-best rescoring},
doi={10.1109/ICASSP40776.2020.9054745},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053896,
author={Q. {Zhang} and H. {Lu} and H. {Sak} and A. {Tripathi} and E. {McDermott} and S. {Koo} and S. {Kumar}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss},
year={2020},
volume={},
number={},
pages={7829-7833},
abstract={In this paper we present an end-to-end speech recognition model with Transformer encoders that can be used in a streaming speech recognition system. Transformer computation blocks based on self-attention are used to encode both audio and label sequences independently. The activations from both audio and label encoders are combined with a feed-forward layer to compute a probability distribution over the label space for every combination of acoustic frame position and label history. This is similar to the Recurrent Neural Network Transducer (RNN-T) model, which uses RNNs for information encoding instead of Transformer encoders. The model is trained with the RNN-T loss well-suited to streaming decoding. We present results on the LibriSpeech dataset showing that limiting the left context for self-attention in the Transformer layers makes decoding computationally tractable for streaming, with only a slight degradation in accuracy. We also show that the full attention version of our model beats the-state-of-the art accuracy on the LibriSpeech benchmarks. Our results also show that we can bridge the gap between full attention and limited attention versions of our model by attending to a limited number of future frames.},
keywords={Transformer;RNN-T;sequence-to-sequence;encoder-decoder;end-to-end;speech recognition},
doi={10.1109/ICASSP40776.2020.9053896},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054304,
author={K. {Gillespie} and I. C. {Konstantakopoulos} and X. {Guo} and V. T. {Vasudevan} and A. {Sethy}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Improving Device Directedness Classification of Utterances With Semantic Lexical Features},
year={2020},
volume={},
number={},
pages={7859-7863},
abstract={User interactions with personal assistants like Alexa, Google Home and Siri are typically initiated by a wake term or wake-word. Several personal assistants feature "follow-up" modes that allow users to make additional interactions without the need of a wakeword. For the system to only respond when appropriate, and to ignore speech not intended for it, utterances must be classified as device-directed or non-device-directed. State of the art systems have largely used acoustic features for this task, while others have used only lexical features or have added LM-based lexical features. We propose a directedness classifier that combines semantic lexical features with a lightweight acoustic feature and show it is effective in classifying directedness. The mixed-domain lexical and acoustic feature model is able to achieve 14% relative reduction of EER over a state of the art acoustic-only baseline model. Finally, we successfully apply transfer learning and semi-supervised learning to the model to improve accuracy even further.},
keywords={Directedness;Semantic Classification;LSTM;Semi-supervised Learning;Word Embeddings},
doi={10.1109/ICASSP40776.2020.9054304},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054039,
author={H. {Park} and P. {Violette} and N. {Subrahmanya}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learning to Detect Keyword Parts and Whole by Smoothed Max Pooling},
year={2020},
volume={},
number={},
pages={7899-7903},
abstract={We propose smoothed max pooling loss and its application to keyword spotting systems. The proposed approach jointly trains an encoder (to detect keyword parts) and a decoder (to detect whole keyword) in a semi-supervised manner. The proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame-level labeling from LVCSR (Large vocabulary continuous speech recognition), making further optimization possible. The proposed system outperforms the baseline keyword spotting model in [1] due to increased optimizability. Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR.},
keywords={deep neural networks;keyword spotting;audio processing;embedded speech recognition;on-device learning},
doi={10.1109/ICASSP40776.2020.9054039},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054585,
author={N. {Arivazhagan} and C. {Cherry} and I. {Te} and W. {Macherey} and P. {Baljekar} and G. {Foster}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Re-Translation Strategies for Long Form, Simultaneous, Spoken Language Translation},
year={2020},
volume={},
number={},
pages={7919-7923},
abstract={We investigate the problem of simultaneous machine translation of long-form speech content. We target a continuous speech-to-text scenario, generating translated captions for a live audio feed, such as a lecture or play-by-play commentary. As this scenario allows for revisions to our incremental translations, we adopt a re-translation approach to simultaneous translation, where the source is repeatedly translated from scratch as it grows. This approach naturally exhibits very low latency and high final quality, but at the cost of incremental instability as the output is continuously refined. We experiment with a pipeline of industry-grade speech recognition and translation tools, augmented with simple inference heuristics to improve stability. We use TED Talks as a source of multilingual test data, developing our techniques on English-to-German spoken language translation. Our minimalist approach to simultaneous translation allows us to scale our final evaluation to several other target languages, dramatically improving incremental stability for all of them.},
keywords={Speech Recognition Neural Machine Translation},
doi={10.1109/ICASSP40776.2020.9054585},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053443,
author={A. {Datta} and B. {Ramabhadran} and J. {Emond} and A. {Kannan} and B. {Roark}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Language-Agnostic Multilingual Modeling},
year={2020},
volume={},
number={},
pages={8239-8243},
abstract={Multilingual Automated Speech Recognition (ASR) systems allow for the joint training of data-rich and data-scarce languages in a single model. This enables data and parameter sharing across languages, which is especially beneficial for the data-scarce languages. However, most state-of-the-art multilingual models require the encoding of language information and therefore are not as flexible or scalable when expanding to newer languages. Language-independent multilingual models help to address this issue, and are also better suited for multicultural societies where several languages are frequently used together (but often rendered with different writing systems). In this paper, we propose a new approach to building a language-agnostic multilingual ASR system which transforms all languages to one writing system through a many-to-one transliteration transducer. Thus, similar sounding acoustics are mapped to a single, canonical target sequence of graphemes, effectively separating the modeling and rendering problems. We show with four Indic languages, namely, Hindi, Bengali, Tamil and Kannada, that the language-agnostic multilingual model achieves up to 10% relative reduction in Word Error Rate (WER) over a language-dependent multilingual model.},
keywords={speech recognition;language-independent;multilingual;transliteration;RNN-T},
doi={10.1109/ICASSP40776.2020.9053443},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053240,
author={Q. {Huang} and A. {Jansen} and L. {Zhang} and D. P. W. {Ellis} and R. A. {Saurous} and J. {Anderson}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Large-Scale Weakly-Supervised Content Embeddings for Music Recommendation and Tagging},
year={2020},
volume={},
number={},
pages={8364-8368},
abstract={We explore content-based representation learning strategies tailored for large-scale, uncurated music collections that afford only weak supervision through unstructured natural language metadata and colisten statistics. At the core is a hybrid training scheme that uses classification and metric learning losses to incorporate both metadata-derived text labels and aggregate co-listen supervisory signals into a single convolutional model. The resulting joint text and audio content embedding defines a similarity metric and supports prediction of semantic text labels using a vocabulary of unprecedented granularity, which we refine using a novel word-sense disambiguation procedure. As input to simple classifier architectures, our representation achieves state-of-the-art performance on two music tagging benchmarks.},
keywords={Music information retrieval;music content embedding;word sense disambiguation;joint audio and text models},
doi={10.1109/ICASSP40776.2020.9053240},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053856,
author={I. M. {Comsa} and T. {Fischbacher} and K. {Potempa} and A. {Gesmundo} and L. {Versari} and J. {Alakuijala}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function},
year={2020},
volume={},
number={},
pages={8529-8533},
abstract={We propose a spiking neural network model that encodes information in the relative timing of individual neuron spikes and performs classification using the first output neuron to spike. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic with respect to presynaptic spike times. The network uses a biologically-inspired alpha synaptic transfer function and trainable synchronisation pulses as temporal references. We successfully train the network on the MNIST dataset encoded in time. Our spiking neural network outperforms comparable spiking models and achieves similar accuracy to a fully connected conventional network. During training, our network displays a speed-accuracy trade-off, with either slow and highly-accurate or very fast but less accurate classification. The results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. Our code is publicly available.},
keywords={spiking networks;temporal coding;back-propagation;image classification;machine learning},
doi={10.1109/ICASSP40776.2020.9053856},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053173,
author={B. {Zylich} and J. {Whitehill}},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Noise-Robust Key-Phrase Detectors for Automated Classroom Feedback},
year={2020},
volume={},
number={},
pages={9215-9219},
abstract={With the goal of giving teachers automated feedback about their classrooms, we investigate how to train automatic speech detectors of key phrases such as good job, thank you, please, and you’re welcome. This kind of language conveys support and respect from teacher to student and is one of the behavioral markers used in the established CLASS [1] classroom observation protocol. School classrooms are noisy and contain overlapping speech, presenting a highly challenging environment for automatic speech recognition (ASR), even for state-of-the-art approaches. We train deep neural networks using hierarchical multitask learning (MTL) on a modest-sized but highly-tailored dataset of classroom speech. Compared to 2 state-of-the-art ASR systems for general-purpose speech recognition (Google [2] and Deep-Speech [3]), our system delivers a substantially improved recall rate (50.4% versus 20.5%) while matching their precision (30%). Moreover, our system’s predictions correlate with several dimensions of the CLASS.},
keywords={education;speech recognition;deep learning;multitask learning},
doi={10.1109/ICASSP40776.2020.9053173},
ISSN={2379-190X},
month={May},}
('speech recognition', 10)
('', 6)
('rnnt', 5)
('multitask learning', 4)
('keyword spotting', 4)
('endtoend', 4)
('texttospeech', 3)
('deep learning', 3)
('speech synthesis', 3)
('ondevice learning', 2)
('convolutional neural networks', 2)
('finegrained vae', 2)
('tacotron 2', 2)
('deep neural networks', 2)
('encoderdecoder', 2)
('data augmentation', 2)
('contextual asr', 2)
('sound classification', 2)
('audio pitch estimation', 1)
('convolutional neural network', 1)
('graph convolutional networks', 1)
('semisupervised learning', 1)
('sincconvolutions', 1)
('lstm', 1)
('limited data', 1)
('machine translation', 1)
('adversarial attack', 1)
('dynamic noisecontrastive estimation', 1)
('opensource', 1)
('joint audio and text models', 1)
('word embeddings', 1)
('active speaker detection', 1)
('transducers', 1)
('language models', 1)
('raw audio', 1)
('human visual model', 1)
('selfsupervised learning', 1)
('asr pretraining', 1)
('video coding', 1)
('phonology', 1)
('hierarchical structure', 1)
('semantic audio representations', 1)
('isogloss', 1)
('motion compensation', 1)
('unsupervised learning', 1)
('audio tagging', 1)
('semantic classification', 1)
('multimodal models', 1)
('asr error correction', 1)
('image classification', 1)
('e2e asr', 1)
('cepstral feature', 1)
('spoken term detection', 1)
('domain adaptation', 1)
('multidomain training', 1)
('transliteration', 1)
('overlapping speech', 1)
('speaker verification', 1)
('decoding', 1)
('noise reduction', 1)
('transdimensional random fields', 1)
('asr', 1)
('endtoend asr model', 1)
('iterative refinement', 1)
('audiovisual', 1)
('attention', 1)
('audiovisual speaker diarization', 1)
('cough', 1)
('training', 1)
('pronunciation learning', 1)
('reinforcement learning', 1)
('dataset', 1)
('medical transcription', 1)
('speech recognition neural machine translation', 1)
('compression artifact', 1)
('clustering', 1)
('temporal coding', 1)
('speech commands', 1)
('transformer', 1)
('health sensing', 1)
('music content embedding', 1)
('linear estimation', 1)
('sequencetosequence models', 1)
('error analysis', 1)
('nbest rescoring', 1)
('hierarchical', 1)
('endpointer', 1)
('sequencetosequence', 1)
('dialectology', 1)
('false contour', 1)
('machine learning', 1)
('rnntransducer', 1)
('speech enhancement', 1)
('time domain', 1)
('word sense disambiguation', 1)
('audio source separation', 1)
('banding artifact', 1)
('spiking networks', 1)
('sequence to sequence', 1)
('multimodal', 1)
('automatic speech recognition', 1)
('multitalker', 1)
('memory reduction', 1)
('education', 1)
('listenattendspell', 1)
('aml', 1)
('latency', 1)
('recurrent neural networks', 1)
('endtoend speech recognition', 1)
('speech sentiment analysis', 1)
('audiovisual speech recognition', 1)
('audio processing', 1)
('video quality predictor', 1)
('active learning', 1)
('multilingual', 1)
('lowrank gradient', 1)
('embedded speech recognition', 1)
('fsts', 1)
('diarization', 1)
('prediction network', 1)
('music information retrieval', 1)
('stateless', 1)
('acoustics', 1)
('multispeaker', 1)
('directedness', 1)
('backpropagation', 1)
('languageindependent', 1)
('wakeword detection', 1)